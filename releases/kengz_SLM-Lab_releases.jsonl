{"url": "https://api.github.com/repos/kengz/SLM-Lab/releases/55599369", "assets_url": "https://api.github.com/repos/kengz/SLM-Lab/releases/55599369/assets", "upload_url": "https://uploads.github.com/repos/kengz/SLM-Lab/releases/55599369/assets{?name,label}", "html_url": "https://github.com/kengz/SLM-Lab/releases/tag/v4.2.4", "id": 55599369, "author": {"login": "kengz", "id": 8209263, "node_id": "MDQ6VXNlcjgyMDkyNjM=", "avatar_url": "https://avatars.githubusercontent.com/u/8209263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kengz", "html_url": "https://github.com/kengz", "followers_url": "https://api.github.com/users/kengz/followers", "following_url": "https://api.github.com/users/kengz/following{/other_user}", "gists_url": "https://api.github.com/users/kengz/gists{/gist_id}", "starred_url": "https://api.github.com/users/kengz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kengz/subscriptions", "organizations_url": "https://api.github.com/users/kengz/orgs", "repos_url": "https://api.github.com/users/kengz/repos", "events_url": "https://api.github.com/users/kengz/events{/privacy}", "received_events_url": "https://api.github.com/users/kengz/received_events", "type": "User", "site_admin": false}, "node_id": "RE_kwDOBksxGc4DUGEJ", "tag_name": "v4.2.4", "target_commitish": "master", "name": "upgrade plotly, replace orca with kaleido", "draft": false, "prerelease": false, "created_at": "2021-12-18T15:52:15Z", "published_at": "2021-12-18T15:52:55Z", "assets": [], "tarball_url": "https://api.github.com/repos/kengz/SLM-Lab/tarball/v4.2.4", "zipball_url": "https://api.github.com/repos/kengz/SLM-Lab/zipball/v4.2.4", "body": "## What's Changed\r\n* upgrade plotly, replace orca with kaleido by @kengz in https://github.com/kengz/SLM-Lab/pull/501\r\n\r\n\r\n**Full Changelog**: https://github.com/kengz/SLM-Lab/compare/v4.2.3...v4.2.4", "mentions_count": 1}
{"url": "https://api.github.com/repos/kengz/SLM-Lab/releases/54663247", "assets_url": "https://api.github.com/repos/kengz/SLM-Lab/releases/54663247/assets", "upload_url": "https://uploads.github.com/repos/kengz/SLM-Lab/releases/54663247/assets{?name,label}", "html_url": "https://github.com/kengz/SLM-Lab/releases/tag/v4.2.3", "id": 54663247, "author": {"login": "kengz", "id": 8209263, "node_id": "MDQ6VXNlcjgyMDkyNjM=", "avatar_url": "https://avatars.githubusercontent.com/u/8209263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kengz", "html_url": "https://github.com/kengz", "followers_url": "https://api.github.com/users/kengz/followers", "following_url": "https://api.github.com/users/kengz/following{/other_user}", "gists_url": "https://api.github.com/users/kengz/gists{/gist_id}", "starred_url": "https://api.github.com/users/kengz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kengz/subscriptions", "organizations_url": "https://api.github.com/users/kengz/orgs", "repos_url": "https://api.github.com/users/kengz/repos", "events_url": "https://api.github.com/users/kengz/events{/privacy}", "received_events_url": "https://api.github.com/users/kengz/received_events", "type": "User", "site_admin": false}, "node_id": "RE_kwDOBksxGc4DQhhP", "tag_name": "v4.2.3", "target_commitish": "master", "name": "fix GPU installation and assignment issue", "draft": false, "prerelease": false, "created_at": "2021-12-06T00:04:09Z", "published_at": "2021-12-06T00:05:08Z", "assets": [], "tarball_url": "https://api.github.com/repos/kengz/SLM-Lab/tarball/v4.2.3", "zipball_url": "https://api.github.com/repos/kengz/SLM-Lab/zipball/v4.2.3", "body": "## What's Changed\r\n* Added Algorithms config files for VideoPinball-v0 game by @dd-iuonac in https://github.com/kengz/SLM-Lab/pull/488\r\n* fix build for new RTX GPUs by @kengz and @Karl-Grantham in https://github.com/kengz/SLM-Lab/pull/496\r\n* remove the `reinforce_pong.json` spec to prevent confusion in https://github.com/kengz/SLM-Lab/pull/499\r\n\r\n## New Contributors\r\n* @dd-iuonac made their first contribution in https://github.com/kengz/SLM-Lab/pull/488\r\n* @Karl-Grantham for help with debugging #496 \r\n\r\n**Full Changelog**: https://github.com/kengz/SLM-Lab/compare/v4.2.2...v4.2.3", "mentions_count": 3}
{"url": "https://api.github.com/repos/kengz/SLM-Lab/releases/43544923", "assets_url": "https://api.github.com/repos/kengz/SLM-Lab/releases/43544923/assets", "upload_url": "https://uploads.github.com/repos/kengz/SLM-Lab/releases/43544923/assets{?name,label}", "html_url": "https://github.com/kengz/SLM-Lab/releases/tag/v4.2.2", "id": 43544923, "author": {"login": "kengz", "id": 8209263, "node_id": "MDQ6VXNlcjgyMDkyNjM=", "avatar_url": "https://avatars.githubusercontent.com/u/8209263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kengz", "html_url": "https://github.com/kengz", "followers_url": "https://api.github.com/users/kengz/followers", "following_url": "https://api.github.com/users/kengz/following{/other_user}", "gists_url": "https://api.github.com/users/kengz/gists{/gist_id}", "starred_url": "https://api.github.com/users/kengz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kengz/subscriptions", "organizations_url": "https://api.github.com/users/kengz/orgs", "repos_url": "https://api.github.com/users/kengz/repos", "events_url": "https://api.github.com/users/kengz/events{/privacy}", "received_events_url": "https://api.github.com/users/kengz/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTQzNTQ0OTIz", "tag_name": "v4.2.2", "target_commitish": "master", "name": "Improve Installation / Colab notebook", "draft": false, "prerelease": false, "created_at": "2021-05-25T16:19:02Z", "published_at": "2021-05-25T16:28:27Z", "assets": [], "tarball_url": "https://api.github.com/repos/kengz/SLM-Lab/tarball/v4.2.2", "zipball_url": "https://api.github.com/repos/kengz/SLM-Lab/zipball/v4.2.2", "body": "## Improve Installation Stability\r\n*:raised_hands: Thanks to @Nickfagiano help with debugging.*\r\n- #487 update installation to work with MacOS BigSur\r\n- #487 improve setup with Conda path guard\r\n- #487 lock `atari-py` version to 0.2.6 for safety\r\n\r\n## Google Colab/Jupyter\r\n*:raised_hands: Thanks to @piosif97 for helping.*\r\n- added [instruction for Google Colab/Jupyter](https://slm-lab.gitbook.io/slm-lab/resources/help#google-colab-jupyter-setup)\r\n- [example Colab notebook](https://gist.github.com/kengz/6fd52a902129fb6d4509c721d71bda48)\r\n\r\n## Windows setup\r\n*:raised_hands: Thanks to @vladimirnitu and @steindaian for providing the PDF.*\r\n- added [Windows setup instruction](https://slm-lab.gitbook.io/slm-lab/setup/installation#windows)"}
{"url": "https://api.github.com/repos/kengz/SLM-Lab/releases/43028045", "assets_url": "https://api.github.com/repos/kengz/SLM-Lab/releases/43028045/assets", "upload_url": "https://uploads.github.com/repos/kengz/SLM-Lab/releases/43028045/assets{?name,label}", "html_url": "https://github.com/kengz/SLM-Lab/releases/tag/v4.2.1", "id": 43028045, "author": {"login": "kengz", "id": 8209263, "node_id": "MDQ6VXNlcjgyMDkyNjM=", "avatar_url": "https://avatars.githubusercontent.com/u/8209263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kengz", "html_url": "https://github.com/kengz", "followers_url": "https://api.github.com/users/kengz/followers", "following_url": "https://api.github.com/users/kengz/following{/other_user}", "gists_url": "https://api.github.com/users/kengz/gists{/gist_id}", "starred_url": "https://api.github.com/users/kengz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kengz/subscriptions", "organizations_url": "https://api.github.com/users/kengz/orgs", "repos_url": "https://api.github.com/users/kengz/repos", "events_url": "https://api.github.com/users/kengz/events{/privacy}", "received_events_url": "https://api.github.com/users/kengz/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTQzMDI4MDQ1", "tag_name": "v4.2.1", "target_commitish": "master", "name": "Update installation", "draft": false, "prerelease": false, "created_at": "2021-05-17T03:24:08Z", "published_at": "2021-05-17T03:35:20Z", "assets": [], "tarball_url": "https://api.github.com/repos/kengz/SLM-Lab/tarball/v4.2.1", "zipball_url": "https://api.github.com/repos/kengz/SLM-Lab/zipball/v4.2.1", "body": "## Update installation\r\nDependencies and systems around SLM Lab has changed and caused some breakages. This release fixes these installation issues.\r\n\r\n- #461, #476 update to `homebrew/cask` (thanks @ben-e, @amjadmajid )\r\n- #463 add pybullet to dependencies (thanks @rafapi)\r\n- #483 fix missing install command in Arch Linux setup (thanks @sebimarkgraf)\r\n- #485 update GitHub Actions CI to v2\r\n- #485 fix demo spec to use strict json"}
{"url": "https://api.github.com/repos/kengz/SLM-Lab/releases/25496334", "assets_url": "https://api.github.com/repos/kengz/SLM-Lab/releases/25496334/assets", "upload_url": "https://uploads.github.com/repos/kengz/SLM-Lab/releases/25496334/assets{?name,label}", "html_url": "https://github.com/kengz/SLM-Lab/releases/tag/v4.2.0", "id": 25496334, "author": {"login": "kengz", "id": 8209263, "node_id": "MDQ6VXNlcjgyMDkyNjM=", "avatar_url": "https://avatars.githubusercontent.com/u/8209263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kengz", "html_url": "https://github.com/kengz", "followers_url": "https://api.github.com/users/kengz/followers", "following_url": "https://api.github.com/users/kengz/following{/other_user}", "gists_url": "https://api.github.com/users/kengz/gists{/gist_id}", "starred_url": "https://api.github.com/users/kengz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kengz/subscriptions", "organizations_url": "https://api.github.com/users/kengz/orgs", "repos_url": "https://api.github.com/users/kengz/repos", "events_url": "https://api.github.com/users/kengz/events{/privacy}", "received_events_url": "https://api.github.com/users/kengz/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTI1NDk2MzM0", "tag_name": "v4.2.0", "target_commitish": "master", "name": "Resume mode, Plotly and PyTorch update, OnPolicyCrossEntropy memory", "draft": false, "prerelease": false, "created_at": "2020-04-14T16:56:23Z", "published_at": "2020-04-14T17:08:35Z", "assets": [], "tarball_url": "https://api.github.com/repos/kengz/SLM-Lab/tarball/v4.2.0", "zipball_url": "https://api.github.com/repos/kengz/SLM-Lab/zipball/v4.2.0", "body": "## Resume mode\r\n\r\n- #455 adds `train@` resume mode and refactors the `enjoy` mode. See PR for detailed info.\r\n\r\n### `train@` usage example\r\n\r\nSpecify train mode as `train@{predir}`, where `{predir} is the data directory of the last training run, or simply use `latest` to use the latest. e.g.:\r\n```bash\r\npython run_lab.py slm_lab/spec/benchmark/reinforce/reinforce_cartpole.json reinforce_cartpole train\r\n# terminate run before its completion\r\n# optionally edit the spec file in a past-future-consistent manner\r\n\r\n# run resume with either of the commands:\r\npython run_lab.py slm_lab/spec/benchmark/reinforce/reinforce_cartpole.json reinforce_cartpole train@latest\r\n# or to use a specific run folder\r\npython run_lab.py slm_lab/spec/benchmark/reinforce/reinforce_cartpole.json reinforce_cartpole train@data/reinforce_cartpole_2020_04_13_232521\r\n```\r\n\r\n### `enjoy` mode refactor\r\n\r\nThe `train@` resume mode API allows for the `enjoy` mode to be refactored. Both share similar syntax. Continuing with the example above, to enjoy a train model, we now use:\r\n```bash\r\npython run_lab.py slm_lab/spec/benchmark/reinforce/reinforce_cartpole.json reinforce_cartpole enjoy@data/reinforce_cartpole_2020_04_13_232521/reinforce_cartpole_t0_s0_spec.json\r\n```\r\n\r\n## Plotly and PyTorch update\r\n\r\n- #453 updates Plotly to 4.5.4 and PyTorch to 1.3.1.\r\n- #454 explicitly shuts down Plotly orca server after plotting to prevent zombie processes\r\n\r\n## PPO batch size optimization\r\n\r\n- #453 adds chunking to allow PPO to run on larger batch size by breaking up the forward loop.\r\n\r\n## New OnPolicyCrossEntropy memory\r\n\r\n- #446 adds a new `OnPolicyCrossEntropy` memory class. See PR for details. Credits to @ingambe.\r\n"}
{"url": "https://api.github.com/repos/kengz/SLM-Lab/releases/21431668", "assets_url": "https://api.github.com/repos/kengz/SLM-Lab/releases/21431668/assets", "upload_url": "https://uploads.github.com/repos/kengz/SLM-Lab/releases/21431668/assets{?name,label}", "html_url": "https://github.com/kengz/SLM-Lab/releases/tag/v4.1.1", "id": 21431668, "author": {"login": "kengz", "id": 8209263, "node_id": "MDQ6VXNlcjgyMDkyNjM=", "avatar_url": "https://avatars.githubusercontent.com/u/8209263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kengz", "html_url": "https://github.com/kengz", "followers_url": "https://api.github.com/users/kengz/followers", "following_url": "https://api.github.com/users/kengz/following{/other_user}", "gists_url": "https://api.github.com/users/kengz/gists{/gist_id}", "starred_url": "https://api.github.com/users/kengz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kengz/subscriptions", "organizations_url": "https://api.github.com/users/kengz/orgs", "repos_url": "https://api.github.com/users/kengz/repos", "events_url": "https://api.github.com/users/kengz/events{/privacy}", "received_events_url": "https://api.github.com/users/kengz/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTIxNDMxNjY4", "tag_name": "v4.1.1", "target_commitish": "master", "name": "Discrete SAC benchmark update", "draft": false, "prerelease": false, "created_at": "2019-11-13T08:20:31Z", "published_at": "2019-11-13T08:21:09Z", "assets": [], "tarball_url": "https://api.github.com/repos/kengz/SLM-Lab/tarball/v4.1.1", "zipball_url": "https://api.github.com/repos/kengz/SLM-Lab/zipball/v4.1.1", "body": "## Discrete SAC benchmark update\r\n\r\n- [Upload PR #429](https://github.com/kengz/SLM-Lab/pull/429)\r\n- [Dropbox data](https://www.dropbox.com/s/az4vncwwktyotol/benchmark_discrete_2019_09.zip?dl=0)\r\n\r\n||||||||\r\n|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\r\n| Env. \\ Alg. | DQN | DDQN+PER | A2C (GAE) | A2C (n-step) | PPO | SAC |\r\n| Breakout <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67737546-dabb6380-f9c8-11e9-901e-b96cc28f1fdf.png\"></details> | 80.88 | 182 | 377 | 398 | **443** | 3.51* |\r\n| Pong <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67737554-e018ae00-f9c8-11e9-92b5-3bd8d213b1e0.png\"></details> | 18.48 | 20.5 | 19.31 | 19.56 | **20.58** | 19.87* |\r\n| Seaquest <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67737557-e3139e80-f9c8-11e9-9446-119593ca956b.png\"></details> | 1185 | **4405** | 1070 | 1684 | 1715 | 171* |\r\n| Qbert <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67737559-e575f880-f9c8-11e9-8c98-f14c82041a45.png\"></details> | 5494 | 11426 | 12405 | **13590** | 13460 | 923* |\r\n| LunarLander <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67737566-e7d85280-f9c8-11e9-8df8-39c1205c5308.png\"></details> | 192 | 233 | 25.21 | 68.23 | 214 | **276** |\r\n| UnityHallway <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67737569-ead34300-f9c8-11e9-9e26-61fe1d779989.png\"></details> | -0.32 | 0.27 | 0.08 | -0.96 | **0.73** | 0.01 |\r\n| UnityPushBlock <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67737577-eeff6080-f9c8-11e9-931c-843ba697779c.png\"></details> | 4.88 | 4.93 | 4.68 | 4.93 | **4.97** | -0.70 |\r\n\r\n>Episode score at the end of training attained by SLM Lab implementations on discrete-action control problems. Reported episode scores are the average over the last 100 checkpoints, and then averaged over 4 Sessions. A Random baseline with score averaged over 100 episodes is included. Results marked with `*` were trained using the hybrid synchronous/asynchronous version of SAC to parallelize and speed up training time. For SAC, Breakout, Pong and Seaquest were trained for 2M frames instead of 10M frames.\r\n\r\n>For the full Atari benchmark, see [Atari Benchmark](https://github.com/kengz/SLM-Lab/blob/benchmark/BENCHMARK.md#atari-benchmark)"}
{"url": "https://api.github.com/repos/kengz/SLM-Lab/releases/21042489", "assets_url": "https://api.github.com/repos/kengz/SLM-Lab/releases/21042489/assets", "upload_url": "https://uploads.github.com/repos/kengz/SLM-Lab/releases/21042489/assets{?name,label}", "html_url": "https://github.com/kengz/SLM-Lab/releases/tag/v4.1.0", "id": 21042489, "author": {"login": "kengz", "id": 8209263, "node_id": "MDQ6VXNlcjgyMDkyNjM=", "avatar_url": "https://avatars.githubusercontent.com/u/8209263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kengz", "html_url": "https://github.com/kengz", "followers_url": "https://api.github.com/users/kengz/followers", "following_url": "https://api.github.com/users/kengz/following{/other_user}", "gists_url": "https://api.github.com/users/kengz/gists{/gist_id}", "starred_url": "https://api.github.com/users/kengz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kengz/subscriptions", "organizations_url": "https://api.github.com/users/kengz/orgs", "repos_url": "https://api.github.com/users/kengz/repos", "events_url": "https://api.github.com/users/kengz/events{/privacy}", "received_events_url": "https://api.github.com/users/kengz/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTIxMDQyNDg5", "tag_name": "v4.1.0", "target_commitish": "master", "name": "RAdam+Lookahead optim, TensorBoard, Full Benchmark Upload", "draft": false, "prerelease": false, "created_at": "2019-10-29T05:10:58Z", "published_at": "2019-10-29T05:11:21Z", "assets": [], "tarball_url": "https://api.github.com/repos/kengz/SLM-Lab/tarball/v4.1.0", "zipball_url": "https://api.github.com/repos/kengz/SLM-Lab/zipball/v4.1.0", "body": "This marks a stable release of SLM Lab with full benchmark results\r\n\r\n## RAdam+Lookahead optimizer\r\n\r\n- [Lookahead](https://arxiv.org/abs/1907.08610) + [RAdam](https://arxiv.org/abs/1908.03265) optimizer significantly improves the performance of some RL algorithms (A2C (n-step), PPO) on continuous domain problems, but does not improve (A2C (GAE), SAC). #416\r\n\r\n## TensorBoard\r\n\r\n- Add TensorBoard in body to auto-log summary variables, graph, network parameter histograms, action histogram. To launch TensorBoard, run `tensorboard --logdir=data` after a session/trial is completed. Example screenshot:\r\n\r\n<img width=\"1423\" alt=\"Screen Shot 2019-10-14 at 10 41 36 PM\" src=\"https://user-images.githubusercontent.com/8209263/66803221-d9bc0980-eed3-11e9-92b8-0e5cd42a6eab.png\">\r\n\r\n## Full Benchmark Upload\r\n\r\n#### Plot Legend\r\n\r\n><img width=\"400\" alt=\"legend\" src=\"https://user-images.githubusercontent.com/8209263/67737544-d727dc80-f9c8-11e9-904a-319b9aafd41b.png\">\r\n\r\n\r\n### Discrete Benchmark\r\n\r\n- [Upload PR #427](https://github.com/kengz/SLM-Lab/pull/427)\r\n- [Dropbox data](https://www.dropbox.com/s/az4vncwwktyotol/benchmark_discrete_2019_09.zip?dl=0)\r\n\r\n||||||||\r\n|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\r\n| Env. \\ Alg. | DQN | DDQN+PER | A2C (GAE) | A2C (n-step) | PPO | SAC |\r\n| Breakout <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67737546-dabb6380-f9c8-11e9-901e-b96cc28f1fdf.png\"></details> | 80.88 | 182 | 377 | 398 | **443** |  - |\r\n| Pong <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67737554-e018ae00-f9c8-11e9-92b5-3bd8d213b1e0.png\"></details> | 18.48 | 20.5 | 19.31 | 19.56 | **20.58** | 19.87* |\r\n| Seaquest <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67737557-e3139e80-f9c8-11e9-9446-119593ca956b.png\"></details> | 1185 | **4405** | 1070 | 1684 | 1715 |  - |\r\n| Qbert <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67737559-e575f880-f9c8-11e9-8c98-f14c82041a45.png\"></details> | 5494 | 11426 | 12405 | **13590** | 13460 | 214* |\r\n| LunarLander <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67737566-e7d85280-f9c8-11e9-8df8-39c1205c5308.png\"></details> | 192 | 233 | 25.21 | 68.23 | 214 | **276** |\r\n| UnityHallway <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67737569-ead34300-f9c8-11e9-9e26-61fe1d779989.png\"></details> | -0.32 | 0.27 | 0.08 | -0.96 | **0.73** | - |\r\n| UnityPushBlock <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67737577-eeff6080-f9c8-11e9-931c-843ba697779c.png\"></details> | 4.88 | 4.93 | 4.68 | 4.93 | **4.97** | - |\r\n\r\n>Episode score at the end of training attained by SLM Lab implementations on discrete-action control problems. Reported episode scores are the average over the last 100 checkpoints, and then averaged over 4 Sessions. Results marked with `*` were trained using the hybrid synchronous/asynchronous version of SAC to parallelize and speed up training time.\r\n\r\n>For the full Atari benchmark, see [Atari Benchmark](https://github.com/kengz/SLM-Lab/blob/benchmark/BENCHMARK.md#atari-benchmark)\r\n\r\n### Continuous Benchmark\r\n\r\n- [Upload PR #427](https://github.com/kengz/SLM-Lab/pull/427)\r\n- [Dropbox data](https://www.dropbox.com/s/xaxybertpwt4s9j/benchmark_continuous_2019_09.zip?dl=0)\r\n\r\n||||||\r\n|:---:|:---:|:---:|:---:|:---:|\r\n| Env. \\ Alg. | A2C (GAE) | A2C (n-step) | PPO | SAC |\r\n| RoboschoolAnt <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67737923-1571cb80-f9ca-11e9-8f6b-b288fa19bff0.png\"></details> | 787 | 1396 | 1843 | **2915** |\r\n| RoboschoolAtlasForwardWalk <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67737924-1571cb80-f9ca-11e9-98ee-82c920dfbf44.png\"></details> | 59.87 | 88.04 | 172 | **800** |\r\n| RoboschoolHalfCheetah <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67737925-1571cb80-f9ca-11e9-9c7f-3a8294a517af.png\"></details> | 712 | 439 | 1960 | **2497** |\r\n| RoboschoolHopper <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67737926-160a6200-f9ca-11e9-8cae-9afc532e5af8.png\"></details> | 710 | 285 | 2042 | **2045** |\r\n| RoboschoolInvertedDoublePendulum <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67737927-160a6200-f9ca-11e9-8eb2-e04554e3844f.png\"></details> | 996 | 4410 | 8076 | **8085** |\r\n| RoboschoolInvertedPendulum <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67737928-160a6200-f9ca-11e9-8eae-e7a3ccbe914a.png\"></details> | **995** | 978 | 986 | 941 |\r\n| RoboschoolReacher <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67737929-160a6200-f9ca-11e9-9423-b27165def32e.png\"></details> | 12.9 | 10.16 | 19.51 | **19.99** |\r\n| RoboschoolWalker2d <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67737930-160a6200-f9ca-11e9-9a0f-edbd4f01f4e0.png\"></details> | 280 | 220 | 1660 | **1894** |\r\n| RoboschoolHumanoid <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67737931-16a2f880-f9ca-11e9-9340-fe90ab48e95f.png\"></details> | 99.31 | 54.58 | 2388 | **2621*** |\r\n| RoboschoolHumanoidFlagrun <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67737932-16a2f880-f9ca-11e9-92bb-9c896ec3991e.png\"></details> | 73.57 | 178 | 2014 | **2056*** |\r\n| RoboschoolHumanoidFlagrunHarder <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67737933-16a2f880-f9ca-11e9-98c8-7388fa9e1775.png\"></details> | -429 | 253 | **680** | 280* |\r\n| Unity3DBall <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67737934-16a2f880-f9ca-11e9-912b-37c8840d0acc.png\"></details> | 33.48 | 53.46 | 78.24 | **98.44** |\r\n| Unity3DBallHard <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67737935-16a2f880-f9ca-11e9-9275-f3b5fef22e1b.png\"></details> | 62.92 | 71.92 | 91.41 | **97.06** |\r\n\r\n>Episode score at the end of training attained by SLM Lab implementations on continuous control problems. Reported episode scores are the average over the last 100 checkpoints, and then averaged over 4 Sessions. Results marked with `*` require 50M-100M frames, so we use the hybrid synchronous/asynchronous version of SAC to parallelize and speed up training time.\r\n\r\n### Atari Benchmark\r\n\r\n- [Upload PR #427](https://github.com/kengz/SLM-Lab/pull/427)\r\n- [Dropbox data: DQN](https://www.dropbox.com/s/5hg78znvmi41ys5/benchmark_dqn_atari_2019_09.zip?dl=0)\r\n- [Dropbox data: DDQN+PER](https://www.dropbox.com/s/s8pgset1ewi0da1/benchmark_ddqn_per_atari_2019_09.zip?dl=0)\r\n- [Dropbox data: A2C (GAE)](https://www.dropbox.com/s/kbqw9a5f0e55y0y/benchmark_a2c_gae_atari_2019_09.zip?dl=0)\r\n- [Dropbox data: A2C (n-step)](https://www.dropbox.com/s/jc1xzd4uru6tksd/benchmark_a2c_nstep_atari_2019_09.zip?dl=0)\r\n- [Dropbox data: PPO](https://www.dropbox.com/s/o42fsnfoef5y9zl/benchmark_ppo_atari_2019_09.zip?dl=0)\r\n- [Dropbox data: all Atari graphs](https://www.dropbox.com/s/odxxr2cquw4bcfj/benchmark_atari_graphs_2019_09.zip?dl=0)\r\n\r\n|||||||\r\n|:---:|:---:|:---:|:---:|:---:|:---:|\r\n| Env. \\ Alg. | DQN | DDQN+PER | A2C (GAE) | A2C (n-step) | PPO |\r\n| Adventure <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738131-d6904580-f9ca-11e9-8818-0d027b668a97.png\"></details> | -0.94 | -0.92 | -0.77 | -0.85 | **-0.3** |\r\n| AirRaid <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738132-d6904580-f9ca-11e9-9585-41f69fd8bb33.png\"></details> | 1876 | 3974 | **4202** | 3557 | 4028 |\r\n| Alien <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738133-d6904580-f9ca-11e9-8375-4c134255cfe1.png\"></details> | 822 | 1574 | 1519 | **1627** | 1413 |\r\n| Amidar <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738134-d6904580-f9ca-11e9-865c-eb41f4e712f9.png\"></details> | 90.95 | 431 | 577 | 418 | **795** |\r\n| Assault <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738135-d6904580-f9ca-11e9-8f8d-61732ecc3ce4.png\"></details> | 1392 | 2567 | 3366 | 3312 | **3619** |\r\n| Asterix <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738138-d6904580-f9ca-11e9-86c0-3589622a311c.png\"></details> | 1253 | **6866** | 5559 | 5223 | 6132 |\r\n| Asteroids <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738139-d728dc00-f9ca-11e9-8741-e9a59883197e.png\"></details> | 439 | 426 | **2951** | 2147 | 2186 |\r\n| Atlantis <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738140-d728dc00-f9ca-11e9-9649-ecc4b2db782f.png\"></details> | 68679 | 644810 | **2747371** | 2259733 | 2148077 |\r\n| BankHeist <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738141-d728dc00-f9ca-11e9-924a-a02be1639ee6.png\"></details> | 131 | 623 | 855 | 1170 | **1183** |\r\n| BattleZone <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738142-d728dc00-f9ca-11e9-82b0-382bbb0bcc6c.png\"></details> | 6564 | 6395 | 4336 | 4533 | **13649** |\r\n| BeamRider <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738143-d728dc00-f9ca-11e9-84eb-2ec8988ff545.png\"></details> | 2799 | **5870** | 2659 | 4139 | 4299 |\r\n| Berzerk <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738144-d728dc00-f9ca-11e9-83c6-2e50a69b4ed3.png\"></details> | 319 | 401 | **1073** | 763 | 860 |\r\n| Bowling <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738145-d7c17280-f9ca-11e9-9a2e-bc179e3186f4.png\"></details> | 30.29 | **39.5** | 24.51 | 23.75 | 31.64 |\r\n| Boxing <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738146-d7c17280-f9ca-11e9-95ac-008f35834ed1.png\"></details> | 72.11 | 90.98 | 1.57 | 1.26 | **96.53** |\r\n| Breakout <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738147-d7c17280-f9ca-11e9-890e-319a21e036e0.png\"></details> | 80.88 | 182 | 377 | 398 | **443** |\r\n| Carnival <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738148-d7c17280-f9ca-11e9-95e9-58309efb8ee4.png\"></details> | 4280 | **4773** | 2473 | 1827 | 4566 |\r\n| Centipede <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738150-d7c17280-f9ca-11e9-8a27-3cc7160c1e60.png\"></details> | 1899 | 2153 | 3909 | 4202 | **5003** |\r\n| ChopperCommand <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738151-d7c17280-f9ca-11e9-8316-90cf4e944e97.png\"></details> | 1083 | **4020** | 3043 | 1280 | 3357 |\r\n| CrazyClimber <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738152-d85a0900-f9ca-11e9-8b48-1a988dc31627.png\"></details> | 46984 | 88814 | 106256 | 109998 | **116820** |\r\n| Defender <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738153-d85a0900-f9ca-11e9-8b30-750fc49b25dd.png\"></details> | 281999 | 313018 | **665609** | 657823 | 534639 |\r\n| DemonAttack <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738154-d85a0900-f9ca-11e9-8e5e-e99b336e6fbb.png\"></details> | 1705 | 19856 | 23779 | 19615 | **121172** |\r\n| DoubleDunk <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738155-d85a0900-f9ca-11e9-8fd4-e94d1be4a6ee.png\"></details> | -21.44 | -22.38 | **-5.15** | -13.3 | -6.01 |\r\n| ElevatorAction <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738156-d85a0900-f9ca-11e9-9006-903a9c823230.png\"></details> | 32.62 | 17.91 | **9966** | 8818 | 6471 |\r\n| Enduro <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738158-d85a0900-f9ca-11e9-8167-ebc713c59fdc.png\"></details> | 437 | 959 | 787 | 0.0 | **1926** |\r\n| FishingDerby <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738159-d8f29f80-f9ca-11e9-9166-ebe3ea5339ab.png\"></details> | -88.14 | -1.7 | 16.54 | 1.65 | **36.03** |\r\n| Freeway <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738161-d8f29f80-f9ca-11e9-9727-2584ac850507.png\"></details> | 24.46 | 30.49 | 30.97 | 0.0 | **32.11** |\r\n| Frostbite <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738163-d8f29f80-f9ca-11e9-9d36-1cb7985360ac.png\"></details> | 98.8 | **2497** | 277 | 261 | 1062 |\r\n| Gopher <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738164-d8f29f80-f9ca-11e9-8ba3-fb1d75ef81f1.png\"></details> | 1095 | **7562** | 929 | 1545 | 2933 |\r\n| Gravitar <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738166-d8f29f80-f9ca-11e9-9d57-c02118eba7c1.png\"></details> | 87.34 | 258 | 313 | **433** | 223 |\r\n| Hero <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738167-d8f29f80-f9ca-11e9-9faf-2c30048c8621.png\"></details> | 1051 | 12579 | 16502 | **19322** | 17412 |\r\n| IceHockey <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738168-d98b3600-f9ca-11e9-8695-8014fd177416.png\"></details> | -14.96 | -14.24 | **-5.79** | -6.06 | -6.43 |\r\n| Jamesbond <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738170-d98b3600-f9ca-11e9-9f4a-25929639efc1.png\"></details> | 44.87 | **702** | 521 | 453 | 561 |\r\n| JourneyEscape <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738171-d98b3600-f9ca-11e9-9679-15a1586719dd.png\"></details> | -4818 | -2003 | **-921** | -2032 | -1094 |\r\n| Kangaroo <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738172-d98b3600-f9ca-11e9-9770-3d63043a716b.png\"></details> | 1965 | **8897** | 67.62 | 554 | 4989 |\r\n| Krull <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738173-d98b3600-f9ca-11e9-9244-0933adbfedd8.png\"></details> | 5522 | 6650 | 7785 | 6642 | **8477** |\r\n| KungFuMaster <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738174-d98b3600-f9ca-11e9-95e3-33621db77541.png\"></details> | 2288 | 16547 | 31199 | 25554 | **34523** |\r\n| MontezumaRevenge <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738175-da23cc80-f9ca-11e9-81cf-58e16e210b5e.png\"></details> | 0.0 | 0.02 | 0.08 | 0.19 | **1.08** |\r\n| MsPacman <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738176-da23cc80-f9ca-11e9-8906-d54475705442.png\"></details> | 1175 | 2215 | 1965 | 2158 | **2350** |\r\n| NameThisGame <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738177-da23cc80-f9ca-11e9-9093-0a0e2456fb4c.png\"></details> | 3915 | 4474 | 5178 | 5795 | **6386** |\r\n| Phoenix <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738178-da23cc80-f9ca-11e9-93a1-188c75b888f6.png\"></details> | 2909 | 8179 | 16345 | 13586 | **30504** |\r\n| Pitfall <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738179-da23cc80-f9ca-11e9-8c76-0d339ac0034a.png\"></details> | -68.83 | -73.65 | -101 | **-31.13** | -35.93 |\r\n| Pong <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738180-dabc6300-f9ca-11e9-826b-3d72cd0b13a0.png\"></details> | 18.48 | 20.5 | 19.31 | 19.56 | **20.58** |\r\n| Pooyan <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738181-dabc6300-f9ca-11e9-922e-0b13b973a4d9.png\"></details> | 1958 | 2741 | 2862 | 2531 | **6799** |\r\n| PrivateEye <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738182-dabc6300-f9ca-11e9-87b3-072ce2637405.png\"></details> | **784** | 303 | 93.22 | 78.07 | 50.12 |\r\n| Qbert <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738183-dabc6300-f9ca-11e9-8ab1-d66c6b12cd2f.png\"></details> | 5494 | 11426 | 12405 | **13590** | 13460 |\r\n| Riverraid <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738184-dabc6300-f9ca-11e9-82fb-d6b7f7f0d696.png\"></details> | 953 | **10492** | 8308 | 7565 | 9636 |\r\n| RoadRunner <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738185-dabc6300-f9ca-11e9-9291-1303718c9a50.png\"></details> | 15237 | 29047 | 30152 | 31030 | **32956** |\r\n| Robotank <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738186-db54f980-f9ca-11e9-8aef-41c9a3250d8c.png\"></details> | 3.43 | **9.05** | 2.98 | 2.27 | 2.27 |\r\n| Seaquest <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738187-db54f980-f9ca-11e9-9764-da60d54e1406.png\"></details> | 1185 | **4405** | 1070 | 1684 | 1715 |\r\n| Skiing <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738188-db54f980-f9ca-11e9-9966-1f22f57a96e0.png\"></details> | -14094 | **-12883** | -19481 | -14234 | -24713 |\r\n| Solaris <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738190-db54f980-f9ca-11e9-84c6-8bc1313e1e96.png\"></details> | 612 | 1396 | 2115 | **2236** | 1892 |\r\n| SpaceInvaders <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738191-dbed9000-f9ca-11e9-84e9-ec324d7b2544.png\"></details> | 451 | 670 | 733 | 750 | **797** |\r\n| StarGunner <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738193-dbed9000-f9ca-11e9-9d01-42865df8ca1e.png\"></details> | 3565 | 38238 | 44816 | 48410 | **60579** |\r\n| Tennis <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738194-dbed9000-f9ca-11e9-84c4-aaf8c59371a2.png\"></details> | -23.78 | **-10.33** | -22.42 | -19.06 | -11.52 |\r\n| TimePilot <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738195-dbed9000-f9ca-11e9-8bea-33ed2428afe2.png\"></details> | 2819 | 1884 | 3331 | 3440 | **4398** |\r\n| Tutankham <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738196-dc862680-f9ca-11e9-8beb-144e4fb4b36d.png\"></details> | 35.03 | 159 | 161 | 175 | **211** |\r\n| UpNDown <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738197-dc862680-f9ca-11e9-9903-d1eb924f56e2.png\"></details> | 2043 | 11632 | 89769 | 18878 | **262208** |\r\n| Venture <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738198-dc862680-f9ca-11e9-8c37-04e057822a20.png\"></details> | 4.56 | 9.61 | 0.0 | 0.0 | **11.84** |\r\n| VideoPinball <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738199-dc862680-f9ca-11e9-9ab3-50064bd5112c.png\"></details> | 8056 | **79730** | 35371 | 40423 | 58096 |\r\n| WizardOfWor <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738200-dc862680-f9ca-11e9-8722-67a664dbbf10.png\"></details> | 869 | 328 | 1516 | 1247 | **4283** |\r\n| YarsRevenge <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738201-dd1ebd00-f9ca-11e9-9c27-3a8dd8c13953.png\"></details> | 5816 | 15698 | **27097** | 11742 | 10114 |\r\n| Zaxxon <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/67738202-dd1ebd00-f9ca-11e9-98bd-f737a02107f9.png\"></details> | 442 | 54.28 | 64.72 | 24.7 | **641** |\r\n\r\n>The table above presents results for 62 Atari games. All agents were trained for 10M frames (40M including skipped frames). Reported results are the episode score at the end of training, averaged over the previous 100 evaluation checkpoints with each checkpoint averaged over 4 Sessions. Agents were checkpointed every 10k training frames.\r\n"}
{"url": "https://api.github.com/repos/kengz/SLM-Lab/releases/19216832", "assets_url": "https://api.github.com/repos/kengz/SLM-Lab/releases/19216832/assets", "upload_url": "https://uploads.github.com/repos/kengz/SLM-Lab/releases/19216832/assets{?name,label}", "html_url": "https://github.com/kengz/SLM-Lab/releases/tag/v4.0.1", "id": 19216832, "author": {"login": "kengz", "id": 8209263, "node_id": "MDQ6VXNlcjgyMDkyNjM=", "avatar_url": "https://avatars.githubusercontent.com/u/8209263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kengz", "html_url": "https://github.com/kengz", "followers_url": "https://api.github.com/users/kengz/followers", "following_url": "https://api.github.com/users/kengz/following{/other_user}", "gists_url": "https://api.github.com/users/kengz/gists{/gist_id}", "starred_url": "https://api.github.com/users/kengz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kengz/subscriptions", "organizations_url": "https://api.github.com/users/kengz/orgs", "repos_url": "https://api.github.com/users/kengz/repos", "events_url": "https://api.github.com/users/kengz/events{/privacy}", "received_events_url": "https://api.github.com/users/kengz/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTE5MjE2ODMy", "tag_name": "v4.0.1", "target_commitish": "master", "name": "v4.0.1: Soft Actor-Critic", "draft": false, "prerelease": false, "created_at": "2019-08-11T18:13:45Z", "published_at": "2019-08-11T18:14:14Z", "assets": [], "tarball_url": "https://api.github.com/repos/kengz/SLM-Lab/tarball/v4.0.1", "zipball_url": "https://api.github.com/repos/kengz/SLM-Lab/zipball/v4.0.1", "body": "This release adds a new algorithm: Soft Actor-Critic (SAC).\r\n\r\n## Soft Actor-Critic\r\n-implement the original paper: \"Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\" https://arxiv.org/abs/1801.01290 #398 \r\n- implement the improvement of SAC paper: \"Soft Actor-Critic Algorithms and Applications\" https://arxiv.org/abs/1812.05905 #399 \r\n- extend SAC to work directly for discrete environment using `GumbelSoftmax` distribution (custom)\r\n\r\n### Roboschool (continuous control) Benchmark\r\n\r\n>Note that the Roboschool reward scales are different from MuJoCo's.\r\n\r\n| Env. \\ Alg. | SAC |\r\n|:---|---|\r\n| RoboschoolAnt | 2451.55 <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/62837481-c1eead80-bc24-11e9-913e-7685d64ecf87.png\"></details> |\r\n| RoboschoolHalfCheetah | 2004.27 <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/62837485-daf75e80-bc24-11e9-8fba-279802ccdd1d.png\"></details> |\r\n| RoboschoolHopper | 2090.52 <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/62837491-e8144d80-bc24-11e9-9d06-27a35b4aacca.png\"></details> |\r\n| RoboschoolWalker2d | 1711.92 <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/62837495-f2364c00-bc24-11e9-8bdc-fa88831c227b.png\"></details> |\r\n\r\n\r\n### LunarLander (discrete control) Benchmark\r\n\r\n| | |\r\n|---|---|\r\n|![sac_lunar_t0_trial_graph_mean_returns_vs_frames](https://user-images.githubusercontent.com/8209263/62837421-0cbbf580-bc24-11e9-89db-4a0da92b27b8.png)|![sac_lunar_t0_trial_graph_mean_returns_ma_vs_frames](https://user-images.githubusercontent.com/8209263/62837420-0cbbf580-bc24-11e9-9214-2efb14778014.png)|\r\n| Trial graph | Moving average |\r\n\r\n"}
{"url": "https://api.github.com/repos/kengz/SLM-Lab/releases/17595696", "assets_url": "https://api.github.com/repos/kengz/SLM-Lab/releases/17595696/assets", "upload_url": "https://uploads.github.com/repos/kengz/SLM-Lab/releases/17595696/assets{?name,label}", "html_url": "https://github.com/kengz/SLM-Lab/releases/tag/v4.0.0", "id": 17595696, "author": {"login": "kengz", "id": 8209263, "node_id": "MDQ6VXNlcjgyMDkyNjM=", "avatar_url": "https://avatars.githubusercontent.com/u/8209263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kengz", "html_url": "https://github.com/kengz", "followers_url": "https://api.github.com/users/kengz/followers", "following_url": "https://api.github.com/users/kengz/following{/other_user}", "gists_url": "https://api.github.com/users/kengz/gists{/gist_id}", "starred_url": "https://api.github.com/users/kengz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kengz/subscriptions", "organizations_url": "https://api.github.com/users/kengz/orgs", "repos_url": "https://api.github.com/users/kengz/repos", "events_url": "https://api.github.com/users/kengz/events{/privacy}", "received_events_url": "https://api.github.com/users/kengz/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTE3NTk1Njk2", "tag_name": "v4.0.0", "target_commitish": "master", "name": "v4.0.0: Algorithm Benchmark, Analysis, API simplification", "draft": false, "prerelease": false, "created_at": "2019-07-31T17:18:59Z", "published_at": "2019-07-31T17:19:25Z", "assets": [], "tarball_url": "https://api.github.com/repos/kengz/SLM-Lab/tarball/v4.0.0", "zipball_url": "https://api.github.com/repos/kengz/SLM-Lab/zipball/v4.0.0", "body": "This release corrects and optimizes all the algorithms from benchmarking on Atari. New metrics are introduced. The lab's API is also redesigned for simplicity.\r\n\r\n## Benchmark\r\n- full algorithm benchmark on 4 core Atari environments #396\r\n- LunarLander benchmark #388 and BipedalWalker benchmark #377\r\n\r\nThis benchmark table is pulled from [PR396](https://github.com/kengz/SLM-Lab/pull/396). See the full [benchmark results here](https://github.com/kengz/SLM-Lab/blob/master/BENCHMARK.md).\r\n\r\n| Env. \\ Alg. | A2C (GAE) | A2C (n-step) | PPO | DQN | DDQN+PER |\r\n|:---|---|---|---|---|---|\r\n| Breakout <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/62232119-554cf680-b37a-11e9-9059-3e49bbb799d2.png\"><img src=\"https://user-images.githubusercontent.com/8209263/62232118-554cf680-b37a-11e9-9d5b-dd2ddf527305.png\"></details> | 389.99 <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/62019989-0171c000-b176-11e9-94da-017b146afe65.png\"></details> | 391.32 <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/62020340-6c6fc680-b177-11e9-8aa1-9ac5c2001783.png\"></details> | **425.89** <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/62067085-c0b28f00-b1e7-11e9-9dd5-c52b6104878f.png\"></details> | 65.04 <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/62100441-9ba13900-b246-11e9-9373-95c6063915ab.png\"></details> | 181.72 <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/62230967-dd7dcc80-b377-11e9-965b-60a9f3d5a7a1.png\"></details> |\r\n| Pong <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/62232135-5b42d780-b37a-11e9-9454-ff2d109ef4f4.png\"><img src=\"https://user-images.githubusercontent.com/8209263/62232134-5b42d780-b37a-11e9-892f-a84ea8881e78.png\"></details> | 20.04 <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/62020247-10a53d80-b177-11e9-9f0d-1433d4d87210.png\"></details> | 19.66 <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/62020342-6f6ab700-b177-11e9-824e-75f431dc14ec.png\"></details> | 20.09 <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/62067100-c6a87000-b1e7-11e9-919e-ad68e4166213.png\"></details> | 18.34 <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/62100450-9fcd5680-b246-11e9-8170-2ad4473e8294.png\"></details> | **20.44** <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/62230975-e2428080-b377-11e9-8970-6917ae80c0b4.png\"></details> |\r\n| Qbert <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/62232149-60078b80-b37a-11e9-99bb-cedc9fe064d5.png\"><img src=\"https://user-images.githubusercontent.com/8209263/62232148-60078b80-b37a-11e9-9610-17ac447a479f.png\"></details> | 13,328.32 <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/62020263-261a6780-b177-11e9-8936-22a74d2405d3.png\"></details> | 13,259.19 <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/62020347-742f6b00-b177-11e9-8bfb-edfcfd44c8b7.png\"></details> | **13,691.89** <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/62067104-cb6d2400-b1e7-11e9-9c4f-9eaac265d7d6.png\"></details> | 4,787.79 <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/62100455-a4920a80-b246-11e9-8ca5-d4dc1ce3d76f.png\"></details> | 11,673.52 <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/62230986-e79fcb00-b377-11e9-8861-3686954b7e1a.png\"></details> |\r\n| Seaquest <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/62232168-6bf34d80-b37a-11e9-9564-fa3609dc5c75.png\"><img src=\"https://user-images.githubusercontent.com/8209263/62232167-6bf34d80-b37a-11e9-8db3-c79a0e78292b.png\"></details> | 892.68 <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/62020266-29adee80-b177-11e9-83c2-fafbdbb982b9.png\"></details> | 1,686.08 <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/62020350-772a5b80-b177-11e9-8917-e3c8a745cd08.png\"></details> | 1,583.04 <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/62067113-cf994180-b1e7-11e9-870b-b9bba71f2a7e.png\"></details> | 1,118.50 <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/62100462-a9ef5500-b246-11e9-8699-9356ff81ff93.png\"></details> | **3,751.34** <details><summary><i>graph</i></summary><img src=\"https://user-images.githubusercontent.com/8209263/62230991-ebcbe880-b377-11e9-8de4-a01379d1d61c.png\"></details> |\r\n\r\n## Algorithms\r\n- correct and optimize all algorithms with benchmarking #315 #327 #328 #361 \r\n- introduce \"shared\" and \"synced\" Hogwild modes for distributed training #337 #340\r\n- streamline and optimize agent components too\r\n\r\nNow, the full list of algorithms are:\r\n- SARSA\r\n- DQN, distributed-DQN\r\n- Double-DQN, Dueling-DQN, PER-DQN\r\n- REINFORCE\r\n- A2C, A3C (N-step & GAE)\r\n- PPO, distributed-PPO\r\n- SIL (A2C, PPO)\r\nAll the algorithms can be ran in distributed mode also; which in some cases they have their special names (mentioned above)\r\n\r\n## Environments\r\n- implement vector environments #302\r\n- implement more environment wrappers for preprocessing. Some replay memories are retired. #303 #330 #331 #342 \r\n- make Lab Env wrapper interface identical to gym #304, #305, #306, #307\r\n\r\n## API\r\n- all the Space objects (AgentSpace, EnvSpace, AEBSpace, InfoSpace) are retired, to opt for a much simpler interface. #335 #348 \r\n- major API simplification throughout\r\n\r\n## Analysis\r\n- rework analysis, introduce new metrics: strength, sample efficiency, training efficiency, stability, consistency #347 #349 \r\n- fast evaluation using vectorized env for `rigorous_eval` #390 , and using inference for fast eval #391 \r\n\r\n## Search\r\n- update and rework Ray search #350 #351\r\n"}
{"url": "https://api.github.com/repos/kengz/SLM-Lab/releases/16829442", "assets_url": "https://api.github.com/repos/kengz/SLM-Lab/releases/16829442/assets", "upload_url": "https://uploads.github.com/repos/kengz/SLM-Lab/releases/16829442/assets{?name,label}", "html_url": "https://github.com/kengz/SLM-Lab/releases/tag/v3.2.1", "id": 16829442, "author": {"login": "kengz", "id": 8209263, "node_id": "MDQ6VXNlcjgyMDkyNjM=", "avatar_url": "https://avatars.githubusercontent.com/u/8209263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kengz", "html_url": "https://github.com/kengz", "followers_url": "https://api.github.com/users/kengz/followers", "following_url": "https://api.github.com/users/kengz/following{/other_user}", "gists_url": "https://api.github.com/users/kengz/gists{/gist_id}", "starred_url": "https://api.github.com/users/kengz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kengz/subscriptions", "organizations_url": "https://api.github.com/users/kengz/orgs", "repos_url": "https://api.github.com/users/kengz/repos", "events_url": "https://api.github.com/users/kengz/events{/privacy}", "received_events_url": "https://api.github.com/users/kengz/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTE2ODI5NDQy", "tag_name": "v3.2.1", "target_commitish": "master", "name": "Improve installation, functions, add out layer activation", "draft": false, "prerelease": false, "created_at": "2019-04-17T16:41:55Z", "published_at": "2019-04-17T16:49:44Z", "assets": [], "tarball_url": "https://api.github.com/repos/kengz/SLM-Lab/tarball/v3.2.1", "zipball_url": "https://api.github.com/repos/kengz/SLM-Lab/zipball/v3.2.1", "body": "## Improve installation\r\n- #288 split out yarn installation as extra step\r\n\r\n## Improve functions\r\n- #283 #284 redesign fitness slightly\r\n- #281 simplify PER sample index\r\n- #287 #290 improve DQN polyak and network switching\r\n- #291 refactor advantage functions\r\n- #295 #296 refactor various utils, fix PyTorch inplace ops\r\n\r\n## Add out layer activation\r\n- #300 add out layer activation"}
{"url": "https://api.github.com/repos/kengz/SLM-Lab/releases/15366937", "assets_url": "https://api.github.com/repos/kengz/SLM-Lab/releases/15366937/assets", "upload_url": "https://uploads.github.com/repos/kengz/SLM-Lab/releases/15366937/assets{?name,label}", "html_url": "https://github.com/kengz/SLM-Lab/releases/tag/v3.2.0", "id": 15366937, "author": {"login": "kengz", "id": 8209263, "node_id": "MDQ6VXNlcjgyMDkyNjM=", "avatar_url": "https://avatars.githubusercontent.com/u/8209263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kengz", "html_url": "https://github.com/kengz", "followers_url": "https://api.github.com/users/kengz/followers", "following_url": "https://api.github.com/users/kengz/following{/other_user}", "gists_url": "https://api.github.com/users/kengz/gists{/gist_id}", "starred_url": "https://api.github.com/users/kengz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kengz/subscriptions", "organizations_url": "https://api.github.com/users/kengz/orgs", "repos_url": "https://api.github.com/users/kengz/repos", "events_url": "https://api.github.com/users/kengz/events{/privacy}", "received_events_url": "https://api.github.com/users/kengz/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTE1MzY2OTM3", "tag_name": "v3.2.0", "target_commitish": "master", "name": "Rework eval mode; major refactoring", "draft": false, "prerelease": false, "created_at": "2019-02-05T08:09:54Z", "published_at": "2019-02-05T08:10:23Z", "assets": [], "tarball_url": "https://api.github.com/repos/kengz/SLM-Lab/tarball/v3.2.0", "zipball_url": "https://api.github.com/repos/kengz/SLM-Lab/zipball/v3.2.0", "body": "## Eval rework\r\n\r\n#275 #278 #279 #280 \r\n\r\nThis release adds an eval mode that is the same as OpenAI baseline. Spawn 2 environments, 1 for training and 1 more eval. In the same process (blocking), run training as usual, then at ckpt, run an episode on eval env and update stats.\r\n\r\nThe logic for the stats are the same as before, except the original `body.df` is now split into two: `body.train_df` and `body.eval_df`. Eval df uses the main env stats except for `t, reward` to reflect progress on eval env. Correspondingly, session analysis also produces both versions of data.\r\n\r\nData from `body.eval_df` is used to generate `session_df, session_graph, session_fitness_df`, whereas the data from `body.train_df` is used to generate a new set of `trainsession_df, trainsession_graph, trainsession_fitness_df` for debugging.\r\n\r\nThe previous process-based eval functionality is kept, but is now considered as `parallel_eval`. This can be useful for more robust checkpointing and eval.\r\n\r\n## Refactoring\r\n#279 \r\n\r\n- purge useless computations\r\n- properly and efficiently gather and organize all update variable computations.\r\n\r\nThis also speeds up run time by x2. For Atari Beamrider with DQN on V100 GPU, manual benchmark measurement gives 110 FPS for training every 4 frames, while eval achieves 160 FPS. This translates to 10M frames in roughly 24 hours."}
{"url": "https://api.github.com/repos/kengz/SLM-Lab/releases/15076929", "assets_url": "https://api.github.com/repos/kengz/SLM-Lab/releases/15076929/assets", "upload_url": "https://uploads.github.com/repos/kengz/SLM-Lab/releases/15076929/assets{?name,label}", "html_url": "https://github.com/kengz/SLM-Lab/releases/tag/v3.1.1", "id": 15076929, "author": {"login": "kengz", "id": 8209263, "node_id": "MDQ6VXNlcjgyMDkyNjM=", "avatar_url": "https://avatars.githubusercontent.com/u/8209263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kengz", "html_url": "https://github.com/kengz", "followers_url": "https://api.github.com/users/kengz/followers", "following_url": "https://api.github.com/users/kengz/following{/other_user}", "gists_url": "https://api.github.com/users/kengz/gists{/gist_id}", "starred_url": "https://api.github.com/users/kengz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kengz/subscriptions", "organizations_url": "https://api.github.com/users/kengz/orgs", "repos_url": "https://api.github.com/users/kengz/repos", "events_url": "https://api.github.com/users/kengz/events{/privacy}", "received_events_url": "https://api.github.com/users/kengz/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTE1MDc2OTI5", "tag_name": "v3.1.1", "target_commitish": "master", "name": "v3.1.1: Add Retro Eval, fix Boltzmann spec", "draft": false, "prerelease": false, "created_at": "2019-01-20T19:37:27Z", "published_at": "2019-01-20T19:38:09Z", "assets": [], "tarball_url": "https://api.github.com/repos/kengz/SLM-Lab/tarball/v3.1.1", "zipball_url": "https://api.github.com/repos/kengz/SLM-Lab/zipball/v3.1.1", "body": ">[**Docker image `kengz/slm_lab:v3.0.0` released**](https://hub.docker.com/r/kengz/slm_lab/tags/)\r\n\r\n## Add Retro Eval\r\n- #270 add retro eval mode to run fail online eval sessions. Use command `yarn retro_eval data/reinforce_cartpole_2018_01_22_211751`\r\n- #272 #273 fix eval saving 0 index to `eval_session_df` causing trial analysis to break; add reset_index for safety\r\n\r\n## fix Boltzmann spec\r\n- #271 change Boltzmann spec to use Categorical instead of the wrong Argmax\r\n\r\n## misc\r\n- #273 update colorlover package to proper pip after they fixed division error\r\n- #274 remove unused torchvision package to lighten build"}
{"url": "https://api.github.com/repos/kengz/SLM-Lab/releases/14820063", "assets_url": "https://api.github.com/repos/kengz/SLM-Lab/releases/14820063/assets", "upload_url": "https://uploads.github.com/repos/kengz/SLM-Lab/releases/14820063/assets{?name,label}", "html_url": "https://github.com/kengz/SLM-Lab/releases/tag/v3.1.0", "id": 14820063, "author": {"login": "kengz", "id": 8209263, "node_id": "MDQ6VXNlcjgyMDkyNjM=", "avatar_url": "https://avatars.githubusercontent.com/u/8209263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kengz", "html_url": "https://github.com/kengz", "followers_url": "https://api.github.com/users/kengz/followers", "following_url": "https://api.github.com/users/kengz/following{/other_user}", "gists_url": "https://api.github.com/users/kengz/gists{/gist_id}", "starred_url": "https://api.github.com/users/kengz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kengz/subscriptions", "organizations_url": "https://api.github.com/users/kengz/orgs", "repos_url": "https://api.github.com/users/kengz/repos", "events_url": "https://api.github.com/users/kengz/events{/privacy}", "received_events_url": "https://api.github.com/users/kengz/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTE0ODIwMDYz", "tag_name": "v3.1.0", "target_commitish": "master", "name": "v3.1.0: L1 fitness norm, code and spec refactor, online eval", "draft": false, "prerelease": false, "created_at": "2019-01-09T06:11:42Z", "published_at": "2019-01-09T06:11:57Z", "assets": [], "tarball_url": "https://api.github.com/repos/kengz/SLM-Lab/tarball/v3.1.0", "zipball_url": "https://api.github.com/repos/kengz/SLM-Lab/zipball/v3.1.0", "body": "# v3.1.0: L1 fitness norm, code and spec refactor, online eval\r\n\r\n>[**Docker image `kengz/slm_lab:v3.1.0` released**](https://hub.docker.com/r/kengz/slm_lab/tags/)\r\n\r\n## L1 fitness norm (breaking change)\r\n- change fitness vector norm from L2 to L1 for intuitiveness and non-extreme values\r\n\r\n## code and spec refactor\r\n- #254 PPO cleanup: remove hack and restore minimization scheme\r\n- #255 remove `use_gae` and `use_nstep` param to infer from `lam, num_step_returns`\r\n- #260 fix decay `start_step` offset, add unit tests for rate decay methods\r\n- #262 make epi start from 0 instead of 1 for code logic consistency\r\n- #264 switch `max_total_t`, `max_epi` to `max_tick` and `max_tick_unit` for directness. retire `graph_x` for the unit above\r\n- #266 add Atari fitness std, fix CUDA coredump issue\r\n- #269 update gym, remove box2d hack\r\n\r\n## Online Eval mode\r\n#252 #257 #261 #267\r\nEvaluation sessions during training on a subprocess. This does not interfere with the training process, but spawns multiple subprocesses to do independent evaluation, which then adds to an eval file, and at the end a final eval will finish and plot all the graphs and save all the data for eval.\r\n\r\n- enabled by meta spec `'training_eval'`\r\n- configure `NUM_EVAL_EPI` in `analysis.py`\r\n- update `enjoy` and `eval` mode syntax. see README.\r\n- change ckpt behavior to use e.g. tag `ckpt-epi10-totalt1000`\r\n- add new `eval` mode to lab. runs on a checkpoint file. see below\r\n\r\n### Eval Session\r\n- add a proper eval Session which loads from the ckpt like above, and does not interfere with existing files. This can be ran on terminal, and it's also used by the internal eval logic, e.g. command `python run_lab.py data/dqn_cartpole_2018_12_20_214412/dqn_cartpole_t0_spec.json dqn_cartpole eval@dqn_cartpole_t0_s2_ckpt-epi10-totalt1000`\r\n- when eval session is done, it will average all of its ran episodes and append to a row in an `eval_session_df.csv`\r\n- after that it will delete the ckpt files it had just used (to prevent large storage)\r\n- then, it will run a trial analysis to update `eval_trial_graph.png`, and an accompanying `trial_df` as average of all `session_df`s\r\n\r\n### How eval mode works\r\n- checkpoint will save the models using the scheme which records its `epi` and `total_t`. This allows one to eval using the ckpt model\r\n- after creating ckpt files, if `spec.meta.training_eval in `train` mode, a subprocess will launch using the ckpt prepath to run an eval Session, using the same way above `python run_lab.py data/dqn_cartpole_2018_12_20_214412/dqn_cartpole_t0_spec.json dqn_cartpole eval@dqn_cartpole_t0_s2_ckpt-epi10-totalt1000`\r\n- eval session runs as above. ckpt will now run at the starting timestep, ckpt timestep, and at the end\r\n- the main Session will wait for the final eval session and it's final eval trial to finish before closing, to ensure that other processes like zipping wait for them.\r\n\r\nExample eval trial graph:\r\n\r\n![dqn_cartpole_t0_ckpt-eval_trial_graph](https://user-images.githubusercontent.com/8209263/50327674-eaaf1080-04a4-11e9-8586-1ca025aec3e0.png)\r\n"}
{"url": "https://api.github.com/repos/kengz/SLM-Lab/releases/14286600", "assets_url": "https://api.github.com/repos/kengz/SLM-Lab/releases/14286600/assets", "upload_url": "https://uploads.github.com/repos/kengz/SLM-Lab/releases/14286600/assets{?name,label}", "html_url": "https://github.com/kengz/SLM-Lab/releases/tag/v3.0.0", "id": 14286600, "author": {"login": "kengz", "id": 8209263, "node_id": "MDQ6VXNlcjgyMDkyNjM=", "avatar_url": "https://avatars.githubusercontent.com/u/8209263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kengz", "html_url": "https://github.com/kengz", "followers_url": "https://api.github.com/users/kengz/followers", "following_url": "https://api.github.com/users/kengz/following{/other_user}", "gists_url": "https://api.github.com/users/kengz/gists{/gist_id}", "starred_url": "https://api.github.com/users/kengz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kengz/subscriptions", "organizations_url": "https://api.github.com/users/kengz/orgs", "repos_url": "https://api.github.com/users/kengz/repos", "events_url": "https://api.github.com/users/kengz/events{/privacy}", "received_events_url": "https://api.github.com/users/kengz/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTE0Mjg2NjAw", "tag_name": "v3.0.0", "target_commitish": "master", "name": "V3: PyTorch 1.0, faster NN, Variable Scheduler, working Atari", "draft": false, "prerelease": false, "created_at": "2018-12-03T00:55:30Z", "published_at": "2018-12-03T00:55:56Z", "assets": [], "tarball_url": "https://api.github.com/repos/kengz/SLM-Lab/tarball/v3.0.0", "zipball_url": "https://api.github.com/repos/kengz/SLM-Lab/zipball/v3.0.0", "body": "# V3: PyTorch 1.0, faster Neural Network, Variable Scheduler\r\n\r\n>[**Docker image `kengz/slm_lab:v3.0.0` released**](https://hub.docker.com/r/kengz/slm_lab/tags/)\r\n\r\nPRs included #240 #241 #239 #238 #244 #248 \r\n\r\n## PyTorch 1.0 and parallel CUDA\r\n- switch to PyTorch 1.0 with various improvements and parallel CUDA fix\r\n\r\n## new Neural Network API (breaking changes)\r\nTo accommodate more advanced features and improvements, all the networks have been improved with better spec and code design, faster operations, and added features\r\n- single-tail networks will now not use list but a single tail for fast output compute (for loop is slow)\r\n- use PyTorch `optim.lr_scheduler` for learning rate decay. retire old methods.\r\n- more efficient spec format for network, `clip_grad`, `lr_scheduler_spec`\r\n- fix and add proper generalization for ConvNet and RecurrentNet\r\n- add full basic network unit tests\r\n\r\n## DQN\r\n- rewrite DQN loss for 2x speedup and code simplicity. extend to SARSA\r\n- retire MultitaskDQN for HydraDQN\r\n\r\n## Memory\r\n- add `OnpolicyConcatReplay`\r\n- standardize `preprocess_state` logic in onpolicy memories\r\n\r\n## Variable Scheduler (breaking spec changes)\r\n- implement variable decay class `VarScheduler` similar to pytorch's LR scheduler. use clock with flexible scheduling units `epi` or `total_t`\r\n- unify VarScheduler to use standard `clock.max_tick_unit` specified from env\r\n- retire `action_policy_update`, update agent spec to `explore_var_spec`\r\n- replace `entropy_coef` with `entropy_coef_spec`\r\n- replace `clip_eps` with `clip_eps_spec` (PPO)\r\n- update all specs\r\n\r\n## Math util\r\n- move decay methods to `math_util.py`\r\n- move `math_util.py` from `algorithm/` to `lib/`\r\n\r\n## env max tick (breaking spec changes)\r\n- spec/variable renamings:\r\n  - `max_episode` to `max_epi`\r\n  - `max_timestep` to `max_t`\r\n  - `save_epi_frequency` to `save_frequency`\r\n  - `traininig_min_timestep` to `training_start_step`\r\n- allow env to stop based on `max_epi` as well as `max_total_t`. propagate clock unit usage\r\n- introduce `max_tick, max_tick_unit` properties to env and clock from above\r\n- allow `save_frequency` to use the same units accordingly\r\n- update Pong and Beamrider to use `max_total_t` as end-condition\r\n\r\n## Update Ray to reenable CUDA in search\r\n- update ray from `0.3.1` to `0.5.3` to address broken GPU with pytorch 1.0.0\r\n- to fix CUDA not discovered in Ray worker, have to manually set CUDA devices at ray remote function due to poor design.\r\n\r\n## Improved logging and Enjoy mode\r\n#243 #245\r\n- Best models checkpointing measured using the the reward_ma\r\n- Early termination if the environment is solved\r\n- method for logging learning rate to session data frame needed to be updated after move to PyTorch lr_scheduler\r\n- Also removed training_net from the mean learning rate reported in the session dataframe since the learning rate doesn't change\r\n- update naming scheme to work with enjoy mode\r\n- unify and simplify prepath methods\r\n- info_space now uses a `ckpt` for loading ckpt model. Example usage: `yarn start pong.json dqn_pong enjoy@data/dqn_cartpole_2018_12_02_124127/dqn_cartpole_t0_s0_ckptbest`\r\n- update agent load and policy to properly set variables to `end_val` in enjoy mode\r\n- random-seed env as well\r\n\r\n## Working Atari\r\n#242 \r\nAtari benchmark had been failing, but the root cause had finally been discovered and fix: wrong image preprocessing. This can be due to several factors, and we are doing ablation studies to check against the old code: - Image normalization cause the input values to be lowered by ~255, and the resultant loss is too small for optimizer.\r\n- blackframes in stacking at the beginning timesteps\r\n- wrong image permutation\r\n\r\nPR #242 introduces:\r\n- global environment preprocessor in the form of env wrapper borrowed from OpenAI baselines, in `env/wrapper.py`\r\n- a `TransformImage` to do the proper image transform: grayscale, downsize, and **shape from w,h,c to PyTorch format c,h,w**\r\n- a `FrameStack` which uses `LazyFrames` for efficiency to replace the agent-specific Atari stack frame preprocessing. This simplifies the Atari memories\r\n- update convnet to use honest shape (c,h,w) without extra transform, and remove its expensive image axis permutation since input now is in the right shape\r\n- update Vizdoom to produce (c,h,w) shape consistent with convnet input expectation\r\n\r\nTuned parameters will be obtained and released next version.\r\n\r\nAttached is a quick training curve on Pong, DQN, where the solution avg is +18:\r\n![fast_dqn_pong_t0_s0_session_graph](https://user-images.githubusercontent.com/8209263/49337610-c610f880-f5ca-11e8-9957-0fb53cf7fba8.png)\r\n![pong](https://user-images.githubusercontent.com/8209263/49346161-07dd8580-f643-11e8-975c-38972465a587.gif)"}
{"url": "https://api.github.com/repos/kengz/SLM-Lab/releases/13813720", "assets_url": "https://api.github.com/repos/kengz/SLM-Lab/releases/13813720/assets", "upload_url": "https://uploads.github.com/repos/kengz/SLM-Lab/releases/13813720/assets{?name,label}", "html_url": "https://github.com/kengz/SLM-Lab/releases/tag/v2.2.0", "id": 13813720, "author": {"login": "kengz", "id": 8209263, "node_id": "MDQ6VXNlcjgyMDkyNjM=", "avatar_url": "https://avatars.githubusercontent.com/u/8209263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kengz", "html_url": "https://github.com/kengz", "followers_url": "https://api.github.com/users/kengz/followers", "following_url": "https://api.github.com/users/kengz/following{/other_user}", "gists_url": "https://api.github.com/users/kengz/gists{/gist_id}", "starred_url": "https://api.github.com/users/kengz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kengz/subscriptions", "organizations_url": "https://api.github.com/users/kengz/orgs", "repos_url": "https://api.github.com/users/kengz/repos", "events_url": "https://api.github.com/users/kengz/events{/privacy}", "received_events_url": "https://api.github.com/users/kengz/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTEzODEzNzIw", "tag_name": "v2.2.0", "target_commitish": "master", "name": "VizDoom, NN Weight Init, Plotly Update", "draft": false, "prerelease": false, "created_at": "2018-11-03T18:02:08Z", "published_at": "2018-11-03T18:02:50Z", "assets": [], "tarball_url": "https://api.github.com/repos/kengz/SLM-Lab/tarball/v2.2.0", "zipball_url": "https://api.github.com/repos/kengz/SLM-Lab/zipball/v2.2.0", "body": "## Add VizDoom environment\r\n#222 #224 \r\n- add new `OnPolicyImageReplay` and `ImageReplay` memories\r\n- add [VizDoom](http://vizdoom.cs.put.edu.pl/)  environment, thanks to @joelouismarino\r\n\r\n## Add NN Weight Initialization functionality\r\n#223 #225 \r\n- allow specification of NN weight init function in spec, thanks to @mwcvitkovic\r\n\r\n## Update Plotly to v3\r\n#221 \r\n- move to v3 to allow Python based (instead of bash) image saving for stability\r\n\r\n## Fixes\r\n- #207 fix PPO loss function broken during refactoring\r\n- #217 fix multi-device CUDA parallelization in grad assignment"}
{"url": "https://api.github.com/repos/kengz/SLM-Lab/releases/13171665", "assets_url": "https://api.github.com/repos/kengz/SLM-Lab/releases/13171665/assets", "upload_url": "https://uploads.github.com/repos/kengz/SLM-Lab/releases/13171665/assets{?name,label}", "html_url": "https://github.com/kengz/SLM-Lab/releases/tag/v2.1.2", "id": 13171665, "author": {"login": "kengz", "id": 8209263, "node_id": "MDQ6VXNlcjgyMDkyNjM=", "avatar_url": "https://avatars.githubusercontent.com/u/8209263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kengz", "html_url": "https://github.com/kengz", "followers_url": "https://api.github.com/users/kengz/followers", "following_url": "https://api.github.com/users/kengz/following{/other_user}", "gists_url": "https://api.github.com/users/kengz/gists{/gist_id}", "starred_url": "https://api.github.com/users/kengz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kengz/subscriptions", "organizations_url": "https://api.github.com/users/kengz/orgs", "repos_url": "https://api.github.com/users/kengz/repos", "events_url": "https://api.github.com/users/kengz/events{/privacy}", "received_events_url": "https://api.github.com/users/kengz/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTEzMTcxNjY1", "tag_name": "v2.1.2", "target_commitish": "master", "name": "Benchmarking; Reward Scaling; HydraDQN", "draft": false, "prerelease": false, "created_at": "2018-10-02T06:05:56Z", "published_at": "2018-10-02T06:06:44Z", "assets": [], "tarball_url": "https://api.github.com/repos/kengz/SLM-Lab/tarball/v2.1.2", "zipball_url": "https://api.github.com/repos/kengz/SLM-Lab/zipball/v2.1.2", "body": "## Benchmark\r\n- #177 #183 zip experiment data file for easy upload\r\n- #178 #186 #188 #194 add benchmark spec files\r\n- #193 add benchmark standard data to compute fitness\r\n- #196 add benchmark mode\r\n\r\n## Reward scaling\r\n- #175 add environment-specific reward scaling\r\n\r\n## HydraDQN\r\n- #175 HydraDQN works on cartpole and 2dball using reward scaling. spec committed\r\n\r\n## Add code of conduct\r\n- #199 add a code of conduct file for community\r\n\r\n## Misc\r\n- #172 add MA reward to dataframe\r\n- #174 refactor session parallelization\r\n- #196 add sys args to run lab\r\n- #198 add `train@` mode\r\n"}
{"url": "https://api.github.com/repos/kengz/SLM-Lab/releases/12917973", "assets_url": "https://api.github.com/repos/kengz/SLM-Lab/releases/12917973/assets", "upload_url": "https://uploads.github.com/repos/kengz/SLM-Lab/releases/12917973/assets{?name,label}", "html_url": "https://github.com/kengz/SLM-Lab/releases/tag/v2.1.1", "id": 12917973, "author": {"login": "kengz", "id": 8209263, "node_id": "MDQ6VXNlcjgyMDkyNjM=", "avatar_url": "https://avatars.githubusercontent.com/u/8209263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kengz", "html_url": "https://github.com/kengz", "followers_url": "https://api.github.com/users/kengz/followers", "following_url": "https://api.github.com/users/kengz/following{/other_user}", "gists_url": "https://api.github.com/users/kengz/gists{/gist_id}", "starred_url": "https://api.github.com/users/kengz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kengz/subscriptions", "organizations_url": "https://api.github.com/users/kengz/orgs", "repos_url": "https://api.github.com/users/kengz/repos", "events_url": "https://api.github.com/users/kengz/events{/privacy}", "received_events_url": "https://api.github.com/users/kengz/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTEyOTE3OTcz", "tag_name": "v2.1.1", "target_commitish": "master", "name": "Distributed CUDA; DQN replace; AtariPrioritizedReplay", "draft": false, "prerelease": false, "created_at": "2018-09-15T18:27:58Z", "published_at": "2018-09-15T18:30:37Z", "assets": [], "tarball_url": "https://api.github.com/repos/kengz/SLM-Lab/tarball/v2.1.1", "zipball_url": "https://api.github.com/repos/kengz/SLM-Lab/zipball/v2.1.1", "body": "## Enable Distributed CUDA\r\n#170 \r\nFix the long standing pytorch + distributed using `spawn` multiprocessing due to Lab classes not pickleable. Just let the class wrapped in a `mp_runner` passed as `mp.Process(target=mp_runner, args)` so the classes don't get cloned from memory when spawning process, since it is now passed from outside.\r\n\r\n## DQN replace method fix\r\n#169 \r\nDQN target network replacement was in the wrong direction. Fix that.\r\n\r\n## AtariPrioritizedReplay\r\n#170 #171 \r\nAdd a quick AtariPrioritizedReplay via some multi-inheritance black magic with `PrioritizedReplay, AtariReplay`"}
{"url": "https://api.github.com/repos/kengz/SLM-Lab/releases/12800089", "assets_url": "https://api.github.com/repos/kengz/SLM-Lab/releases/12800089/assets", "upload_url": "https://uploads.github.com/repos/kengz/SLM-Lab/releases/12800089/assets{?name,label}", "html_url": "https://github.com/kengz/SLM-Lab/releases/tag/v2.1.0", "id": 12800089, "author": {"login": "kengz", "id": 8209263, "node_id": "MDQ6VXNlcjgyMDkyNjM=", "avatar_url": "https://avatars.githubusercontent.com/u/8209263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kengz", "html_url": "https://github.com/kengz", "followers_url": "https://api.github.com/users/kengz/followers", "following_url": "https://api.github.com/users/kengz/following{/other_user}", "gists_url": "https://api.github.com/users/kengz/gists{/gist_id}", "starred_url": "https://api.github.com/users/kengz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kengz/subscriptions", "organizations_url": "https://api.github.com/users/kengz/orgs", "repos_url": "https://api.github.com/users/kengz/repos", "events_url": "https://api.github.com/users/kengz/events{/privacy}", "received_events_url": "https://api.github.com/users/kengz/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTEyODAwMDg5", "tag_name": "v2.1.0", "target_commitish": "master", "name": "Remove Data Space History, Optimize Memory", "draft": false, "prerelease": false, "created_at": "2018-09-09T06:35:30Z", "published_at": "2018-09-09T06:37:13Z", "assets": [], "tarball_url": "https://api.github.com/repos/kengz/SLM-Lab/tarball/v2.1.0", "zipball_url": "https://api.github.com/repos/kengz/SLM-Lab/zipball/v2.1.0", "body": "This release optimizes the RAM consumption and memory sampling speed after stress-testing with Atari. RAM growth is curbed, and replay memory RAM usage is now near theoretical optimality.\r\n\r\nThanks to @mwcvitkovic for providing major help with this release.\r\n\r\n## Remove DataSpace history\r\n#163\r\n- debug and fix memory growth (cause: data space saving history)\r\n- remove history saving altogether, and mdp data. remove aeb `add_single`. This changes the API.\r\n- create `body.df` to track data efficiently as a replacement. This is the API replacement for above.\r\n\r\n## Optimize Replay Memory RAM\r\n#163 first optimization, halves replay RAM\r\n- make memory state numpy storage `float16` to accommodate big memory size. half a million max_size virtual memory goes from 200GB to 50GB\r\n- memory index sampling for training with large size is very slow. add a method `fast_uniform_sampling` to speed up\r\n\r\n#165 second optimization, halves replay RAM again to the theoretical minimum\r\n- do not save `next_states` for replay memories due to redundancy\r\n- replace with sentinel `self.latest_next_states` during sampling\r\n- 1 mil max_size for Atari replay now consumes 50Gb instead of 100Gb (was 200Gb before float16 downcasting in #163 )\r\n\r\n## Add OnPolicyAtariReplay\r\n#164 \r\n- add OnPolicyAtariReplay memory so that policy based algorithms can be applied to the Atari suite.\r\n\r\n## Misc\r\n- #157 allow usage as a python module via `pip install -e .` or `python setup.py install`\r\n- #160 guard lab `default.json` creation on first install\r\n- #161 fix agent save method, improve logging\r\n- #162 split logger by session for easier debugging\r\n- #164 fix N-Step-returns calculation\r\n- #166 fix pandas weird casting breaking issue causing process to hang\r\n- #167 uninstall unused tensorflow and tensorboard that come with Unity ML-Agents. rebuild Docker image.\r\n- #168 rebuild Docker and CI images\r\n"}
{"url": "https://api.github.com/repos/kengz/SLM-Lab/releases/12706272", "assets_url": "https://api.github.com/repos/kengz/SLM-Lab/releases/12706272/assets", "upload_url": "https://uploads.github.com/repos/kengz/SLM-Lab/releases/12706272/assets{?name,label}", "html_url": "https://github.com/kengz/SLM-Lab/releases/tag/v2.0.0", "id": 12706272, "author": {"login": "kengz", "id": 8209263, "node_id": "MDQ6VXNlcjgyMDkyNjM=", "avatar_url": "https://avatars.githubusercontent.com/u/8209263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kengz", "html_url": "https://github.com/kengz", "followers_url": "https://api.github.com/users/kengz/followers", "following_url": "https://api.github.com/users/kengz/following{/other_user}", "gists_url": "https://api.github.com/users/kengz/gists{/gist_id}", "starred_url": "https://api.github.com/users/kengz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kengz/subscriptions", "organizations_url": "https://api.github.com/users/kengz/orgs", "repos_url": "https://api.github.com/users/kengz/repos", "events_url": "https://api.github.com/users/kengz/events{/privacy}", "received_events_url": "https://api.github.com/users/kengz/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTEyNzA2Mjcy", "tag_name": "v2.0.0", "target_commitish": "master", "name": "v2.0.0 Singleton Mode, CUDA Support, Distributed Training", "draft": false, "prerelease": false, "created_at": "2018-09-03T20:26:18Z", "published_at": "2018-09-03T20:28:49Z", "assets": [], "tarball_url": "https://api.github.com/repos/kengz/SLM-Lab/tarball/v2.0.0", "zipball_url": "https://api.github.com/repos/kengz/SLM-Lab/zipball/v2.0.0", "body": "This major v2.0.0 release addresses the user feedbacks on usability and feature requests:\r\n- makes the singleton case (single-agent-env) default\r\n- adds CUDA GPU support for all algorithms (except for distributed)\r\n- adds distributed training to all algorithms (ala A3C style)\r\n- optimizes compute, fixes some computation bugs\r\n\r\n*Note that this release is backward-incompatible with v1.x. and earlier.*\r\n\r\n`v2.0.0`: make components independent of the framework so it can be used outside of SLM-Lab for development and production, and improve usability. **Backward-incompatible with `v1.x`.**\r\n\r\n## Singleton Mode as Default\r\n#153 \r\n- **singleton** case (single-agent-env-body) is now the default. Any implementations need only to worry about singleton. Uses the `Session` in lab.\r\n- **space** case (multi-agent-env-body) is now an extension from singleton case. Simply add `space_{method}` to handle the space logic. Uses the `SpaceSession` in lab.\r\n- make components more independent from framework\r\n- major logic simplification to improve usability. Simplify the AEB and init sequences. remove `post_body_init()`\r\n- make network update and grad norm check more robust\r\n\r\n## CUDA support\r\n#153 \r\n- add attribute `Net.cuda_id` for device assignment (per network basis), and auto-calculate the `cuda_id` by trial and session index to distribute jobs\r\n- enable CUDA and **add GPU support for all algorithms**, except for distributed (A3C, DPPO etc.)\r\n- properly assign tensors to CUDA automatically depending if GPU is available and desired\r\n- run unit tests on machine with GTX 1070\r\n\r\n## Distributed Training\r\n#153 #148 \r\n- add `distributed` key to meta spec\r\n- enable distributed training using pytorch multiprocessing. Create new `DistSession` class which acts as the worker.\r\n- In distributed training, `Trial` creates the global networks for agents, then passes to and spawns `DistSession`. Effectively, the semantics of a session changes from being a disjoint copy to being a training worker.\r\n- make distributed usable for both singleton (single agent) and space (multiagent) cases.\r\n- add distributed cases to unit tests\r\n\r\n## State Normalization\r\n#155 \r\n- add state normalization using running mean and std: `state = (state - mean) / std`\r\n- apply to all algorithms\r\n- TODO conduct a large scale systematic study of the effect is state normalization vs without it\r\n\r\n## Bug Fixes and Improvements\r\n#153 \r\n- `save()` and `load()` now include network optimizers\r\n- refactor `set_manual_seed` to util\r\n- rename `StackReplay` to `ConcatReplay` for clarity\r\n- improve network training check of weights and grad norms\r\n- introduce `BaseEnv` as base class to `OpenAIEnv` and `UnityEnv`\r\n- optimize computations, major refactoring\r\n- update Dockerfile and release\r\n\r\n### Misc\r\n- #155 add state normalization using running mean and std\r\n- #154 fix A2C advantage calculation for Nstep returns\r\n- #152 refactor SIL implementation using multi-inheritance\r\n- #151 refactor Memory module\r\n- #150 refactor Net module\r\n- #147 update grad clipping, norm check, multicategorical API\r\n- #156 fix multiprocessing for device with cuda, without using cuda\r\n- #156 fix multi policy arguments to be consistent, and add missing state append logic\r\n"}
{"url": "https://api.github.com/repos/kengz/SLM-Lab/releases/12301606", "assets_url": "https://api.github.com/repos/kengz/SLM-Lab/releases/12301606/assets", "upload_url": "https://uploads.github.com/repos/kengz/SLM-Lab/releases/12301606/assets{?name,label}", "html_url": "https://github.com/kengz/SLM-Lab/releases/tag/v1.1.2", "id": 12301606, "author": {"login": "kengz", "id": 8209263, "node_id": "MDQ6VXNlcjgyMDkyNjM=", "avatar_url": "https://avatars.githubusercontent.com/u/8209263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kengz", "html_url": "https://github.com/kengz", "followers_url": "https://api.github.com/users/kengz/followers", "following_url": "https://api.github.com/users/kengz/following{/other_user}", "gists_url": "https://api.github.com/users/kengz/gists{/gist_id}", "starred_url": "https://api.github.com/users/kengz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kengz/subscriptions", "organizations_url": "https://api.github.com/users/kengz/orgs", "repos_url": "https://api.github.com/users/kengz/repos", "events_url": "https://api.github.com/users/kengz/events{/privacy}", "received_events_url": "https://api.github.com/users/kengz/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTEyMzAxNjA2", "tag_name": "v1.1.2", "target_commitish": "master", "name": "PPOSIL, fix continuous actions and PPO", "draft": false, "prerelease": false, "created_at": "2018-08-08T06:18:56Z", "published_at": "2018-08-08T06:19:45Z", "assets": [], "tarball_url": "https://api.github.com/repos/kengz/SLM-Lab/tarball/v1.1.2", "zipball_url": "https://api.github.com/repos/kengz/SLM-Lab/zipball/v1.1.2", "body": "This release adds PPOSIL, fixes some small issues with continuous actions, and PPO ratio computation.\r\n\r\n## Implementations\r\n#145 Implement PPOSIL. Improve debug logging\r\n#143 add Arch installer thanks to @angel-ayala \r\n\r\n## Bug Fixes\r\n#138 kill hanging processes of Electron for plotting\r\n#145 fix PPO wrong graph update sequence causing ratio to be 1. Fix continuous action output construction. add guards.\r\n#146 fix continuous actions and add full tests\r\n"}
{"url": "https://api.github.com/repos/kengz/SLM-Lab/releases/11686200", "assets_url": "https://api.github.com/repos/kengz/SLM-Lab/releases/11686200/assets", "upload_url": "https://uploads.github.com/repos/kengz/SLM-Lab/releases/11686200/assets{?name,label}", "html_url": "https://github.com/kengz/SLM-Lab/releases/tag/v1.1.1", "id": 11686200, "author": {"login": "kengz", "id": 8209263, "node_id": "MDQ6VXNlcjgyMDkyNjM=", "avatar_url": "https://avatars.githubusercontent.com/u/8209263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kengz", "html_url": "https://github.com/kengz", "followers_url": "https://api.github.com/users/kengz/followers", "following_url": "https://api.github.com/users/kengz/following{/other_user}", "gists_url": "https://api.github.com/users/kengz/gists{/gist_id}", "starred_url": "https://api.github.com/users/kengz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kengz/subscriptions", "organizations_url": "https://api.github.com/users/kengz/orgs", "repos_url": "https://api.github.com/users/kengz/repos", "events_url": "https://api.github.com/users/kengz/events{/privacy}", "received_events_url": "https://api.github.com/users/kengz/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTExNjg2MjAw", "tag_name": "v1.1.1", "target_commitish": "master", "name": "add SIL, fix PG loss bug, add dueling networks", "draft": false, "prerelease": false, "created_at": "2018-06-28T07:46:27Z", "published_at": "2018-06-28T07:46:58Z", "assets": [], "tarball_url": "https://api.github.com/repos/kengz/SLM-Lab/tarball/v1.1.1", "zipball_url": "https://api.github.com/repos/kengz/SLM-Lab/zipball/v1.1.1", "body": "This release adds some new implementations, and fixes some bugs from first benchmark runs.\r\n\r\n## Implementations\r\n#127 Self-Imitation Learning\r\n#128 Checkpointing for saving models\r\n#129 Dueling Networks\r\n\r\n## Bug Fixes\r\n#132 GPU test-run fixes\r\n#133 fix ActorCritic family loss compute getting detached, and linux plotting issues, add SHA to generated specs"}
{"url": "https://api.github.com/repos/kengz/SLM-Lab/releases/11518224", "assets_url": "https://api.github.com/repos/kengz/SLM-Lab/releases/11518224/assets", "upload_url": "https://uploads.github.com/repos/kengz/SLM-Lab/releases/11518224/assets{?name,label}", "html_url": "https://github.com/kengz/SLM-Lab/releases/tag/v1.1.0", "id": 11518224, "author": {"login": "kengz", "id": 8209263, "node_id": "MDQ6VXNlcjgyMDkyNjM=", "avatar_url": "https://avatars.githubusercontent.com/u/8209263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kengz", "html_url": "https://github.com/kengz", "followers_url": "https://api.github.com/users/kengz/followers", "following_url": "https://api.github.com/users/kengz/following{/other_user}", "gists_url": "https://api.github.com/users/kengz/gists{/gist_id}", "starred_url": "https://api.github.com/users/kengz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kengz/subscriptions", "organizations_url": "https://api.github.com/users/kengz/orgs", "repos_url": "https://api.github.com/users/kengz/repos", "events_url": "https://api.github.com/users/kengz/events{/privacy}", "received_events_url": "https://api.github.com/users/kengz/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTExNTE4MjI0", "tag_name": "v1.1.0", "target_commitish": "master", "name": "v1.1.0 full roadmap algorithms and features", "draft": false, "prerelease": false, "created_at": "2018-06-19T03:23:59Z", "published_at": "2018-06-19T03:24:54Z", "assets": [], "tarball_url": "https://api.github.com/repos/kengz/SLM-Lab/tarball/v1.1.0", "zipball_url": "https://api.github.com/repos/kengz/SLM-Lab/zipball/v1.1.0", "body": "- [**v1.1.0 Roadmap**](https://github.com/kengz/SLM-Lab/projects/1)\r\n- [**v1.1.0 Docker release**](https://hub.docker.com/r/kengz/slm_lab/tags/)\r\n\r\n## Canonical Algorithms and Components\r\n\r\n>This release is research-ready.\r\n\r\nFinish implementation of all canonical algorithms and components. All design is fully refactored and usable across components as suitable. This release is ready for research. Read the [updated doc](https://kengz.gitbooks.io/slm-lab/content/)\r\n\r\nSLM Lab implements most of the recent canonical algorithms and various extensions. These are used as the base of research.\r\n\r\n#### Algorithm\r\n\r\ncode: [slm_lab/agent/algorithm](https://github.com/kengz/SLM-Lab/tree/master/slm_lab/agent/algorithm)\r\n\r\nVarious algorithms are in fact extensions of some simpler ones, and they are implemented as such. This makes the code very concise.\r\n\r\n**Policy Gradient:**\r\n- REINFORCE\r\n- AC (Vanilla Actor-Critic)\r\n    - shared or separate actor critic networks\r\n    - plain TD\r\n    - entropy term control\r\n- A2C (Advantage Actor-Critic)\r\n    - extension of AC with with advantage function\r\n    - N-step returns as advantage\r\n    - GAE (Generalized Advantage Estimate) as advantage\r\n- PPO (Proximal Policy Optimization)\r\n    - extension of A3C with PPO loss function\r\n\r\n**Value-based:**\r\n- SARSA\r\n- DQN (Deep Q Learning)\r\n    - boltzmann or epsilon-greedy policy\r\n- DRQN (Recurrent DQN)\r\n- Double DQN\r\n- Double DRQN\r\n- Multitask DQN (multi-environment DQN)\r\n- Hydra DQN (multi-environment DQN)\r\n\r\nBelow are the modular building blocks for the algorithms. They are designed to be general, and are reused extensively.\r\n\r\n#### Memory\r\n\r\ncode: [slm_lab/agent/memory](https://github.com/kengz/SLM-Lab/tree/master/slm_lab/agent/memory)\r\n\r\nFor on-policy algorithms (policy gradient):\r\n- OnPolicyReplay\r\n- OnPolicySeqReplay\r\n- OnPolicyBatchReplay\r\n- OnPolicyBatchSeqReplay\r\n\r\nFor off-policy algorithms (value-based)\r\n- Replay\r\n- SeqReplay\r\n- StackReplay\r\n- AtariReplay\r\n- PrioritizedReplay\r\n\r\n#### Neural Network\r\n\r\ncode: [slm_lab/agent/net](https://github.com/kengz/SLM-Lab/tree/master/slm_lab/agent/net)\r\n\r\nThese networks are usable for all algorithms.\r\n\r\n- MLPNet (Multi Layer Perceptron)\r\n- MLPHeterogenousTails (multi-tails)\r\n- HydraMLPNet (multi-heads, multi-tails)\r\n- RecurrentNet\r\n- ConvNet\r\n\r\n#### Policy\r\n\r\ncode: [slm_lab/agent/algorithm/policy_util.py](https://github.com/kengz/SLM-Lab/blob/master/slm_lab/agent/algorithm/policy_util.py)\r\n\r\n- different probability distributions for sampling actions\r\n- default policy\r\n- Boltzmann policy\r\n- Epsilon-greedy policy\r\n- numerous rate decay methods\r\n"}
{"url": "https://api.github.com/repos/kengz/SLM-Lab/releases/11030348", "assets_url": "https://api.github.com/repos/kengz/SLM-Lab/releases/11030348/assets", "upload_url": "https://uploads.github.com/repos/kengz/SLM-Lab/releases/11030348/assets{?name,label}", "html_url": "https://github.com/kengz/SLM-Lab/releases/tag/v1.0.3", "id": 11030348, "author": {"login": "kengz", "id": 8209263, "node_id": "MDQ6VXNlcjgyMDkyNjM=", "avatar_url": "https://avatars.githubusercontent.com/u/8209263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kengz", "html_url": "https://github.com/kengz", "followers_url": "https://api.github.com/users/kengz/followers", "following_url": "https://api.github.com/users/kengz/following{/other_user}", "gists_url": "https://api.github.com/users/kengz/gists{/gist_id}", "starred_url": "https://api.github.com/users/kengz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kengz/subscriptions", "organizations_url": "https://api.github.com/users/kengz/orgs", "repos_url": "https://api.github.com/users/kengz/repos", "events_url": "https://api.github.com/users/kengz/events{/privacy}", "received_events_url": "https://api.github.com/users/kengz/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTExMDMwMzQ4", "tag_name": "v1.0.3", "target_commitish": "master", "name": "Atari, Dockerfile, PPO", "draft": false, "prerelease": false, "created_at": "2018-05-16T15:35:09Z", "published_at": "2018-05-16T15:37:46Z", "assets": [], "tarball_url": "https://api.github.com/repos/kengz/SLM-Lab/tarball/v1.0.3", "zipball_url": "https://api.github.com/repos/kengz/SLM-Lab/zipball/v1.0.3", "body": "## New features and improvements\r\n\r\n- some code cleanup to prepare for the next version\r\n- DQN Atari working, not optimized yet\r\n- Dockerfile finished, ready to run lab at scale on server\r\n- implemented PPO in tensorflow from OpenAI, along with the utils\r\n"}
{"url": "https://api.github.com/repos/kengz/SLM-Lab/releases/9930046", "assets_url": "https://api.github.com/repos/kengz/SLM-Lab/releases/9930046/assets", "upload_url": "https://uploads.github.com/repos/kengz/SLM-Lab/releases/9930046/assets{?name,label}", "html_url": "https://github.com/kengz/SLM-Lab/releases/tag/v1.0.2", "id": 9930046, "author": {"login": "kengz", "id": 8209263, "node_id": "MDQ6VXNlcjgyMDkyNjM=", "avatar_url": "https://avatars.githubusercontent.com/u/8209263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kengz", "html_url": "https://github.com/kengz", "followers_url": "https://api.github.com/users/kengz/followers", "following_url": "https://api.github.com/users/kengz/following{/other_user}", "gists_url": "https://api.github.com/users/kengz/gists{/gist_id}", "starred_url": "https://api.github.com/users/kengz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kengz/subscriptions", "organizations_url": "https://api.github.com/users/kengz/orgs", "repos_url": "https://api.github.com/users/kengz/repos", "events_url": "https://api.github.com/users/kengz/events{/privacy}", "received_events_url": "https://api.github.com/users/kengz/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTk5MzAwNDY=", "tag_name": "v1.0.2", "target_commitish": "master", "name": "v1.0.2 Evolutionary Search", "draft": false, "prerelease": false, "created_at": "2018-03-04T17:15:44Z", "published_at": "2018-03-04T17:16:10Z", "assets": [], "tarball_url": "https://api.github.com/repos/kengz/SLM-Lab/tarball/v1.0.2", "zipball_url": "https://api.github.com/repos/kengz/SLM-Lab/zipball/v1.0.2", "body": "## New features and improvements\r\n\r\n- add EvolutionarySearch for hyperparameter search\r\n- rewrite and simplify the underlying Ray logic\r\n- fix categorical error in a2c\r\n- improve experiment graph: wider, add opacity"}
{"url": "https://api.github.com/repos/kengz/SLM-Lab/releases/9717802", "assets_url": "https://api.github.com/repos/kengz/SLM-Lab/releases/9717802/assets", "upload_url": "https://uploads.github.com/repos/kengz/SLM-Lab/releases/9717802/assets{?name,label}", "html_url": "https://github.com/kengz/SLM-Lab/releases/tag/v1.0.1", "id": 9717802, "author": {"login": "kengz", "id": 8209263, "node_id": "MDQ6VXNlcjgyMDkyNjM=", "avatar_url": "https://avatars.githubusercontent.com/u/8209263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kengz", "html_url": "https://github.com/kengz", "followers_url": "https://api.github.com/users/kengz/followers", "following_url": "https://api.github.com/users/kengz/following{/other_user}", "gists_url": "https://api.github.com/users/kengz/gists{/gist_id}", "starred_url": "https://api.github.com/users/kengz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kengz/subscriptions", "organizations_url": "https://api.github.com/users/kengz/orgs", "repos_url": "https://api.github.com/users/kengz/repos", "events_url": "https://api.github.com/users/kengz/events{/privacy}", "received_events_url": "https://api.github.com/users/kengz/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTk3MTc4MDI=", "tag_name": "v1.0.1", "target_commitish": "master", "name": "v1.0.1: fitness, analysis, tune A2C and Reinforce", "draft": false, "prerelease": false, "created_at": "2018-02-17T02:19:00Z", "published_at": "2018-02-17T02:19:30Z", "assets": [], "tarball_url": "https://api.github.com/repos/kengz/SLM-Lab/tarball/v1.0.1", "zipball_url": "https://api.github.com/repos/kengz/SLM-Lab/zipball/v1.0.1", "body": "## New features and improvements\r\n\r\n- improve fitness computation after usage\r\n- add retro analysis script, via `yarn analyze <dir>`\r\n- improve plotly renderings\r\n- improve CNN and RNN architectures, bring to Reinforce\r\n- fine tune A2C and Reinforce specs\r\n"}
{"url": "https://api.github.com/repos/kengz/SLM-Lab/releases/9527487", "assets_url": "https://api.github.com/repos/kengz/SLM-Lab/releases/9527487/assets", "upload_url": "https://uploads.github.com/repos/kengz/SLM-Lab/releases/9527487/assets{?name,label}", "html_url": "https://github.com/kengz/SLM-Lab/releases/tag/v1.0.0", "id": 9527487, "author": {"login": "kengz", "id": 8209263, "node_id": "MDQ6VXNlcjgyMDkyNjM=", "avatar_url": "https://avatars.githubusercontent.com/u/8209263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kengz", "html_url": "https://github.com/kengz", "followers_url": "https://api.github.com/users/kengz/followers", "following_url": "https://api.github.com/users/kengz/following{/other_user}", "gists_url": "https://api.github.com/users/kengz/gists{/gist_id}", "starred_url": "https://api.github.com/users/kengz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kengz/subscriptions", "organizations_url": "https://api.github.com/users/kengz/orgs", "repos_url": "https://api.github.com/users/kengz/repos", "events_url": "https://api.github.com/users/kengz/events{/privacy}", "received_events_url": "https://api.github.com/users/kengz/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTk1Mjc0ODc=", "tag_name": "v1.0.0", "target_commitish": "master", "name": "v1.0.0: First stable release with full lab features", "draft": false, "prerelease": false, "created_at": "2018-02-04T23:03:48Z", "published_at": "2018-02-04T23:09:57Z", "assets": [], "tarball_url": "https://api.github.com/repos/kengz/SLM-Lab/tarball/v1.0.0", "zipball_url": "https://api.github.com/repos/kengz/SLM-Lab/zipball/v1.0.0", "body": "This is the first stable release of the lab, with the core API and features finalized.\r\n\r\nRefer to the docs:\r\n**[Github Repo](https://github.com/kengz/SLM-Lab) | [Lab Documentation](https://kengz.gitbooks.io/slm-lab/content/) | [Experiment Log Book](https://lgraesser.gitbooks.io/slm-experiment-log/content/)**\r\n\r\n## Features\r\n\r\nAll the crucial features of the lab are stable and tested:\r\n\r\n- baseline algorithms\r\n- [OpenAI gym](https://github.com/openai/gym), [Unity environments](https://github.com/Unity-Technologies/ml-agents)\r\n- modular reusable components\r\n- multi-agents, multi-environments\r\n- scalable hyperparameter search with [ray](https://github.com/ray-project/ray)\r\n- useful graphs and analytics\r\n- fitness vector for universal benchmarking of agents, environments\r\n\r\n## Baselines\r\n\r\nThe first release includes the following algorithms, with more to come later.\r\n\r\n- DQN\r\n- Double DQN\r\n- REINFORCE\r\n  - Option to add entropy to encourage exploration\r\n- Actor-Critic\r\n  - Batch or episodic training\r\n  - Shared or separate actor and critic params\r\n  - Advantage calculated using n-step returns or generalized advantage estimation\r\n  - Option to add entropy to encourage exploration"}
