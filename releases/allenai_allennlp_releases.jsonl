{"url": "https://api.github.com/repos/allenai/allennlp/releases/80270324", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/80270324/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/80270324/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v2.10.1", "id": 80270324, "author": {"login": "dirkgr", "id": 920638, "node_id": "MDQ6VXNlcjkyMDYzOA==", "avatar_url": "https://avatars.githubusercontent.com/u/920638?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dirkgr", "html_url": "https://github.com/dirkgr", "followers_url": "https://api.github.com/users/dirkgr/followers", "following_url": "https://api.github.com/users/dirkgr/following{/other_user}", "gists_url": "https://api.github.com/users/dirkgr/gists{/gist_id}", "starred_url": "https://api.github.com/users/dirkgr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dirkgr/subscriptions", "organizations_url": "https://api.github.com/users/dirkgr/orgs", "repos_url": "https://api.github.com/users/dirkgr/repos", "events_url": "https://api.github.com/users/dirkgr/events{/privacy}", "received_events_url": "https://api.github.com/users/dirkgr/received_events", "type": "User", "site_admin": false}, "node_id": "RE_kwDOBXH8-M4EyNP0", "tag_name": "v2.10.1", "target_commitish": "main", "name": "", "draft": false, "prerelease": false, "created_at": "2022-10-18T23:31:08Z", "published_at": "2022-10-18T23:32:24Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v2.10.1", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v2.10.1", "body": "## What's new\r\n\r\n### Fixed \u2705\r\n\r\n- Updated dependencies\r\n\r\n## Commits\r\n\r\nc51707ed Add a shout to allennlp-light to the README\r\n928df394 Be flexible about rich (#5719)\r\nd5f8e0c2 Update torch requirement from <1.12.0,>=1.10.0 to >=1.10.0,<1.13.0 (#5680)\r\n9f879b09 Add flair as an alternative (#5712)\r\nb2eb036e Allowed transitions (#5706)\r\nc6b248f4 Relax requirements on protobuf to allow for minor changes and patches. (#5694)\r\n8571d930 Add a list of alternatives for people to try (#5691)\r\n"}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/72028519", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/72028519/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/72028519/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v2.10.0", "id": 72028519, "author": {"login": "epwalsh", "id": 8812459, "node_id": "MDQ6VXNlcjg4MTI0NTk=", "avatar_url": "https://avatars.githubusercontent.com/u/8812459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/epwalsh", "html_url": "https://github.com/epwalsh", "followers_url": "https://api.github.com/users/epwalsh/followers", "following_url": "https://api.github.com/users/epwalsh/following{/other_user}", "gists_url": "https://api.github.com/users/epwalsh/gists{/gist_id}", "starred_url": "https://api.github.com/users/epwalsh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/epwalsh/subscriptions", "organizations_url": "https://api.github.com/users/epwalsh/orgs", "repos_url": "https://api.github.com/users/epwalsh/repos", "events_url": "https://api.github.com/users/epwalsh/events{/privacy}", "received_events_url": "https://api.github.com/users/epwalsh/received_events", "type": "User", "site_admin": false}, "node_id": "RE_kwDOBXH8-M4ESxFn", "tag_name": "v2.10.0", "target_commitish": "main", "name": "v2.10.0", "draft": false, "prerelease": false, "created_at": "2022-07-14T18:34:51Z", "published_at": "2022-07-14T18:35:21Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v2.10.0", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v2.10.0", "body": "## What's new\r\n\r\n### Added \ud83c\udf89\r\n\r\n- Added metric `FBetaVerboseMeasure` which extends `FBetaMeasure` to ensure compatibility with logging plugins and add some options.\r\n- Added three sample weighting techniques to `ConditionalRandomField` by supplying three new subclasses: `ConditionalRandomFieldWeightEmission`, `ConditionalRandomFieldWeightTrans`, and `ConditionalRandomFieldWeightLannoy`.\r\n\r\n### Fixed \u2705\r\n\r\n- Fix error from `cached-path` version update.\r\n\r\n## Commits\r\n\r\nb7f1cb41 Prepare for release v2.10.0\r\n5a3acbaa Implementation of Weighted CRF Tagger (handling unbalanced datasets) (#5676)\r\n20df7cdd Disable dependabot, add notice of future sunsetting (#5685)\r\n7bcbb5ad Update transformers requirement from <4.20,>=4.1 to >=4.1,<4.21 (#5675)\r\n0770b00a Bump black from 22.3.0 to 22.6.0 (#5679)\r\n38b9a9eb Update transformers requirement from <4.19,>=4.1 to >=4.1,<4.20 (#5636)\r\ned89a2e2 Update twine requirement from <4.0.0,>=1.11.0 to >=1.11.0,<5.0.0 (#5669)\r\n5ace70ef Bump actions/setup-python from 2 to 4 (#5662)\r\nbd6b0604 Update mkdocs-material requirement from <8.3.0,>=5.5.0 to >=5.5.0,<8.4.0 (#5655)\r\n55663be7 Bump actions/checkout from 2 to 3 (#5647)\r\nfb92b76d Bump actions/upload-artifact from 1 to 3 (#5644)\r\n775919a2 Bump actions/cache from 2 to 3 (#5645)\r\nf71eca96 Bump webfactory/ssh-agent from 0.4.1 to 0.5.4 (#5643)\r\n62d413d9 Bump codecov/codecov-action from 1 to 3 (#5642)\r\n428cb7d3 Bump actions/download-artifact from 1 to 3 (#5641)\r\neac4829d Bump mypy from 0.960 to 0.961 (#5658)\r\ndf9d7ca0 Make saliency interpreter GPU compatible (#5656)\r\nea4a53ca Update cached_path version (#5665)\r\na6271a31 FBetaMeasure metric with one value per key (#5638)\r\n8b5ccc4a Bump mypy from 0.950 to 0.960 (#5639)\r\n39b3c961 Dependabot GitHub Actions (#5640)\r\n60fae31f Update filelock requirement from <3.7,>=3.3 to >=3.3,<3.8 (#5635)\r\n67f32d3f Update spacy requirement from <3.3,>=2.1.0 to >=2.1.0,<3.4 (#5631)\r\n2fd8dfa6 Bump mypy from 0.942 to 0.950 (#5630)\r\nd7409d26 Missing `f` prefix on f-strings fix (#5629)", "reactions": {"url": "https://api.github.com/repos/allenai/allennlp/releases/72028519/reactions", "total_count": 3, "+1": 2, "-1": 0, "laugh": 0, "hooray": 1, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/64499175", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/64499175/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/64499175/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v2.9.3", "id": 64499175, "author": {"login": "dirkgr", "id": 920638, "node_id": "MDQ6VXNlcjkyMDYzOA==", "avatar_url": "https://avatars.githubusercontent.com/u/920638?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dirkgr", "html_url": "https://github.com/dirkgr", "followers_url": "https://api.github.com/users/dirkgr/followers", "following_url": "https://api.github.com/users/dirkgr/following{/other_user}", "gists_url": "https://api.github.com/users/dirkgr/gists{/gist_id}", "starred_url": "https://api.github.com/users/dirkgr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dirkgr/subscriptions", "organizations_url": "https://api.github.com/users/dirkgr/orgs", "repos_url": "https://api.github.com/users/dirkgr/repos", "events_url": "https://api.github.com/users/dirkgr/events{/privacy}", "received_events_url": "https://api.github.com/users/dirkgr/received_events", "type": "User", "site_admin": false}, "node_id": "RE_kwDOBXH8-M4D2C3n", "tag_name": "v2.9.3", "target_commitish": "main", "name": "", "draft": false, "prerelease": false, "created_at": "2022-04-14T21:47:45Z", "published_at": "2022-04-14T21:48:50Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v2.9.3", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v2.9.3", "body": "## What's new\r\n\r\n### Added \ud83c\udf89\r\n\r\n- Added `verification_tokens` argument to `TestPretrainedTransformerTokenizer`.\r\n\r\n### Fixed \u2705\r\n\r\n- Updated various dependencies\r\n\r\n## Commits\r\n\r\n0c4983a4 Prepare for release v2.9.3\r\n426d894c Docspec2 (#5618)\r\n1be8855f Add `custom_dummy_tokens` to `PretrainedTransformerTokenizer` (#5608)\r\nd21854b2 Update transformers requirement from <4.18,>=4.1 to >=4.1,<4.19 (#5617)\r\n8684412a Bump mkdocs from 1.2.3 to 1.3.0 (#5609)\r\n24e48a3f Bump docspec-python from 1.2.0 to 1.3.0 (#5611)\r\nedafff1f Bump docspec from 1.2.0 to 1.3.0 (#5610)\r\nb66e5f81 Bump black from 22.1.0 to 22.3.0 (#5613)\r\n089d03c1 Bump mypy from 0.941 to 0.942 (#5606)\r\ndfb438a1 Bump mypy from 0.931 to 0.941 (#5599)\r\n", "reactions": {"url": "https://api.github.com/repos/allenai/allennlp/releases/64499175/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/62383447", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/62383447/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/62383447/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v2.9.2", "id": 62383447, "author": {"login": "epwalsh", "id": 8812459, "node_id": "MDQ6VXNlcjg4MTI0NTk=", "avatar_url": "https://avatars.githubusercontent.com/u/8812459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/epwalsh", "html_url": "https://github.com/epwalsh", "followers_url": "https://api.github.com/users/epwalsh/followers", "following_url": "https://api.github.com/users/epwalsh/following{/other_user}", "gists_url": "https://api.github.com/users/epwalsh/gists{/gist_id}", "starred_url": "https://api.github.com/users/epwalsh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/epwalsh/subscriptions", "organizations_url": "https://api.github.com/users/epwalsh/orgs", "repos_url": "https://api.github.com/users/epwalsh/repos", "events_url": "https://api.github.com/users/epwalsh/events{/privacy}", "received_events_url": "https://api.github.com/users/epwalsh/received_events", "type": "User", "site_admin": false}, "node_id": "RE_kwDOBXH8-M4Dt-VX", "tag_name": "v2.9.2", "target_commitish": "main", "name": "v2.9.2", "draft": false, "prerelease": false, "created_at": "2022-03-21T23:48:01Z", "published_at": "2022-03-21T23:49:20Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v2.9.2", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v2.9.2", "body": "## What's new\r\n\r\n### Fixed \u2705\r\n\r\n- Removed unnecessary dependencies\r\n- Restore functionality of CLI in absence of now-optional checklist-package\r\n\r\n## Commits\r\n\r\nf6866f95 Fix CLI and install instructions in case optional checklists is not present (#5589)\r\ne1c6935c Update torch requirement from <1.11.0,>=1.6.0 to >=1.6.0,<1.12.0 (#5595)\r\n5f5f8c30 Updated the docs for `PytorchSeq2VecWrapper` to specify that `mask` is required (#5386)\r\n2426ce3d Dependencies (#5593)\r\n2d9fe79f Bump fairscale from 0.4.5 to 0.4.6 (#5590)\r\nab37da7b Update transformers requirement from <4.17,>=4.1 to >=4.1,<4.18 (#5583)", "reactions": {"url": "https://api.github.com/repos/allenai/allennlp/releases/62383447/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/61437421", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/61437421/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/61437421/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v2.9.1", "id": 61437421, "author": {"login": "epwalsh", "id": 8812459, "node_id": "MDQ6VXNlcjg4MTI0NTk=", "avatar_url": "https://avatars.githubusercontent.com/u/8812459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/epwalsh", "html_url": "https://github.com/epwalsh", "followers_url": "https://api.github.com/users/epwalsh/followers", "following_url": "https://api.github.com/users/epwalsh/following{/other_user}", "gists_url": "https://api.github.com/users/epwalsh/gists{/gist_id}", "starred_url": "https://api.github.com/users/epwalsh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/epwalsh/subscriptions", "organizations_url": "https://api.github.com/users/epwalsh/orgs", "repos_url": "https://api.github.com/users/epwalsh/repos", "events_url": "https://api.github.com/users/epwalsh/events{/privacy}", "received_events_url": "https://api.github.com/users/epwalsh/received_events", "type": "User", "site_admin": false}, "node_id": "RE_kwDOBXH8-M4DqXXt", "tag_name": "v2.9.1", "target_commitish": "main", "name": "v2.9.1", "draft": false, "prerelease": false, "created_at": "2022-03-09T21:54:47Z", "published_at": "2022-03-09T21:56:06Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v2.9.1", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v2.9.1", "body": "## What's new\r\n\r\n### Fixed \u2705\r\n\r\n- Updated dependencies, especially around doc creation.\r\n- Running the test suite out-of-tree (e.g. after installation) is now possible by pointing the environment variable `ALLENNLP_SRC_DIR` to the sources.\r\n- Silenced a warning that happens when you inappropriately clone a tensor.\r\n- Adding more clarification to the `Vocabulary` documentation around `min_pretrained_embeddings` and `only_include_pretrained_words`.\r\n- Fixed bug with type mismatch caused by latest release of `cached-path` that now returns a `Path` instead of a `str`.\r\n\r\n### Added \ud83c\udf89\r\n\r\n- We can now transparently read compressed input files during prediction.\r\n- LZMA compression is now supported.\r\n- Added a way to give JSON blobs as input to dataset readers in the `evaluate` command.\r\n- Added the argument `sub_module` in `PretrainedTransformerMismatchedEmbedder`\r\n\r\n### Changed \u26a0\ufe0f\r\n\r\n- You can automatically include all words from a pretrained file when building a vocabulary by setting the value in `min_pretrained_embeddings` to `-1`\r\n  for that particular namespace.\r\n\r\n## Commits\r\n\r\n3547bfb8 pin cached-path tighter, make sure our cached-path wrapper still returns `str` (#5587)\r\n99c93439 Clarify Vocabulary documentation, add -1 option for `min_pretrained_embeddings` (#5581)\r\n3fa51933 Makes the `evaluate` command work for the multitask case (Second Edition) (#5579)\r\n9f03803b Add \"sub_module\" argument in PretrainedTransformerMismatchedEmbedder (#5580)\r\n92e54cce Open Compressed (#5578)\r\n5b3352ce Clone warns (#5575)\r\n9da4b0fe Add Wassterstein Distance calculation option for fairness metrics (#5546)\r\nb8f92f03 Update mkdocs-material requirement from <8.2.0,>=5.5.0 to >=5.5.0,<8.3.0 (#5572)\r\na21c0b4c Update filelock requirement from <3.5,>=3.3 to >=3.3,<3.7 (#5571)\r\n6614077b Make tests runnable out-of-tree for help with conda-packaging (#5560)\r\ne6792133 Fix CITATION.cff and add automatic validation of your citation metadata (#5561)\r\nefa9f1d0 try to unpin nltk (#5563)\r\nd01179b7 Small typo fix (#5555)\r\n3c2299aa tighten test_sampled_equals_unsampled_when_biased_against_non_sampled_positions bound (#5549)\r\ne463084b Bump black from 21.12b0 to 22.1.0 (#5554)\r\n8226e87d Making checklist optional (#5507)\r\na76bf1e3 Update transformers requirement from <4.16,>=4.1 to >=4.1,<4.17 (#5553)", "reactions": {"url": "https://api.github.com/repos/allenai/allennlp/releases/61437421/reactions", "total_count": 3, "+1": 3, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/58125349", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/58125349/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/58125349/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v2.9.0", "id": 58125349, "author": {"login": "AkshitaB", "id": 6500683, "node_id": "MDQ6VXNlcjY1MDA2ODM=", "avatar_url": "https://avatars.githubusercontent.com/u/6500683?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AkshitaB", "html_url": "https://github.com/AkshitaB", "followers_url": "https://api.github.com/users/AkshitaB/followers", "following_url": "https://api.github.com/users/AkshitaB/following{/other_user}", "gists_url": "https://api.github.com/users/AkshitaB/gists{/gist_id}", "starred_url": "https://api.github.com/users/AkshitaB/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AkshitaB/subscriptions", "organizations_url": "https://api.github.com/users/AkshitaB/orgs", "repos_url": "https://api.github.com/users/AkshitaB/repos", "events_url": "https://api.github.com/users/AkshitaB/events{/privacy}", "received_events_url": "https://api.github.com/users/AkshitaB/received_events", "type": "User", "site_admin": false}, "node_id": "RE_kwDOBXH8-M4Dduwl", "tag_name": "v2.9.0", "target_commitish": "main", "name": "v2.9.0", "draft": false, "prerelease": false, "created_at": "2022-01-27T17:51:24Z", "published_at": "2022-01-27T17:53:29Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v2.9.0", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v2.9.0", "body": "## What's new\r\n\r\n### Added \ud83c\udf89\r\n\r\n- Added an `Evaluator` class to make comparing source, target, and predictions easier.\r\n- Added a way to resize the vocabulary in the T5 module\r\n- Added an argument `reinit_modules` to `cached_transformers.get()` that allows you to re-initialize the pretrained weights of a transformer model, using layer indices or regex strings.\r\n- Added attribute `_should_validate_this_epoch` to `GradientDescentTrainer` that controls whether validation is run at the end of each epoch.\r\n- Added `ShouldValidateCallback` that can be used to configure the frequency of validation during training.\r\n- Added a `MaxPoolingSpanExtractor`. This `SpanExtractor` represents each span by a component wise max-pooling-operation.\r\n\r\n### Fixed \u2705\r\n\r\n- Fixed the docstring information for the `FBetaMultiLabelMeasure` metric.\r\n- Various fixes for Python 3.9\r\n- Fixed the name that the `push-to-hf` command uses to store weights.\r\n- `FBetaMultiLabelMeasure` now works with multiple dimensions\r\n- Support for inferior operating systems when making hardlinks\r\n- Use `,` as a separator for filenames in the `evaluate` command, thus allowing for URLs (eg. `gs://...`) as input files.\r\n- Removed a spurious error message \"'torch.cuda' has no attribute '_check_driver'\" that would be appear in the logs\r\n  when a `ConfigurationError` for missing GPU was raised.\r\n- Load model on CPU post training to save GPU memory.\r\n- Fixed a bug in `ShouldValidateCallback` that leads to validation occuring after the first epoch regardless of `validation_start` value.\r\n- Fixed a bug in `ShouldValidateCallback` that leads to validation occuring every `validation_interval + 1` epochs, instead of every `validation_interval` epochs.\r\n- Fixed a bug in `ShouldValidateCallback` that leads to validation never occuring at the end of training.\r\n\r\n### Removed \ud83d\udc4b\r\n\r\n- Removed Tango components, since they now live at https://github.com/allenai/tango.\r\n- Removed dependency on the `overrides` package\r\n\r\n## Commits\r\n\r\ndd5a010e Evaluator (#5445)\r\n0b54fb0d Bump fairscale from 0.4.4 to 0.4.5 (#5545)\r\n2deacfe5 Fix should validate callback train end (#5542)\r\n2cdb8742 Bump mypy from 0.910 to 0.931 (#5538)\r\na91946ae Keep NLTK down. They broke the download of omw. (#5540)\r\n73a5cfc1 Removes stuff that now lives in the tango repo (#5482)\r\n1278f16d Move changes from #5534 to correct place. (#5535)\r\na7117035 Fix ShouldValidateCallback (#5536)\r\nb0b3ad4b Update mkdocs-material requirement from <8.1.0,>=5.5.0 to >=5.5.0,<8.2.0 (#5503)\r\na3d71254 Max out span extractor (#5520)\r\n515fe9b7 Configure validation frequency (#5534)\r\nd7e0c877 Update transformers requirement from <4.15,>=4.1 to >=4.1,<4.16 (#5528)\r\n42332476 Bump fairscale from 0.4.3 to 0.4.4 (#5525)\r\n71f2d797 fix 'check_for_gpu' (#5522)\r\n06ec7f9a Reinit layers of pretrained transformer in cached_transformers.get() (#5505)\r\nec1fb69f add missing nltk download in CI (#5529)\r\nab4f7b5c Fix model loading on GPU post training (#5518)\r\n3552842f Fix moving average args not rendering properly in docs (#5516)\r\n87ad0061 Update transformers requirement from <4.13,>=4.1 to >=4.1,<4.15 (#5515)\r\n39f4f4c1 tick version for nightly releases\r\n38436d89 Use comma as filename separator (#5506)\r\ne0ee7f43 Dimensions in `FBetaMultiLabelMeasure` (#5501)\r\nd77ba3d6 Hardlink or copy (#5502)\r\ndbcbcf10 Add installation instructions through conda-forge (#5498)\r\nebad9eeb Bump black from 21.11b1 to 21.12b0 (#5496)\r\n82b1f4f8 Use the correct filename when uploading models to the HF Hub (#5499)\r\n19f6c8f9 Resize T5 Vocab (#5497)\r\nc557d512 enforce reading in utf-8 encoding (#5476)\r\n1caf0daf Removes dependency on the overrides package (#5490)\r\nb99376fe Python 3.9 (#5489)\r\n666eaa56 Update mkdocs-material requirement from <7.4.0,>=5.5.0 to >=5.5.0,<8.1.0 (#5486)\r\n64b2c078 Bump fairscale from 0.4.2 to 0.4.3 (#5474)\r\n0a794c6b Fix metric docstring (#5475)\r\nf86ff9f4 Bump black from 21.10b0 to 21.11b1 (#5473)\r\na7f6cdf1 update cached-path (#5477)\r\n844acfa9 Update filelock requirement from <3.4,>=3.3 to >=3.3,<3.5 (#5469)\r\n05fc7f62 Bump fairscale from 0.4.0 to 0.4.2 (#5461)\r\n923dbde0 Bump black from 21.9b0 to 21.10b0 (#5453)\r\n09e22aa6 Update spacy requirement from <3.2,>=2.1.0 to >=2.1.0,<3.3 (#5460)\r\n54b92ae7 HF now raises ValueError (#5464)\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/allenai/allennlp/releases/58125349/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/52476454", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/52476454/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/52476454/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v2.8.0", "id": 52476454, "author": {"login": "AkshitaB", "id": 6500683, "node_id": "MDQ6VXNlcjY1MDA2ODM=", "avatar_url": "https://avatars.githubusercontent.com/u/6500683?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AkshitaB", "html_url": "https://github.com/AkshitaB", "followers_url": "https://api.github.com/users/AkshitaB/followers", "following_url": "https://api.github.com/users/AkshitaB/following{/other_user}", "gists_url": "https://api.github.com/users/AkshitaB/gists{/gist_id}", "starred_url": "https://api.github.com/users/AkshitaB/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AkshitaB/subscriptions", "organizations_url": "https://api.github.com/users/AkshitaB/orgs", "repos_url": "https://api.github.com/users/AkshitaB/repos", "events_url": "https://api.github.com/users/AkshitaB/events{/privacy}", "received_events_url": "https://api.github.com/users/AkshitaB/received_events", "type": "User", "site_admin": false}, "node_id": "RE_kwDOBXH8-M4DILom", "tag_name": "v2.8.0", "target_commitish": "main", "name": "v2.8.0", "draft": false, "prerelease": false, "created_at": "2021-11-01T21:09:36Z", "published_at": "2021-11-01T21:11:27Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v2.8.0", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v2.8.0", "body": "## What's new\r\n\r\n### Added \ud83c\udf89\r\n\r\n- Added support to push models directly to the [Hugging Face Hub](https://huggingface.co/) with the command `allennlp push-to-hf`.\r\n- More default tests for the `TextualEntailmentSuite`.\r\n\r\n### Changed \u26a0\ufe0f\r\n\r\n- The behavior of `--overrides` has changed. Previously the final configuration params were simply taken as the union over the original params and the `--overrides` params.\r\n  But now you can use `--overrides` to completely replace any part of the original config. For example, passing `--overrides '{\"model\":{\"type\":\"foo\"}}'` will completely\r\n  replace the \"model\" part of the original config. However, when you just want to change a single field in the JSON structure without removing / replacing adjacent fields,\r\n  you can still use the \"dot\" syntax. For example, `--overrides '{\"model.num_layers\":3}'` will only change the `num_layers` parameter to the \"model\" part of the config, leaving\r\n  everything else unchanged.\r\n- Integrated [`cached_path`](https://github.com/allenai/cached_path) library to replace existing functionality in `common.file_utils`. This introduces some improvements without\r\n  any breaking changes.\r\n\r\n### Fixed \u2705\r\n\r\n- Fixed the implementation of `PairedPCABiasDirection` in `allennlp.fairness.bias_direction`, where the difference vectors should not be centered when performing the PCA.\r\n\r\n## Commits\r\n\r\n7213d520 Update transformers requirement from <4.12,>=4.1 to >=4.1,<4.13 (#5452)\r\n1b022270 bug fix (#5447)\r\n0d8c0fc5 Update torch requirement from <1.10.0,>=1.6.0 to >=1.6.0,<1.11.0 (#5442)\r\n0c79807c Checklist update (#5438)\r\nebd6b5ba integrate cached_path (#5418)\r\ndcd8d9e9 Update mkdocs-material requirement from <7.3.0,>=5.5.0 to >=5.5.0,<7.4.0 (#5419)\r\n362349b5 Registrable _to_params default functionality (#5403)\r\n17ef1aa2 fix a bug when using fp16 training & gradient clipping (#5426)\r\na63e28c2 Update transformers requirement from <4.11,>=4.1 to >=4.1,<4.12 (#5422)\r\n603552fc Add utility function and command to push models to \ud83e\udd17 Hub (#5370)\r\ne5d332a5 Update filelock requirement from <3.1,>=3.0 to >=3.0,<3.2 (#5421)\r\n44155ac6 Make `--overrides` more flexible (#5399)\r\n43fd9825 Fix PairedPCABiasDirection (#5396)\r\n7785068a Bump black from 21.7b0 to 21.9b0 (#5408)\r\na09d057c Update transformers requirement from <4.10,>=4.1 to >=4.1,<4.11 (#5393)\r\n527e43d9 require Python>=3.7 (#5400)\r\n5338bd8b Add scaling to tqdm bar when downloading files (#5397)\r\n\r\n", "reactions": {"url": "https://api.github.com/repos/allenai/allennlp/releases/52476454/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/48848682", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/48848682/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/48848682/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v2.7.0", "id": 48848682, "author": {"login": "epwalsh", "id": 8812459, "node_id": "MDQ6VXNlcjg4MTI0NTk=", "avatar_url": "https://avatars.githubusercontent.com/u/8812459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/epwalsh", "html_url": "https://github.com/epwalsh", "followers_url": "https://api.github.com/users/epwalsh/followers", "following_url": "https://api.github.com/users/epwalsh/following{/other_user}", "gists_url": "https://api.github.com/users/epwalsh/gists{/gist_id}", "starred_url": "https://api.github.com/users/epwalsh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/epwalsh/subscriptions", "organizations_url": "https://api.github.com/users/epwalsh/orgs", "repos_url": "https://api.github.com/users/epwalsh/repos", "events_url": "https://api.github.com/users/epwalsh/events{/privacy}", "received_events_url": "https://api.github.com/users/epwalsh/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTQ4ODQ4Njgy", "tag_name": "v2.7.0", "target_commitish": "main", "name": "v2.7.0", "draft": false, "prerelease": false, "created_at": "2021-09-01T21:17:14Z", "published_at": "2021-09-01T21:21:18Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v2.7.0", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v2.7.0", "body": "## What's new\r\n\r\n### Added \ud83c\udf89\r\n\r\n- Added support to evaluate mutiple datasets and produce corresponding output files in the `evaluate` command.\r\n- Added more documentation to the learning rate schedulers to include a sample config object for how to use it.\r\n- Moved the pytorch learning rate schedulers wrappers to their own file called `pytorch_lr_schedulers.py` so that they will have their own documentation page.\r\n- Added a module `allennlp.nn.parallel` with a new base class, `DdpAccelerator`, which generalizes\r\n  PyTorch's `DistributedDataParallel` wrapper to support other implementations. Two implementations of\r\n  this class are provided. The default is `TorchDdpAccelerator` (registered at \"torch\"), which is just a thin wrapper around\r\n  `DistributedDataParallel`. The other is `FairScaleFsdpAccelerator`, which wraps FairScale's\r\n  [`FullyShardedDataParallel`](https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html).\r\n  You can specify the `DdpAccelerator` in the \"distributed\" section of a configuration file under the key \"ddp_accelerator\".\r\n- Added a module `allennlp.nn.checkpoint` with a new base class, `CheckpointWrapper`, for implementations\r\n  of activation/gradient checkpointing. Two implentations are provided. The default implementation is `TorchCheckpointWrapper` (registered as \"torch\"),\r\n  which exposes [PyTorch's checkpoint functionality](https://pytorch.org/docs/stable/checkpoint.html).\r\n  The other is `FairScaleCheckpointWrapper` which exposes the more flexible\r\n  [checkpointing funtionality from FairScale](https://fairscale.readthedocs.io/en/latest/api/nn/checkpoint/checkpoint_activations.html).\r\n- The `Model` base class now takes a `ddp_accelerator` parameter (an instance of `DdpAccelerator`) which will be available as\r\n  `self.ddp_accelerator` during distributed training. This is useful when, for example, instantiating submodules in your\r\n  model's `__init__()` method by wrapping them with `self.ddp_accelerator.wrap_module()`. See the `allennlp.modules.transformer.t5`\r\n  for an example.\r\n- We now log batch metrics to tensorboard and wandb.\r\n- Added Tango components, to be explored in detail in a later post\r\n- Added `ScaledDotProductMatrixAttention`, and converted the transformer toolkit to use it\r\n- Added tests to ensure that all `Attention` and `MatrixAttention` implementations are interchangeable\r\n- Added a way for AllenNLP Tango to read and write datasets lazily.\r\n- Added a way to remix datasets flexibly\r\n- Added `from_pretrained_transformer_and_instances` constructor to `Vocabulary`\r\n- `TransformerTextField` now supports `__len__`.\r\n\r\n### Fixed \u2705\r\n\r\n- Fixed a bug in `ConditionalRandomField`: `transitions` and `tag_sequence` tensors were not initialized on the desired device causing high CPU usage (see https://github.com/allenai/allennlp/issues/2884)\r\n- Fixed a mispelling: the parameter `contructor_extras` in `Lazy()` is now correctly called `constructor_extras`.\r\n- Fixed broken links in `allennlp.nn.initializers` docs.\r\n- Fixed bug in `BeamSearch` where `last_backpointers` was not being passed to any `Constraint`s.\r\n- `TransformerTextField` can now take tensors of shape `(1, n)` like the tensors produced from a HuggingFace tokenizer.\r\n- `tqdm` lock is now set inside `MultiProcessDataLoading` when new workers are spawned to avoid contention when writing output.\r\n- `ConfigurationError` is now pickleable.\r\n- Checkpointer cleaning was fixed to work on Windows Paths\r\n- Multitask models now support `TextFieldTensor` in heads, not just in the backbone.\r\n- Fixed the signature of `ScaledDotProductAttention` to match the other `Attention` classes\r\n- `allennlp` commands will now catch `SIGTERM` signals and handle them similar to `SIGINT` (keyboard interrupt).\r\n- The `MultiProcessDataLoader` will properly shutdown its workers when a `SIGTERM` is received.\r\n- Fixed the way names are applied to Tango `Step` instances.\r\n- Fixed a bug in calculating loss in the distributed setting.\r\n- Fixed a bug when extending a sparse sequence by 0 items.\r\n\r\n### Changed \u26a0\ufe0f\r\n\r\n- The type of the `grad_norm` parameter of `GradientDescentTrainer` is now `Union[float, bool]`,\r\n  with a default value of `False`. `False` means gradients are not rescaled and the gradient\r\n  norm is never even calculated. `True` means the gradients are still not rescaled but the gradient\r\n  norm is calculated and passed on to callbacks. A `float` value means gradients are rescaled.\r\n- `TensorCache` now supports more concurrent readers and writers.\r\n- We no longer log parameter statistics to tensorboard or wandb by default.\r\n\r\n## Commits\r\n\r\n48af9d34 Multiple datasets and output files support for the evaluate command (#5340)\r\n60213cd7 Tiny tango tweaks (#5383)\r\n28950215 improve signal handling and worker cleanup (#5378)\r\nb41cb3eb Fix distributed loss (#5381)\r\n6355f073 Fix Checkpointer cleaner regex on Windows (#5361)\r\n27da04cf Dataset remix (#5372)\r\n75af38e0 Create Vocabulary from both pretrained transformers and instances (#5368)\r\n5dc80a65 Adds a dataset that can be read and written lazily (#5344)\r\n01e8a35a Improved Documentation For Learning Rate Schedulers (#5365)\r\n8370cfa3 skip loading t5-base in CI (#5371)\r\n13de38d1 Log batch metrics (#5362)\r\n1f5c6e5b Use our own base images to build allennlp Docker images (#5366)\r\nbffdbfd1 Bugfix: initializing all tensors and parameters of the `ConditionalRandomField` model on the proper device (#5335)\r\nd45a2dab Make sure that all attention works the same (#5360)\r\nc1edaef8 Update google-cloud-storage requirement (#5357)\r\n524244b6 Update wandb requirement from <0.12.0,>=0.10.0 to >=0.10.0,<0.13.0 (#5356)\r\n90bf33b8 small fixes for tango (#5350)\r\n2e11a15e tick version for nightly releases\r\n311f1104 Tango (#5162)\r\n1df2e517 Bump fairscale from 0.3.8 to 0.3.9 (#5337)\r\nb72bbfc9 fix constraint bug in beam search, clean up tests (#5328)\r\nec3e2943 Create CITATION.cff (#5336)\r\n8714aa0b This is a desperate attempt to make TensorCache a little more stable (#5334)\r\nfd429b2b Update transformers requirement from <4.9,>=4.1 to >=4.1,<4.10 (#5326)\r\n1b5ef3a0 Update spacy requirement from <3.1,>=2.1.0 to >=2.1.0,<3.2 (#5305)\r\n1f20513d TextFieldTensor in multitask models (#5331)\r\n76f2487b set tqdm lock when new workers are spawned (#5330)\r\n67add9d9 Fix `ConfigurationError` deserialization (#5319)\r\n42d85298 allow TransformerTextField to take input directly from HF tokenizer (#5329)\r\n64043ac6 Bump black from 21.6b0 to 21.7b0 (#5320)\r\n32750550 Update mkdocs-material requirement from <7.2.0,>=5.5.0 to >=5.5.0,<7.3.0 (#5327)\r\n5b1da908 Update links in initializers documentation (#5317)\r\nca656fc6 FairScale integration (#5242)", "discussion_url": "https://github.com/allenai/allennlp/discussions/5394", "reactions": {"url": "https://api.github.com/repos/allenai/allennlp/releases/48848682/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/46441495", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/46441495/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/46441495/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v2.6.0", "id": 46441495, "author": {"login": "dirkgr", "id": 920638, "node_id": "MDQ6VXNlcjkyMDYzOA==", "avatar_url": "https://avatars.githubusercontent.com/u/920638?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dirkgr", "html_url": "https://github.com/dirkgr", "followers_url": "https://api.github.com/users/dirkgr/followers", "following_url": "https://api.github.com/users/dirkgr/following{/other_user}", "gists_url": "https://api.github.com/users/dirkgr/gists{/gist_id}", "starred_url": "https://api.github.com/users/dirkgr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dirkgr/subscriptions", "organizations_url": "https://api.github.com/users/dirkgr/orgs", "repos_url": "https://api.github.com/users/dirkgr/repos", "events_url": "https://api.github.com/users/dirkgr/events{/privacy}", "received_events_url": "https://api.github.com/users/dirkgr/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTQ2NDQxNDk1", "tag_name": "v2.6.0", "target_commitish": "main", "name": "", "draft": false, "prerelease": false, "created_at": "2021-07-19T22:46:28Z", "published_at": "2021-07-19T22:47:14Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v2.6.0", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v2.6.0", "body": "## What's new\r\n\r\n### Added \ud83c\udf89\r\n\r\n- Added `on_backward` training callback which allows for control over backpropagation and gradient manipulation.\r\n- Added `AdversarialBiasMitigator`, a Model wrapper to adversarially mitigate biases in predictions produced by a pretrained model for a downstream task.\r\n- Added `which_loss` parameter to `ensure_model_can_train_save_and_load` in `ModelTestCase` to specify which loss to test.\r\n- Added `**kwargs` to `Predictor.from_path()`. These key-word argument will be passed on to the `Predictor`'s constructor.\r\n- The activation layer in the transformer toolkit now can be queried for its output dimension.\r\n- `TransformerEmbeddings` now takes, but ignores, a parameter for the attention mask. This is needed for compatibility with some other modules that get called the same way and use the mask.\r\n- `TransformerPooler` can now be instantiated from a pretrained transformer module, just like the other modules in the transformer toolkit.\r\n- `TransformerTextField`, for cases where you don't care about AllenNLP's advanced text handling capabilities.\r\n- Added `TransformerModule._post_load_pretrained_state_dict_hook()` method. Can be used to modify `missing_keys` and `unexpected_keys` after loading a pretrained state dictionary. This is useful when tying weights, for example.\r\n- Added an end-to-end test for the Transformer Toolkit.\r\n- Added `vocab` argument to `BeamSearch`, which is passed to each contraint in `constraints` (if provided).\r\n\r\n### Fixed \u2705\r\n\r\n- Fixed missing device mapping in the `allennlp.modules.conditional_random_field.py` file.\r\n- Fixed Broken link in `allennlp.fairness.fairness_metrics.Separation` docs\r\n- Ensured all `allennlp` submodules are imported with `allennlp.common.plugins.import_plugins()`.\r\n- Fixed `IndexOutOfBoundsException` in `MultiOptimizer` when checking if optimizer received any parameters.\r\n- Removed confusing zero mask from VilBERT.\r\n- Ensured `ensure_model_can_train_save_and_load` is consistently random.\r\n- Fixed weight tying logic in `T5` transformer module. Previously input/output embeddings were always tied. Now this is optional,\r\n  and the default behavior is taken from the `config.tie_word_embeddings` value when instantiating `from_pretrained_module()`.\r\n- Implemented slightly faster label smoothing.\r\n- Fixed the docs for `PytorchTransformerWrapper`\r\n- Fixed recovering training jobs with models that expect `get_metrics()` to not be called until they have seen at least one batch.\r\n- Made the Transformer Toolkit compatible with transformers that don't start their positional embeddings at 0.\r\n- Weights & Biases training callback (\"wandb\") now works when resuming training jobs.\r\n\r\n### Changed \u26a0\ufe0f\r\n\r\n- Changed behavior of `MultiOptimizer` so that while a default optimizer is still required, an error is not thrown if the default optimizer receives no parameters.\r\n- Made the epsilon parameter for the layer normalization in token embeddings configurable. \r\n\r\n### Removed \ud83d\udc4b\r\n\r\n- Removed `TransformerModule._tied_weights`. Weights should now just be tied directly in the `__init__()` method. You can also override `TransformerModule._post_load_pretrained_state_dict_hook()` to remove keys associated with tied weights from `missing_keys` after loading a pretrained state dictionary.\r\n\r\n## Commits\r\n\r\nef5400d5 make W&B callback resumable (#5312)\r\n96293407 Update google-cloud-storage requirement (#5309)\r\nf8fad9fc Provide vocab as param to constraints (#5321)\r\n56e1f49d Fix training Conditional Random Fields on GPU (#5313) (#5315)\r\n3c1ac032 Update wandb requirement from <0.11.0,>=0.10.0 to >=0.10.0,<0.12.0 (#5316)\r\n7d4a6726 Transformer Toolkit fixes (#5303)\r\naaa816f7 Faster label smoothing (#5294)\r\n436c52d5 Docs update for `PytorchTransformerWrapper` (#5295)\r\n3d92ac43 Update google-cloud-storage requirement (#5296)\r\n5378533f Fixes recovering when the model expects metrics to be ready (#5293)\r\n7428155a ensure torch always up-to-date in CI (#5286)\r\n3f307ee3 Update README.md (#5288)\r\n672485fb only run CHANGELOG check when source files are modified (#5287)\r\nc6865d79 use smaller snapshot for HFHub integration test\r\nad54d48f Bump mypy from 0.812 to 0.910 (#5283)\r\n42d96dfa typo: missing \"if\" in `drop_last` doc (#5284)\r\na246e277 TransformerTextField (#5280)\r\n82053a98 Improve weight tying logic in transformer module (#5282)\r\nc936da9f Update transformers requirement from <4.8,>=4.1 to >=4.1,<4.9 (#5281)\r\ne8f816dd Update google-cloud-storage requirement (#5277)\r\n86504e6b Making model test case consistently random (#5278)\r\n5a7844b5 add kwargs to Predictor.from_path() (#5275)\r\n8ad562e4 Update transformers requirement from <4.7,>=4.1 to >=4.1,<4.8 (#5273)\r\nc8b8ed36 Transformer toolkit updates (#5270)\r\n6af9069d update Python environment setup in GitHub Actions (#5272)\r\nf1f51fc9 Adversarial bias mitigation (#5269)\r\naf101d67 Removes confusing zero mask from VilBERT (#5264)\r\na1d36e67 Update torchvision requirement from <0.10.0,>=0.8.1 to >=0.8.1,<0.11.0 (#5266)\r\ne5468d96 Bump black from 21.5b2 to 21.6b0 (#5255)\r\nb37686f6 Update torch requirement from <1.9.0,>=1.6.0 to >=1.6.0,<1.10.0 (#5267)\r\n5da5b5ba Upload code coverage reports from different jobs, other CI improvements (#5257)\r\na6cfb122 added `on_backward` trainer callback (#5249)\r\n8db45e87 Ensure all relevant allennlp submodules are imported with `import_plugins()` (#5246)\r\n57df0e37 [Docs] Fixes broken link in Fairness_Metrics (#5245)\r\n154f75d7 Bump black from 21.5b1 to 21.5b2 (#5236)\r\n7a5106d5 tick version for nightly release\r\n"}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/44059690", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/44059690/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/44059690/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v2.5.0", "id": 44059690, "author": {"login": "epwalsh", "id": 8812459, "node_id": "MDQ6VXNlcjg4MTI0NTk=", "avatar_url": "https://avatars.githubusercontent.com/u/8812459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/epwalsh", "html_url": "https://github.com/epwalsh", "followers_url": "https://api.github.com/users/epwalsh/followers", "following_url": "https://api.github.com/users/epwalsh/following{/other_user}", "gists_url": "https://api.github.com/users/epwalsh/gists{/gist_id}", "starred_url": "https://api.github.com/users/epwalsh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/epwalsh/subscriptions", "organizations_url": "https://api.github.com/users/epwalsh/orgs", "repos_url": "https://api.github.com/users/epwalsh/repos", "events_url": "https://api.github.com/users/epwalsh/events{/privacy}", "received_events_url": "https://api.github.com/users/epwalsh/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTQ0MDU5Njkw", "tag_name": "v2.5.0", "target_commitish": "main", "name": "v2.5.0", "draft": false, "prerelease": false, "created_at": "2021-06-03T16:59:03Z", "published_at": "2021-06-03T17:19:41Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v2.5.0", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v2.5.0", "body": "\ud83c\udd95 AllenNLP `v2.5.0` comes with a few big new features and improvements \ud83c\udd95\r\n\r\nThere is a whole new module `allennlp.fairness` that contains implementations of fairness metrics, bias metrics, and bias mitigation tools for your models thanks to @ArjunSubramonian. For a great introduction, check out the corresponding chapter of the guide: https://guide.allennlp.org/fairness.\r\n\r\nAnother major addition is the `allennlp.confidence_checks.task_checklists` submodule, thanks to @AkshitaB, which provides an automated way to run behavioral tests of your models using the [`checklist`](https://github.com/marcotcr/checklist) library.\r\n\r\n`BeamSearch` also has several new important features, including an easy to add arbitrary constraints, thanks to @danieldeutsch.\r\n\r\nSee below for a comprehensive list of updates \ud83d\udc47\r\n\r\n## What's new\r\n\r\n### Added \ud83c\udf89\r\n\r\n- Added `TaskSuite` base class and command line functionality for running [`checklist`](https://github.com/marcotcr/checklist) test suites, along with implementations for `SentimentAnalysisSuite`, `QuestionAnsweringSuite`, and `TextualEntailmentSuite`. These can be found in the `allennlp.confidence_checks.task_checklists` module.\r\n- Added `BiasMitigatorApplicator`, which wraps any Model and mitigates biases by finetuning\r\non a downstream task.\r\n- Added `allennlp diff` command to compute a diff on model checkpoints, analogous to what `git diff` does on two files.\r\n- Meta data defined by the class `allennlp.common.meta.Meta` is now saved in the serialization directory and archive file\r\n  when training models from the command line. This is also now part of the `Archive` named tuple that's returned from `load_archive()`.\r\n- Added `nn.util.distributed_device()` helper function.\r\n- Added `allennlp.nn.util.load_state_dict` helper function.\r\n- Added a way to avoid downloading and loading pretrained weights in modules that wrap transformers\r\n  such as the `PretrainedTransformerEmbedder` and `PretrainedTransformerMismatchedEmbedder`.\r\n  You can do this by setting the parameter `load_weights` to `False`.\r\n  See [PR #5172](https://github.com/allenai/allennlp/pull/5172) for more details.\r\n- Added `SpanExtractorWithSpanWidthEmbedding`, putting specific span embedding computations into the `_embed_spans` method and leaving the common code in `SpanExtractorWithSpanWidthEmbedding` to unify the arguments, and modified `BidirectionalEndpointSpanExtractor`, `EndpointSpanExtractor` and `SelfAttentiveSpanExtractor` accordingly. Now, `SelfAttentiveSpanExtractor` can also embed span widths.\r\n- Added a `min_steps` parameter to `BeamSearch` to set a minimum length for the predicted sequences.\r\n- Added the `FinalSequenceScorer` abstraction to calculate the final scores of the generated sequences in `BeamSearch`. \r\n- Added `shuffle` argument to `BucketBatchSampler` which allows for disabling shuffling.\r\n- Added `allennlp.modules.transformer.attention_module` which contains a generalized `AttentionModule`. `SelfAttention` and `T5Attention` both inherit from this.\r\n- Added a `Constraint` abstract class to `BeamSearch`, which allows for incorporating constraints on the predictions found by `BeamSearch`,\r\n  along with a `RepeatedNGramBlockingConstraint` constraint implementation, which allows for preventing repeated n-grams in the output from `BeamSearch`.\r\n- Added `DataCollator` for dynamic operations for each batch.\r\n\r\n### Changed \u26a0\ufe0f\r\n\r\n- Use `dist_reduce_sum` in distributed metrics.\r\n- Allow Google Cloud Storage paths in `cached_path` (\"gs://...\").\r\n- Renamed `nn.util.load_state_dict()` to `read_state_dict` to avoid confusion with `torch.nn.Module.load_state_dict()`.\r\n- `TransformerModule.from_pretrained_module` now only accepts a pretrained model ID (e.g. \"bert-base-case\") instead of\r\n  an actual `torch.nn.Module`. Other parameters to this method have changed as well.\r\n- Print the first batch to the console by default.\r\n- Renamed `sanity_checks` to `confidence_checks` (`sanity_checks` is deprecated and will be removed in AllenNLP 3.0).\r\n- Trainer callbacks can now store and restore state in case a training run gets interrupted.\r\n- VilBERT backbone now rolls and unrolls extra dimensions to handle input with > 3 dimensions.\r\n- `BeamSearch` is now a `Registrable` class.\r\n\r\n### Fixed \u2705\r\n\r\n- When `PretrainedTransformerIndexer` folds long sequences, it no longer loses the information from token type ids.\r\n- Fixed documentation for `GradientDescentTrainer.cuda_device`.\r\n- Re-starting a training run from a checkpoint in the middle of an epoch now works correctly.\r\n- When using the \"moving average\" weights smoothing feature of the trainer, training checkpoints would also get smoothed, with strange results for resuming a training job. This has been fixed.\r\n- When re-starting an interrupted training job, the trainer will now read out the data loader even for epochs and batches that can be skipped. We do this to try to get any random number generators used by the reader or data loader into the same state as they were the first time the training job ran.\r\n- Fixed the potential for a race condition with `cached_path()` when extracting archives. Although the race condition\r\n  is still possible if used with `force_extract=True`.\r\n- Fixed `wandb` callback to work in distributed training.\r\n- Fixed `tqdm` logging into multiple files with `allennlp-optuna`.\r\n\r\n## Commits\r\n\r\nb92fd9a7 Contextualized bias mitigation (#5176)\r\naa52a9a0 Checklist fixes (#5239)\r\n62067973 Fix tqdm logging into multiple files with allennlp-optuna (#5235)\r\nb0aa1d45 Generalize T5 modules (#5166)\r\n5b111d08 tick version for nightly release\r\n39d7e5ae Make BeamSearch Registrable (#5231)\r\nc0142320 Add constraints to beam search (#5216)\r\n98dae7f4 Emergency fix. I forgot to take this out.\r\nc5bff8ba Fixes Checkpointing (#5220)\r\n3d5799d8 Roll backbone (#5229)\r\nbabc450d Added `DataCollator` for dynamic operations for each batch. (#5221)\r\nd97ed401 Bump checklist from 0.0.10 to 0.0.11 (#5222)\r\n12155c40 fix race condition when extracting files with cached_path (#5227)\r\nd6629772 cancel redundant GH Actions workflows (#5226)\r\n2d8f3904 Fix W&B callback for distributed training (#5223)\r\n59df2ad3 Update nr-interface requirement from <0.0.4 to <0.0.6 (#5213)\r\n3e1b553b Bump black from 20.8b1 to 21.5b1 (#5195)\r\nd2840cba save meta data with model archives (#5209)\r\nbd941c6f added shuffle disable option in BucketBatchSampler (#5212)\r\n3585c9fe Implementing abstraction to score final sequences in `BeamSearch` (#5208)\r\n79d16af1 Add a `min_steps` parameter to `BeamSearch` (#5207)\r\ncf113d70 Changes and improvements to how we initialize transformer modules from pretrained models (#5200)\r\ncccb35de Rename sanity_checks to confidence_checks (#5201)\r\ndb8ff675 Update transformers requirement from <4.6,>=4.1 to >=4.1,<4.7 (#5199)\r\nfd5c9e4c Bias Metrics (#5139)\r\nd9b19b69 Bias Mitigation and Direction Methods (#5130)\r\n74737373 add diff command (#5109)\r\nd85c5c3a Explicitly pass serialization directory and local rank to trainer in train command (#5180)\r\n96c3caf9 fix nltk downloads in install (#5189)\r\nb1b455a2 improve contributing guide / PR template (#5185)\r\n7a260da9 fix cuda_device docs (#5188)\r\n0bf590df Update Makefile (#5183)\r\n3335700c Default print first batch (#5175)\r\nb533733a Refactor span extractors and unify forward. (#5160)\r\n01b232fb Allow google cloud storage locations for cached_path (#5173)\r\neb2ae30e Update README.md (#5165)\r\n55efa683 fix dataclasses import (#5169)\r\na463e0e7 Add way of skipping pretrained weights download (#5172)\r\nc71bb460 improve err msg for PolynomialDecay LR scheduler (#5143)\r\n530dae43 Simplify metrics (#5154)\r\n12f5b0f5 Run some slow tests on the self-hosted runner (#5161)\r\n90915800 Fixes token type ids for folded sequences (#5149)\r\n10400e02 Run checklist suites in AllenNLP (#5065)\r\nd11359ed make dist_reduce_sum work for tensors (#5147)\r\n9184fbcb Fixes Backbone / Model MRO inconsistency (#5148)", "discussion_url": "https://github.com/allenai/allennlp/discussions/5241", "reactions": {"url": "https://api.github.com/repos/allenai/allennlp/releases/44059690/reactions", "total_count": 3, "+1": 0, "-1": 0, "laugh": 0, "hooray": 3, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/41874604", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/41874604/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/41874604/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v2.4.0", "id": 41874604, "author": {"login": "dirkgr", "id": 920638, "node_id": "MDQ6VXNlcjkyMDYzOA==", "avatar_url": "https://avatars.githubusercontent.com/u/920638?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dirkgr", "html_url": "https://github.com/dirkgr", "followers_url": "https://api.github.com/users/dirkgr/followers", "following_url": "https://api.github.com/users/dirkgr/following{/other_user}", "gists_url": "https://api.github.com/users/dirkgr/gists{/gist_id}", "starred_url": "https://api.github.com/users/dirkgr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dirkgr/subscriptions", "organizations_url": "https://api.github.com/users/dirkgr/orgs", "repos_url": "https://api.github.com/users/dirkgr/repos", "events_url": "https://api.github.com/users/dirkgr/events{/privacy}", "received_events_url": "https://api.github.com/users/dirkgr/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTQxODc0NjA0", "tag_name": "v2.4.0", "target_commitish": "main", "name": "", "draft": false, "prerelease": false, "created_at": "2021-04-23T00:14:39Z", "published_at": "2021-04-23T00:15:51Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v2.4.0", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v2.4.0", "body": "## What's new\r\n\r\n### Added \ud83c\udf89\r\n\r\n- Added a T5 implementation to `modules.transformers`.\r\n\r\n### Changed \u26a0\ufe0f\r\n\r\n- Weights & Biases callback can now work in anonymous mode (i.e. without the `WANDB_API_KEY` environment variable).\r\n\r\n### Fixed \u2705\r\n\r\n- The `GradientDescentTrainer` no longer leaves stray model checkpoints around when it runs out of patience.\r\n- Fixed `cached_path()` for \"hf://\" files.\r\n\r\n## Commits\r\n\r\n7c5cc98a Don't orphan checkpoints when we run out of patience (#5142)\r\n6ec64596 allow W&B anon mode (#5110)\r\n4e862a54 T5 (#4969)\r\n7fc5a91f fix cached_path for hub downloads (#5141)\r\nf877fdc3 Fairness Metrics (#5093)\r\n", "discussion_url": "https://github.com/allenai/allennlp/discussions/5146"}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/41742285", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/41742285/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/41742285/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v2.3.1", "id": 41742285, "author": {"login": "epwalsh", "id": 8812459, "node_id": "MDQ6VXNlcjg4MTI0NTk=", "avatar_url": "https://avatars.githubusercontent.com/u/8812459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/epwalsh", "html_url": "https://github.com/epwalsh", "followers_url": "https://api.github.com/users/epwalsh/followers", "following_url": "https://api.github.com/users/epwalsh/following{/other_user}", "gists_url": "https://api.github.com/users/epwalsh/gists{/gist_id}", "starred_url": "https://api.github.com/users/epwalsh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/epwalsh/subscriptions", "organizations_url": "https://api.github.com/users/epwalsh/orgs", "repos_url": "https://api.github.com/users/epwalsh/repos", "events_url": "https://api.github.com/users/epwalsh/events{/privacy}", "received_events_url": "https://api.github.com/users/epwalsh/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTQxNzQyMjg1", "tag_name": "v2.3.1", "target_commitish": "main", "name": "v2.3.1", "draft": false, "prerelease": false, "created_at": "2021-04-20T21:44:23Z", "published_at": "2021-04-20T21:46:17Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v2.3.1", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v2.3.1", "body": "## What's new\r\n\r\n### Added \ud83c\udf89\r\n\r\n- Added support for the HuggingFace Hub as an alternative way to handle loading files through `cached_path()`. Hub downloads should be made through the `hf://` URL scheme.\r\n- Add new dimension to the `interpret` module: influence functions via the `InfluenceInterpreter` base class, along with a concrete implementation: `SimpleInfluence`.\r\n- Added a `quiet` parameter to the `MultiProcessDataLoading` that disables `Tqdm` progress bars.\r\n- The test for distributed metrics now takes a parameter specifying how often you want to run it.\r\n\r\n### Changed \u26a0\ufe0f\r\n\r\n- Updated CONTRIBUTING.md to remind reader to upgrade pip setuptools to avoid spaCy installation issues.\r\n\r\n### Fixed \u2705\r\n\r\n- Fixed a bug with the `ShardedDatasetReader` when used with multi-process data loading (https://github.com/allenai/allennlp/issues/5132).\r\n\r\n## Commits\r\n\r\na84b9b1a Add cached_path support for HF hub (#5052)\r\n24ec7db4 fix #5132 (#5134)\r\n2526674f Update CONTRIBUTING.md (#5133)\r\nc2ffb101 Add influence functions to interpret module (#4988)\r\n0c7d60bc Take the number of runs in the test for distributed metrics (#5127)\r\n8be3828f fix docs CI", "discussion_url": "https://github.com/allenai/allennlp/discussions/5137"}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/41446725", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/41446725/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/41446725/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v2.3.0", "id": 41446725, "author": {"login": "AkshitaB", "id": 6500683, "node_id": "MDQ6VXNlcjY1MDA2ODM=", "avatar_url": "https://avatars.githubusercontent.com/u/6500683?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AkshitaB", "html_url": "https://github.com/AkshitaB", "followers_url": "https://api.github.com/users/AkshitaB/followers", "following_url": "https://api.github.com/users/AkshitaB/following{/other_user}", "gists_url": "https://api.github.com/users/AkshitaB/gists{/gist_id}", "starred_url": "https://api.github.com/users/AkshitaB/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AkshitaB/subscriptions", "organizations_url": "https://api.github.com/users/AkshitaB/orgs", "repos_url": "https://api.github.com/users/AkshitaB/repos", "events_url": "https://api.github.com/users/AkshitaB/events{/privacy}", "received_events_url": "https://api.github.com/users/AkshitaB/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTQxNDQ2NzI1", "tag_name": "v2.3.0", "target_commitish": "main", "name": "v2.3.0", "draft": false, "prerelease": false, "created_at": "2021-04-14T22:48:44Z", "published_at": "2021-04-14T22:50:34Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v2.3.0", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v2.3.0", "body": "## What's new\r\n\r\n### Added \ud83c\udf89\r\n\r\n- Ported the following Huggingface `LambdaLR`-based schedulers: `ConstantLearningRateScheduler`, `ConstantWithWarmupLearningRateScheduler`, `CosineWithWarmupLearningRateScheduler`, `CosineHardRestartsWithWarmupLearningRateScheduler`.\r\n- Added new `sub_token_mode` parameter to `pretrained_transformer_mismatched_embedder` class to support first sub-token embedding\r\n- Added a way to run a multi task model with a dataset reader as part of `allennlp predict`.\r\n- Added new `eval_mode` in `PretrainedTransformerEmbedder`. If it is set to `True`, the transformer is _always_ run in evaluation mode, which, e.g., disables dropout and does not update batch normalization statistics.\r\n- Added additional parameters to the W&B callback: `entity`, `group`, `name`, `notes`, and `wandb_kwargs`.\r\n\r\n### Changed \u26a0\ufe0f\r\n\r\n- Sanity checks in the `GradientDescentTrainer` can now be turned off by setting the `run_sanity_checks` parameter to `False`.\r\n- Allow the order of examples in the task cards to be specified explicitly\r\n- `histogram_interval` parameter is now deprecated in `TensorboardWriter`, please use `distribution_interval` instead.\r\n- Memory usage is not logged in tensorboard during training now. `ConsoleLoggerCallback` should be used instead.\r\n- If you use the `min_count` parameter of the Vocabulary, but you specify a namespace that does not exist, the vocabulary creation will raise a `ConfigurationError`.\r\n- Documentation updates made to SoftmaxLoss regarding padding and the expected shapes of the input and output tensors of `forward`.\r\n- Moved the data preparation script for coref into allennlp-models.\r\n- If a transformer is not in cache but has override weights, the transformer's pretrained weights are no longer downloaded, that is, only its `config.json` file is downloaded.\r\n- `SanityChecksCallback` now raises `SanityCheckError` instead of `AssertionError` when a check fails.\r\n- `jsonpickle` removed from dependencies.\r\n- Improved the error message from `Registrable.by_name()` when the name passed does not match any registered subclassess.\r\n  The error message will include a suggestion if there is a close match between the name passed and a registered name.\r\n\r\n### Fixed \u2705\r\n\r\n- Fixed a bug where some `Activation` implementations could not be pickled due to involving a lambda function.\r\n- Fixed `__str__()` method on `ModelCardInfo` class.\r\n- Fixed a stall when using distributed training and gradient accumulation at the same time\r\n- Fixed an issue where using the `from_pretrained_transformer` `Vocabulary` constructor in distributed training via the `allennlp train` command\r\n  would result in the data being iterated through unnecessarily.\r\n- Fixed a bug regarding token indexers with the `InterleavingDatasetReader` when used with multi-process data loading.\r\n- Fixed a warning from `transformers` when using `max_length` in the `PretrainedTransformerTokenizer`.\r\n\r\n### Removed \ud83d\udc4b\r\n\r\n- Removed the `stride` parameter to `PretrainedTransformerTokenizer`. This parameter had no effect.\r\n\r\n## Commits\r\n\r\nc80e1751 improve error message from Registrable class (#5125)\r\naca16237 Update docstring for basic_classifier (#5124)\r\n059a64fc remove jsonpickle from dependencies (#5121)\r\n5fdce9ad fix bug with interleaving dataset reader (#5122)\r\n6e1f34cb Predicting with a dataset reader on a multitask model (#5115)\r\nb34df73e specify 'truncation' to avoid transformers warning (#5120)\r\n0ddd3d35 Add eval_mode argument to pretrained transformer embedder (#5111)\r\n99415e36 additional W&B params (#5114)\r\n6ee12123 Adding a metadata field to the basic classifier (#5104)\r\n2e8c3e2f Add link to gallery and demo in README (#5103)\r\nde611008 Distributed training with gradient accumulation (#5100)\r\nfe2d6e5a vocab fix (#5099)\r\nd906175d Update transformers requirement from <4.5,>=4.1 to >=4.1,<4.6 (#5102)\r\n99da3156 fix __str__ method of ModelCardInfo (#5096)\r\n29f00ee2 Added new parameter 'sub_token_mode' to 'pretrained_transformer_mismatched_embedder' class to support first sub-token embedding (#4363) (#5087)\r\n6021f7d4 Avoid from_pretrained download of model weights (#5085)\r\nc3fb97eb add SanityCheckError class (#5092)\r\ndecb875b Bring back `run_sanity_checks` parameter (#5091)\r\n913fb8a4 Update mkdocs-material requirement from <7.1.0,>=5.5.0 to >=5.5.0,<7.2.0 (#5074)\r\nf82d3f11 remove lambdas from activations (#5083)\r\nbb703494 Replace master references with main in issue template (#5084)\r\n87504c42 Ported Huggingface LambdaLR-based schedulers (#5082)\r\n63a3b489 set transformer to evaluation mode (#5073)\r\n542ce5d9 Move coref prep script (#5078)\r\nbf8e71e9 compare namespace in counter and min_count (#3644)\r\n4baf19ab Arjuns/softmax loss documentation update (#5075)\r\n59b92106 Allow example categories to be ordered (#5059)\r\n3daa0baf tick version for nightly\r\nbb77bd10 fix date in CHANGELOG\r\n"}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/40553822", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/40553822/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/40553822/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v2.2.0", "id": 40553822, "author": {"login": "epwalsh", "id": 8812459, "node_id": "MDQ6VXNlcjg4MTI0NTk=", "avatar_url": "https://avatars.githubusercontent.com/u/8812459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/epwalsh", "html_url": "https://github.com/epwalsh", "followers_url": "https://api.github.com/users/epwalsh/followers", "following_url": "https://api.github.com/users/epwalsh/following{/other_user}", "gists_url": "https://api.github.com/users/epwalsh/gists{/gist_id}", "starred_url": "https://api.github.com/users/epwalsh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/epwalsh/subscriptions", "organizations_url": "https://api.github.com/users/epwalsh/orgs", "repos_url": "https://api.github.com/users/epwalsh/repos", "events_url": "https://api.github.com/users/epwalsh/events{/privacy}", "received_events_url": "https://api.github.com/users/epwalsh/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTQwNTUzODIy", "tag_name": "v2.2.0", "target_commitish": "main", "name": "v2.2.0", "draft": false, "prerelease": false, "created_at": "2021-03-26T17:00:21Z", "published_at": "2021-03-26T17:11:08Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v2.2.0", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v2.2.0", "body": "## What's new\r\n\r\n### Added \ud83c\udf89\r\n\r\n- Added `WandBCallback` class for [Weights & Biases](https://wandb.ai) integration, registered as a callback under the name \"wandb\".\r\n- Added `TensorBoardCallback` to replace the `TensorBoardWriter`. Registered as a callback\r\n  under the name \"tensorboard\".\r\n- Added `NormalizationBiasVerification` and `SanityChecksCallback` for model sanity checks.\r\n- `SanityChecksCallback` runs by default from the `allennlp train` command.\r\n  It can be turned off by setting `trainer.enable_default_callbacks` to `false` in your config.\r\n- Added new method on `Field` class: `.human_readable_repr() -> Any`, and new method on `Instance` class: `.human_readable_dict() -> JsonDict` (@leo-liuzy).\r\n\r\n### Removed \ud83d\udc4b\r\n\r\n- Removed `TensorBoardWriter`. Please use the `TensorBoardCallback` instead.\r\n\r\n### Changed \u26a0\ufe0f\r\n\r\n- Use attributes of `ModelOutputs` object in `PretrainedTransformerEmbedder` instead of indexing (@JohnGiorgi).\r\n- Added support for PyTorch version 1.8 and `torchvision` version 0.9 (@nelson-liu).\r\n- `Model.get_parameters_for_histogram_tensorboard_logging` is deprecated in favor of\r\n  `Model.get_parameters_for_histogram_logging`.\r\n\r\n### Fixed \u2705\r\n\r\n- Makes sure tensors that are stored in `TensorCache` always live on CPUs.\r\n- Fixed a bug where `FromParams` objects wrapped in `Lazy()` couldn't be pickled.\r\n- Fixed a bug where the `ROUGE` metric couldn't be picked.\r\n- Fixed a bug reported by https://github.com/allenai/allennlp/issues/5036 - we now keep our spacy POS tagger on (@leo-liuzy).\r\n\r\n## Commits\r\n\r\nc5c9df58 refactor LogWriter, add W&B integration (#5061)\r\n385124ad Keep Spacy PoS tagger on by default (#5066)\r\n15b532fb Update transformers requirement from <4.4,>=4.1 to >=4.1,<4.5 (#5057)\r\n3aafb927 clarify how  `predictions_to_labeled_instances` work for targeted or non-targeted hotflip attack (#4957)\r\nb897e57c ensure ROUGE metric can be pickled (#5051)\r\n91e4af94 fix pickle bug for Lazy FromParams (#5049)\r\n5b57be29 Adding normalization bias verification (#4990)\r\nce71901a Update torchvision requirement from <0.9.0,>=0.8.1 to >=0.8.1,<0.10.0 (#5041)\r\n7f609901 Update torch requirement from <1.8.0,>=1.6.0 to >=1.6.0,<1.9.0 (#5037)\r\n96415b2b Use HF Transformers output types (#5035)\r\n0c36019c clean up (#5034)\r\nd2bf35d1 Add methods for human readable representation of fields and instances (#4986)\r\na8b80069 Makes sure serialized tensors live on CPUs (#5026)\r\na0edfae9 Add options to log inputs in trainer (#4970)\r\n\r\n---\r\n\r\nThanks to @nelson-liu for making sure we stay on top of releases! \ud83d\ude1c"}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/39085330", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/39085330/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/39085330/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v1.5.0", "id": 39085330, "author": {"login": "dirkgr", "id": 920638, "node_id": "MDQ6VXNlcjkyMDYzOA==", "avatar_url": "https://avatars.githubusercontent.com/u/920638?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dirkgr", "html_url": "https://github.com/dirkgr", "followers_url": "https://api.github.com/users/dirkgr/followers", "following_url": "https://api.github.com/users/dirkgr/following{/other_user}", "gists_url": "https://api.github.com/users/dirkgr/gists{/gist_id}", "starred_url": "https://api.github.com/users/dirkgr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dirkgr/subscriptions", "organizations_url": "https://api.github.com/users/dirkgr/orgs", "repos_url": "https://api.github.com/users/dirkgr/repos", "events_url": "https://api.github.com/users/dirkgr/events{/privacy}", "received_events_url": "https://api.github.com/users/dirkgr/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTM5MDg1MzMw", "tag_name": "v1.5.0", "target_commitish": "main", "name": "", "draft": false, "prerelease": false, "created_at": "2021-03-01T19:28:51Z", "published_at": "2021-03-01T23:30:13Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v1.5.0", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v1.5.0", "body": "## What's new\r\n\r\n### Added \ud83c\udf89\r\n\r\n- Added a way to specify extra parameters to the predictor in an `allennlp predict` call.\r\n- Added a way to initialize a `Vocabulary` from transformers models.\r\n- Support spaCy v3\r\n\r\n### Changed \u26a0\ufe0f\r\n\r\n- Updated `Paper` and `Dataset` classes in `ModelCard`.\r\n\r\n## Commits\r\n\r\n55ac96a0 re-write docs commit history on releases (#4968)\r\nc61178fa Update spaCy to 3.0 (#4953)\r\nbe595dfd Ensure mean absolute error metric returns a float (#4983)\r\n25562234 raise on HTTP errors in cached_path (#4984)\r\ne1839cfe Inputs to the FBetaMultiLabel metric were copied and pasted wrong (#4975)\r\nb5b72a06 Add method to vocab to instantiate from a pretrained transformer (#4958)\r\n025a0b28 Allows specifying extra arguments for predictors (#4947)\r\n24c9c995 adding ModelUsage, rearranging fields (#4952)\r\n"}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/38679298", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/38679298/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/38679298/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v2.1.0", "id": 38679298, "author": {"login": "dirkgr", "id": 920638, "node_id": "MDQ6VXNlcjkyMDYzOA==", "avatar_url": "https://avatars.githubusercontent.com/u/920638?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dirkgr", "html_url": "https://github.com/dirkgr", "followers_url": "https://api.github.com/users/dirkgr/followers", "following_url": "https://api.github.com/users/dirkgr/following{/other_user}", "gists_url": "https://api.github.com/users/dirkgr/gists{/gist_id}", "starred_url": "https://api.github.com/users/dirkgr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dirkgr/subscriptions", "organizations_url": "https://api.github.com/users/dirkgr/orgs", "repos_url": "https://api.github.com/users/dirkgr/repos", "events_url": "https://api.github.com/users/dirkgr/events{/privacy}", "received_events_url": "https://api.github.com/users/dirkgr/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTM4Njc5Mjk4", "tag_name": "v2.1.0", "target_commitish": "main", "name": "", "draft": false, "prerelease": false, "created_at": "2021-02-24T21:19:20Z", "published_at": "2021-02-24T21:20:22Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v2.1.0", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v2.1.0", "body": "## What's new\r\n\r\n### Changed \u26a0\ufe0f\r\n\r\n- `coding_scheme` parameter is now deprecated in `Conll2003DatasetReader`, please use `convert_to_coding_scheme` instead.\r\n- Support spaCy v3\r\n\r\n### Added \ud83c\udf89\r\n\r\n- Added `ModelUsage` to `ModelCard` class.\r\n- Added a way to specify extra parameters to the predictor in an `allennlp predict` call.\r\n- Added a way to initialize a `Vocabulary` from transformers models.\r\n- Added the ability to use `Predictors` with multitask models through the new `MultiTaskPredictor`.\r\n- Added an example for fields of type `ListField[TextField]` to `apply_token_indexers` API docs.\r\n- Added `text_key` and `label_key` parameters to `TextClassificationJsonReader` class.\r\n- Added `MultiOptimizer`, which allows you to use different optimizers for different parts of your model.\r\n\r\n### Fixed \u2705\r\n\r\n- `@Registrable.register(...)` decorator no longer masks the decorated class's annotations\r\n- Ensured that `MeanAbsoluteError` always returns a `float` metric value instead of a `Tensor`.\r\n- Learning rate schedulers that rely on metrics from the validation set were broken in v2.0.0. This\r\n  brings that functionality back.\r\n- Fixed a bug where the `MultiProcessDataLoading` would crash when `num_workers > 0`, `start_method = \"spawn\"`, `max_instances_in_memory not None`, and `batches_per_epoch not None`.\r\n- Fixed documentation and validation checks for `FBetaMultiLabelMetric`.\r\n- Fixed handling of HTTP errors when fetching remote resources with `cached_path()`. Previously the content would be cached even when\r\n  certain errors - like 404s - occurred. Now an `HTTPError` will be raised whenever the HTTP response is not OK.\r\n- Fixed a bug where the `MultiTaskDataLoader` would crash when `num_workers > 0`\r\n- Fixed an import error that happens when PyTorch's distributed framework is unavailable on the system.\r\n\r\n## Commits\r\n\r\n7c6adeff Fix worker_info bug when num_workers > 0 (#5013)\r\n9d88f8c5 Fixes predictors in the multitask case (#4991)\r\n678518a0 Less opaque registrable annotations (#5010)\r\n4b5fad46 Regex optimizer (#4981)\r\nf091cb9c fix error when torch.distributed not available (#5011)\r\n5974f54e Revert \"drop support for Python 3.6 (#5012)\" (#5016)\r\nbdb0e20a Update mkdocs-material requirement from <6.3.0,>=5.5.0 to >=5.5.0,<7.1.0 (#5015)\r\nd535de67 Bump mypy from 0.800 to 0.812 (#5007)\r\n099786cf Update responses requirement, remove pin on urllib3 (#4783)\r\nb8cfb95c re-write docs commit history on releases (#4968)\r\nc5c9edf0 Add text_key and label_key to TextClassificationJsonReader (#5005)\r\na02f67da drop support for Python 3.6 (#5012)\r\n0078c595 Update spaCy to 3.0 (#4953)\r\nbe9537f6 Update CHANGELOG.md\r\n828ee101 Update CHANGELOG.md\r\n1cff6ad9 update README (#4993)\r\nf8b38075 Add ListField example to apply token indexers (#4987)\r\n7961b8b7 Ensure mean absolute error metric returns a float (#4983)\r\nda4dba15 raise on HTTP errors in cached_path (#4984)\r\nd4926f5e Inputs to the FBetaMultiLabel metric were copied and pasted wrong (#4975)\r\nd2ae540d Update transformers requirement from <4.3,>=4.1 to >=4.1,<4.4 (#4967)\r\nbf8eeafe Add method to vocab to instantiate from a pretrained transformer (#4958)\r\n9267ce7c Resize transformers word embeddings layer for additional_special_tokens (#4946)\r\n52c23dd2 Introduce `convert_to_coding_scheme` and make `coding_scheme` deprecated in CoNLL2003DatasetReader (#4960)\r\nc418f84b Fixes recording validation metrics for learning rate schedulers that rely on it (#4959)\r\n4535f5c8 adding ModelUsage, rearranging fields (#4952)\r\n1ace4bbb fix bug with MultiProcessDataLoader (#4956)\r\n6f222919 Allows specifying extra arguments for predictors (#4947)\r\n2731db12 tick version for nightly release\r\n"}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/37106776", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/37106776/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/37106776/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v2.0.1", "id": 37106776, "author": {"login": "epwalsh", "id": 8812459, "node_id": "MDQ6VXNlcjg4MTI0NTk=", "avatar_url": "https://avatars.githubusercontent.com/u/8812459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/epwalsh", "html_url": "https://github.com/epwalsh", "followers_url": "https://api.github.com/users/epwalsh/followers", "following_url": "https://api.github.com/users/epwalsh/following{/other_user}", "gists_url": "https://api.github.com/users/epwalsh/gists{/gist_id}", "starred_url": "https://api.github.com/users/epwalsh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/epwalsh/subscriptions", "organizations_url": "https://api.github.com/users/epwalsh/orgs", "repos_url": "https://api.github.com/users/epwalsh/repos", "events_url": "https://api.github.com/users/epwalsh/events{/privacy}", "received_events_url": "https://api.github.com/users/epwalsh/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTM3MTA2Nzc2", "tag_name": "v2.0.1", "target_commitish": "main", "name": "v2.0.1", "draft": false, "prerelease": false, "created_at": "2021-01-29T21:46:31Z", "published_at": "2021-01-29T21:48:11Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v2.0.1", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v2.0.1", "body": "## What's new\r\n\r\nA couple minors fixes and additions since the 2.0 release.\r\n\r\n### Added \ud83c\udf89\r\n\r\n- Added `tokenizer_kwargs` and `transformer_kwargs` arguments to `PretrainedTransformerBackbone`\r\n\r\n### Changed \u26a0\ufe0f\r\n\r\n- GradientDescentTrainer makes `serialization_dir` when it's instantiated, if it doesn't exist.\r\n\r\n### Fixed \u2705\r\n\r\n- `common.util.sanitize` now handles sets.\r\n\r\n## Commits\r\n\r\ncaa497f3 Update `GradientDescentTrainer` to automatically create directory for `serialization_dir` (#4940)\r\ncd96d953 Sanitize set (#4945)\r\nf0ae9f3c Adding tokenizer_kwargs argument to PretrainedTransformerBackbone constructor. (#4944)\r\n501b0ab4 Fixing papers and datasets (#4919)\r\nfa625ec0 Adding missing transformer_kwargs arg that was recently added to PretrainedTransformerEmbedder (#4941)\r\n96ea4839 Add missing \"Unreleased\" section to CHANGELOG"}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/37110057", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/37110057/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/37110057/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v1.4.1", "id": 37110057, "author": {"login": "AkshitaB", "id": 6500683, "node_id": "MDQ6VXNlcjY1MDA2ODM=", "avatar_url": "https://avatars.githubusercontent.com/u/6500683?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AkshitaB", "html_url": "https://github.com/AkshitaB", "followers_url": "https://api.github.com/users/AkshitaB/followers", "following_url": "https://api.github.com/users/AkshitaB/following{/other_user}", "gists_url": "https://api.github.com/users/AkshitaB/gists{/gist_id}", "starred_url": "https://api.github.com/users/AkshitaB/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AkshitaB/subscriptions", "organizations_url": "https://api.github.com/users/AkshitaB/orgs", "repos_url": "https://api.github.com/users/AkshitaB/repos", "events_url": "https://api.github.com/users/AkshitaB/events{/privacy}", "received_events_url": "https://api.github.com/users/AkshitaB/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTM3MTEwMDU3", "tag_name": "v1.4.1", "target_commitish": "main", "name": "v1.4.1", "draft": false, "prerelease": false, "created_at": "2021-01-29T22:05:26Z", "published_at": "2021-01-29T23:34:22Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v1.4.1", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v1.4.1", "body": "## What's new\r\n\r\n#### Note: This release is mainly for the AllenNLP demo.\r\n\r\n### Changed \u26a0\ufe0f\r\n\r\n- Updated `Paper` and `Dataset` classes in `ModelCard`.\r\n\r\n## Commits\r\n\r\n14b717c8 Update `GradientDescentTrainer` to automatically create directory for `serialization_dir` (#4940)\r\ne262352f Fixing papers and datasets (#4919)\r\n\r\n"}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/36306898", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/36306898/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/36306898/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v2.0.0", "id": 36306898, "author": {"login": "epwalsh", "id": 8812459, "node_id": "MDQ6VXNlcjg4MTI0NTk=", "avatar_url": "https://avatars.githubusercontent.com/u/8812459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/epwalsh", "html_url": "https://github.com/epwalsh", "followers_url": "https://api.github.com/users/epwalsh/followers", "following_url": "https://api.github.com/users/epwalsh/following{/other_user}", "gists_url": "https://api.github.com/users/epwalsh/gists{/gist_id}", "starred_url": "https://api.github.com/users/epwalsh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/epwalsh/subscriptions", "organizations_url": "https://api.github.com/users/epwalsh/orgs", "repos_url": "https://api.github.com/users/epwalsh/repos", "events_url": "https://api.github.com/users/epwalsh/events{/privacy}", "received_events_url": "https://api.github.com/users/epwalsh/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTM2MzA2ODk4", "tag_name": "v2.0.0", "target_commitish": "main", "name": "v2.0.0", "draft": false, "prerelease": false, "created_at": "2021-01-27T23:28:36Z", "published_at": "2021-01-27T23:30:29Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v2.0.0", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v2.0.0", "body": "# AllenNLP v2.0.0 Release Notes\r\n\r\nThe 2.0 release of AllenNLP represents a major engineering effort that brings several exciting new features to the library, as well as a focus on performance.\r\n\r\nIf you're upgrading from AllenNLP 1.x, we encourage you to read our comprehensive [upgrade guide](https://github.com/allenai/allennlp/discussions/4933).\r\n\r\n## Main new features\r\n\r\n### AllenNLP gets eyes \ud83d\udc40\r\n\r\nOne of the most exciting areas in ML research is multimodal learning, and AllenNLP is now taking its first steps in this direction with support for 2 tasks and 3 datasets in the vision + text domain. Check out our [ViLBERT for VQA](https://docs.allennlp.org/models/v2.0.0/models/vision/models/vilbert_vqa/#vqavilbert) and [Visual Entailment](https://docs.allennlp.org/models/v2.0.0/models/vision/models/visual_entailment/#visualentailmentmodel) models, along with the [VQAv2](https://docs.allennlp.org/models/v2.0.0/models/vision/dataset_readers/vqav2/#vqav2reader), [Visual Entailment](https://docs.allennlp.org/models/v2.0.0/models/vision/dataset_readers/visual_entailment/#visualentailmentreader), and [GQA](https://docs.allennlp.org/models/v2.0.0/models/vision/dataset_readers/gqa/#gqareader) dataset readers in [`allennlp-models`](https://github.com/allenai/allennlp-models).\r\n\r\n### Transformer toolkit\r\n\r\nThe transformer toolkit offers a collection of modules to experiment with various transformer architectures, such as `SelfAttention`, `TransformerEmbeddings`, `TransformerLayer`, etc. It also simplifies the way one can take apart the pretrained transformer weights for an existing module, and combine them in different ways. For instance, one can pull out the first 8 layers of `bert-base-uncased` to separately encode two text inputs, combine the representations in some way, and then use the last 4 layers on the combined representation (More examples can be found in `allennlp.modules.transformer`). \r\n\r\nThe toolkit also contains modules for bimodal architectures such as ViLBERT. Modules include `BiModalEncoder`, which encodes two modalities separately, and performs bi-directional attention (`BiModalAttention`) using a connection layer (`BiModalConnectionLayer`). The `VisionTextModel` class is an example of a model that uses these bimodal layers.\r\n\r\n### Multi-task learning\r\n\r\n2.0 adds support for multi-task learning throughout the AllenNLP system. In multi-task learning, the model consists of a backbone that is common to all the tasks, and tends to be the larger part of the model, and multiple task-specific heads that use the output of the backbone to make predictions for a specific task. This way, the backbone gets many more training examples than you might have available for a single task, and can thus produce better representations, which makes all tasks benefit. The canonical example for this is BERT, where the backbone is made up of the transformer stack, and then there are multiple model heads that do classification, tagging, masked-token prediction, etc. AllenNLP 2.0 helps you build such models by giving you those abstractions. The `MultiTaskDatasetReader` can read datasets for multiple tasks at once. The `MultiTaskDataloader` loads the instances from the reader and makes batches. The trainer feeds these batches to a `MultiTaskModel`, which consists of a `Backbone` and multiple `Head`s. If you want to look at the details of how this works, we have an example config available at https://github.com/allenai/allennlp-models/blob/main/training_config/vision/vilbert_multitask.jsonnet. \r\n\r\n## Changes since `v2.0.0rc1`\r\n\r\n### Added \ud83c\udf89\r\n\r\n- The `TrainerCallback` constructor accepts `serialization_dir` provided by `Trainer`. This can be useful for `Logger` callbacks those need to store files in the run directory.\r\n- The `TrainerCallback.on_start()` is fired at the start of the training.\r\n- The `TrainerCallback` event methods now accept `**kwargs`. This may be useful to maintain backwards-compability of callbacks easier in the future. E.g. we may decide to pass the exception/traceback object in case of failure to `on_end()` and this older callbacks may simply ignore the argument instead of raising a `TypeError`.\r\n- Added a `TensorBoardCallback` which wraps the `TensorBoardWriter`.\r\n\r\n### Changed \u26a0\ufe0f\r\n\r\n- The `TrainerCallack.on_epoch()` does not fire with `epoch=-1` at the start of the training.\r\n  Instead, `TrainerCallback.on_start()` should be used for these cases.\r\n- `TensorBoardBatchMemoryUsage` is converted from `BatchCallback` into `TrainerCallback`.\r\n- `TrackEpochCallback` is converted from `EpochCallback` into `TrainerCallback`.\r\n- `Trainer` can accept callbacks simply with name `callbacks` instead of `trainer_callbacks`.\r\n- `TensorboardWriter` renamed to `TensorBoardWriter`, and removed as an argument to the `GradientDescentTrainer`.\r\n  In order to enable TensorBoard logging during training, you should utilize the `TensorBoardCallback` instead.\r\n\r\n### Removed \ud83d\udc4b\r\n\r\n- Removed `EpochCallback`, `BatchCallback` in favour of `TrainerCallback`.\r\n  The metaclass-wrapping implementation is removed as well.\r\n- Removed the `tensorboard_writer` parameter to `GradientDescentTrainer`. You should use the `TensorBoardCallback` now instead.\r\n\r\n### Fixed \u2705\r\n\r\n- Now Trainer always fires `TrainerCallback.on_end()` so all the resources can be cleaned up properly.\r\n- Fixed the misspelling, changed `TensoboardBatchMemoryUsage` to `TensorBoardBatchMemoryUsage`.\r\n- We set a value to `epoch` so in case of firing `TrainerCallback.on_end()` the variable is bound.\r\n  This could have lead to an error in case of trying to recover a run after it was finished training.\r\n\r\n## Commits since `v2.0.0rc1`\r\n\r\n15300823 Log to TensorBoard through a TrainerCallback in GradientDescentTrainer (#4913)\r\n8b95316b ci quick fix\r\nfa1dc7b8 Add link to upgrade guide to README (#4934)\r\n7364da03 Fix parameter name in the documentation\r\n00e3ff27 tick version for nightly release\r\n67fa291c Merging vision into main (#4800)\r\n65e50b30 Bump mypy from 0.790 to 0.800 (#4927)\r\na7445357 fix mkdocs config (#4923)\r\ned322eba A helper for distributed reductions (#4920)\r\n9ab2bf03 add CUDA 10.1 Docker image (#4921)\r\nd82287e5 Update transformers requirement from <4.1,>=4.0 to >=4.0,<4.2 (#4872)\r\n4183a49c Update mkdocs-material requirement from <6.2.0,>=5.5.0 to >=5.5.0,<6.3.0 (#4880)\r\n54e85eee disable codecov annotations (#4902)\r\n2623c4bf Making TrackEpochCallback an EpochCallback (#4893)\r\n1d21c759 issue warning instead of failing when lock can't be acquired on a resource that exists in a read-only file system (#4867)\r\nec197c3b Create pull_request_template.md (#4891)\r\n9cf41b2f fix navbar link\r\n9635af82 rename 'master' -> 'main' (#4887)\r\nd0a07fb3 docs: fix simple typo, multplication -> multiplication (#4883)\r\nd1f032d8 Moving modelcard and taskcard abstractions to main repo (#4881)\r\n1fff7cae Update docker torch version (#4873)\r\nd2aea979 Fix typo in __str__ (#4874)\r\n6a8d425f add CombinedLearningRateScheduler (#4871)\r\na3732d00 Fix cache volume (#4869)\r\n832901e8 Turn superfluous warning to info when extending the vocab in the embedding matrix (#4854)"}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/36943833", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/36943833/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/36943833/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v1.4.0", "id": 36943833, "author": {"login": "dirkgr", "id": 920638, "node_id": "MDQ6VXNlcjkyMDYzOA==", "avatar_url": "https://avatars.githubusercontent.com/u/920638?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dirkgr", "html_url": "https://github.com/dirkgr", "followers_url": "https://api.github.com/users/dirkgr/followers", "following_url": "https://api.github.com/users/dirkgr/following{/other_user}", "gists_url": "https://api.github.com/users/dirkgr/gists{/gist_id}", "starred_url": "https://api.github.com/users/dirkgr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dirkgr/subscriptions", "organizations_url": "https://api.github.com/users/dirkgr/orgs", "repos_url": "https://api.github.com/users/dirkgr/repos", "events_url": "https://api.github.com/users/dirkgr/events{/privacy}", "received_events_url": "https://api.github.com/users/dirkgr/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTM2OTQzODMz", "tag_name": "v1.4.0", "target_commitish": "main", "name": "v1.4.0", "draft": false, "prerelease": false, "created_at": "2021-01-27T00:26:44Z", "published_at": "2021-01-27T00:27:30Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v1.4.0", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v1.4.0", "body": "## What's new\r\n\r\n### Added \ud83c\udf89\r\n\r\n- Added a `FileLock` class to `common.file_utils`. This is just like the `FileLock` from the `filelock` library, except that\r\n  it adds an optional flag `read_only_ok: bool`, which when set to `True` changes the behavior so that a warning will be emitted\r\n  instead of an exception when lacking write permissions on an existing file lock.\r\n  This makes it possible to use the `FileLock` class on a read-only file system.\r\n- Added a new learning rate scheduler: `CombinedLearningRateScheduler`. This can be used to combine different LR schedulers, using one after the other.\r\n- Added an official CUDA 10.1 Docker image.\r\n- Moving `ModelCard` and `TaskCard` abstractions into the main repository.\r\n- Added a util function `allennlp.nn.util.dist_reduce(...)` for handling distributed reductions.\r\n  This is especially useful when implementing a distributed `Metric`.\r\n\r\n### Changed \u26a0\ufe0f\r\n\r\n- 'master' branch renamed to 'main'\r\n- Torch version bumped to 1.7.1 in Docker images.\r\n\r\n### Fixed \u2705\r\n\r\n- Fixed typo with `LabelField` string representation: removed trailing apostrophe.\r\n- `Vocabulary.from_files` and `cached_path` will issue a warning, instead of failing, when a lock on an existing resource\r\n  can't be acquired because the file system is read-only.\r\n- `TrackEpochCallback` is now a `EpochCallback`.\r\n\r\n## Commits\r\n\r\n4de78ac0 Make CI run properly on the 1.x branch\r\n65e50b30 Bump mypy from 0.790 to 0.800 (#4927)\r\na7445357 fix mkdocs config (#4923)\r\ned322eba A helper for distributed reductions (#4920)\r\n9ab2bf03 add CUDA 10.1 Docker image (#4921)\r\nd82287e5 Update transformers requirement from <4.1,>=4.0 to >=4.0,<4.2 (#4872)\r\n4183a49c Update mkdocs-material requirement from <6.2.0,>=5.5.0 to >=5.5.0,<6.3.0 (#4880)\r\n54e85eee disable codecov annotations (#4902)\r\n2623c4bf Making TrackEpochCallback an EpochCallback (#4893)\r\n1d21c759 issue warning instead of failing when lock can't be acquired on a resource that exists in a read-only file system (#4867)\r\nec197c3b Create pull_request_template.md (#4891)\r\n9cf41b2f fix navbar link\r\n9635af82 rename 'master' -> 'main' (#4887)\r\nd0a07fb3 docs: fix simple typo, multplication -> multiplication (#4883)\r\nd1f032d8 Moving modelcard and taskcard abstractions to main repo (#4881)\r\n1fff7cae Update docker torch version (#4873)\r\nd2aea979 Fix typo in __str__ (#4874)\r\n6a8d425f add CombinedLearningRateScheduler (#4871)\r\na3732d00 Fix cache volume (#4869)\r\n832901e8 Turn superfluous warning to info when extending the vocab in the embedding matrix (#4854)\r\n"}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/36744733", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/36744733/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/36744733/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v2.0.0rc1", "id": 36744733, "author": {"login": "dirkgr", "id": 920638, "node_id": "MDQ6VXNlcjkyMDYzOA==", "avatar_url": "https://avatars.githubusercontent.com/u/920638?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dirkgr", "html_url": "https://github.com/dirkgr", "followers_url": "https://api.github.com/users/dirkgr/followers", "following_url": "https://api.github.com/users/dirkgr/following{/other_user}", "gists_url": "https://api.github.com/users/dirkgr/gists{/gist_id}", "starred_url": "https://api.github.com/users/dirkgr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dirkgr/subscriptions", "organizations_url": "https://api.github.com/users/dirkgr/orgs", "repos_url": "https://api.github.com/users/dirkgr/repos", "events_url": "https://api.github.com/users/dirkgr/events{/privacy}", "received_events_url": "https://api.github.com/users/dirkgr/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTM2NzQ0NzMz", "tag_name": "v2.0.0rc1", "target_commitish": "main", "name": "v2.0.0rc1", "draft": false, "prerelease": true, "created_at": "2021-01-22T01:20:18Z", "published_at": "2021-01-22T01:20:43Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v2.0.0rc1", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v2.0.0rc1", "body": "This is the first (and hopefully only) release candidate for AllenNLP 2.0. Please note that this is a release candidate, and the APIs are still subject to change until the final 2.0 release. We'll provide a detailed writeup with the final 2.0 release, including a migration guide. In the meantime, here are the headline features of AllenNLP 2.0:\r\n* Support for models that combine language and vision features\r\n* Transformer Toolkit, a suite of classes and components that make it easy to experiment with transformer architectures\r\n* A framework for multitask training\r\n* Revamped data loading, for improved performance and flexibility\r\n\r\n\r\n## What's new\r\n\r\n### Added \ud83c\udf89\r\n\r\n- Added `TensorCache` class for caching tensors on disk\r\n- Added abstraction and concrete implementation for image loading\r\n- Added abstraction and concrete implementation for `GridEmbedder`\r\n- Added abstraction and demo implementation for an image augmentation module.\r\n- Added abstraction and concrete implementation for region detectors.\r\n- A new high-performance default `DataLoader`: `MultiProcessDataLoading`.\r\n- A `MultiTaskModel` and abstractions to use with it, including `Backbone` and `Head`.  The\r\n  `MultiTaskModel` first runs its inputs through the `Backbone`, then passes the result (and\r\n  whatever other relevant inputs it got) to each `Head` that's in use.\r\n- A `MultiTaskDataLoader`, with a corresponding `MultiTaskDatasetReader`, and a couple of new\r\n  configuration objects: `MultiTaskEpochSampler` (for deciding what proportion to sample from each\r\n  dataset at every epoch) and a `MultiTaskScheduler` (for ordering the instances within an epoch).\r\n- Transformer toolkit to plug and play with modular components of transformer architectures.\r\n- Added a command to count the number of instances we're going to be training with\r\n- Added a `FileLock` class to `common.file_utils`. This is just like the `FileLock` from the `filelock` library, except that\r\n  it adds an optional flag `read_only_ok: bool`, which when set to `True` changes the behavior so that a warning will be emitted\r\n  instead of an exception when lacking write permissions on an existing file lock.\r\n  This makes it possible to use the `FileLock` class on a read-only file system.\r\n- Added a new learning rate scheduler: `CombinedLearningRateScheduler`. This can be used to combine different LR schedulers, using one after the other.\r\n- Added an official CUDA 10.1 Docker image.\r\n- Moving `ModelCard` and `TaskCard` abstractions into the main repository.\r\n- Added a util function `allennlp.nn.util.dist_reduce(...)` for handling distributed reductions.\r\n  This is especially useful when implementing a distributed `Metric`.\r\n\r\n### Changed \u26a0\ufe0f\r\n\r\n- `DatasetReader`s are now always lazy. This means there is no `lazy` parameter in the base\r\n  class, and the `_read()` method should always be a generator.\r\n- The `DataLoader` now decides whether to load instances lazily or not.\r\n  With the `PyTorchDataLoader` this is controlled with the `lazy` parameter, but with\r\n  the `MultiProcessDataLoading` this is controlled by the `max_instances_in_memory` setting.\r\n- `ArrayField` is now called `TensorField`, and implemented in terms of torch tensors, not numpy.\r\n- Improved `nn.util.move_to_device` function by avoiding an unnecessary recursive check for tensors and\r\n  adding a `non_blocking` optional argument, which is the same argument as in `torch.Tensor.to()`.\r\n- If you are trying to create a heterogeneous batch, you now get a better error message.\r\n- Readers using the new vision features now explicitly log how they are featurizing images.\r\n- `master_addr` and `master_port` renamed to `primary_addr` and `primary_port`, respectively.\r\n- `is_master` parameter for training callbacks renamed to `is_primary`.\r\n- `master` branch renamed to `main`\r\n- Torch version bumped to 1.7.1 in Docker images.\r\n\r\n### Removed \ud83d\udc4b\r\n\r\n- Removed `nn.util.has_tensor`.\r\n\r\n### Fixed \u2705\r\n\r\n- The `build-vocab` command no longer crashes when the resulting vocab file is\r\n  in the current working directory.\r\n- Fixed typo with `LabelField` string representation: removed trailing apostrophe.\r\n- `Vocabulary.from_files` and `cached_path` will issue a warning, instead of failing, when a lock on an existing resource\r\n  can't be acquired because the file system is read-only.\r\n- `TrackEpochCallback` is now a `EpochCallback`.\r\n\r\n## Commits\r\n\r\n9a4a424d Moves vision models to allennlp-models (#4918)\r\n412896bc fix merge conflicts\r\ned322eba A helper for distributed reductions (#4920)\r\n9ab2bf03 add CUDA 10.1 Docker image (#4921)\r\nd82287e5 Update transformers requirement from <4.1,>=4.0 to >=4.0,<4.2 (#4872)\r\n54973947 Multitask example (#4898)\r\n0f00d4d4 resolve \\_read type (#4916)\r\n5229da83 Toolkit decoder (#4914)\r\n4183a49c Update mkdocs-material requirement from <6.2.0,>=5.5.0 to >=5.5.0,<6.3.0 (#4880)\r\nd7c9eab3 improve worker error handling in MultiProcessDataLoader (#4912)\r\n94dd9cc7 rename 'master' -> 'primary' for distributed training (#4910)\r\nc9585afd fix imports in file_utils\r\n03c7ffb5 Merge branch 'main' into vision\r\neffcc4e5 improve data loading docs (#4909)\r\n2f545701 remove PyTorchDataLoader, add SimpleDataLoader for testing (#4907)\r\n31ec6a59 MultiProcessDataLoader takes PathLike data_path (#4908)\r\n5e3757b4 rename 'multi_process_*' -> 'multiprocess' for consistency (#4906)\r\ndf36636e Data loading cuda device (#4879)\r\naedd3be1 Toolkit: Cleaning up TransformerEmbeddings (#4900)\r\n54e85eee disable codecov annotations (#4902)\r\n2623c4bf Making TrackEpochCallback an EpochCallback (#4893)\r\n1d21c759 issue warning instead of failing when lock can't be acquired on a resource that exists in a read-only file system (#4867)\r\nec197c3b Create pull_request_template.md (#4891)\r\n15d32da1 Make GQA work (#4884)\r\nfbab0bd9 import MultiTaskDataLoader to data_loaders/__init__.py (#4885)\r\nd1cc1469 Merge branch 'main' into vision\r\nabacc01b Adding f1 score (#4890)\r\n9cf41b2f fix navbar link\r\n9635af82 rename 'master' -> 'main' (#4887)\r\nd0a07fb3 docs: fix simple typo, multplication -> multiplication (#4883)\r\nd1f032d8 Moving modelcard and taskcard abstractions to main repo (#4881)\r\nf62b819f Make images easier to find for Visual Entailment (#4878)\r\n1fff7cae Update docker torch version (#4873)\r\n7a7c7ea8 Only cache, no featurizing (#4870)\r\nd2aea979 Fix typo in __str__ (#4874)\r\n1c72a302 Merge branch 'master' into vision\r\n6a8d425f add CombinedLearningRateScheduler (#4871)\r\n85d38ff6 doc fixes\r\nc4e3f77f Switch to torchvision for vision components \ud83d\udc40, simplify and improve MultiProcessDataLoader (#4821)\r\n3da8e622 Merge branch 'master' into vision\r\na3732d00 Fix cache volume (#4869)\r\n832901e8 Turn superfluous warning to info when extending the vocab in the embedding matrix (#4854)\r\n147fefe6 Merge branch 'master' into vision\r\n87e35360 Make tests work again (#4865)\r\nd16a5c78 Merge remote-tracking branch 'origin/master' into vision\r\n457e56ef Merge branch 'master' into vision\r\nc8521d80 Toolkit: Adding documentation and small changes for `BiModalAttention` (#4859)\r\nddbc7404 gqa reader fixes during vilbert training (#4851)\r\n50e50df6 Generalizing transformer layers (#4776)\r\n52fdd755 adding multilabel option (#4843)\r\n78871195 Other VQA datasets (#4834)\r\ne729e9a4 Added GQA reader (#4832)\r\n52e9dd92 Visual entailment model code (#4822)\r\n01f3a2db Merge remote-tracking branch 'origin/master' into vision\r\n3be6c975 SNLI_VE dataset reader (#4799)\r\nb659e665 VQAv2 (#4639)\r\nc787230c Merge remote-tracking branch 'origin/master' into vision\r\ndb2d1d38 Merge branch 'master' into vision\r\n6bf19246 Merge branch 'master' into vision\r\n167bcaae remove vision push trigger\r\n75914650 Merge remote-tracking branch 'origin/master' into vision\r\n22d4633c improve independence of vision components (#4793)\r\n98018cca fix merge conflicts\r\nc7803150 fix merge conflicts\r\n5d22ce69 Merge remote-tracking branch 'origin/master' into vision\r\n602399c0 update with master\r\nffafaf64 Multitask data loading and scheduling (#4625)\r\n7c47c3a5 Merge branch 'master' into vision\r\n12c8d1bf Generalizing self attention (#4756)\r\n63f61f0c Merge remote-tracking branch 'origin/master' into vision\r\nb48347be Merge remote-tracking branch 'origin/master' into vision\r\n81892db4 fix failing tests\r\n98edd253 update torch requirement\r\n8da35081 update with master\r\ncc53afec separating TransformerPooler as a new module (#4730)\r\n4ccfa885 Transformer toolkit: BiModalEncoder now has separate `num_attention_heads` for both modalities (#4728)\r\n91631ef9 Transformer toolkit (#4577)\r\n677a9cec Merge remote-tracking branch 'origin/master' into vision\r\n2985236f This should have been part of the previously merged PR\r\nc5d264ae Detectron NLVR2 (#4481)\r\ne39a5f62 Merge remote-tracking branch 'origin/master' into vision\r\nf1e46fdc Add MultiTaskModel (#4601)\r\nfa22f731 Merge remote-tracking branch 'origin/master' into vision\r\n41872ae4 Merge remote-tracking branch 'origin/master' into vision\r\nf886fd06 Merge remote-tracking branch 'origin/master' into vision\r\n191b641e make existing readers work with multi-process loading (#4597)\r\nd7124d4b fix len calculation for new data loader (#4618)\r\n87463612 Merge branch 'master' into vision\r\n319794a1 remove duplicate padding calculations in collate fn (#4617)\r\nde9165e1 rename 'node_rank' to 'global_rank' in dataset reader 'DistributedInfo' (#4608)\r\n3d114197 Formatting updates for new version of black (#4607)\r\ncde06e62 Changelog\r\n1b08fd62 ensure models check runs on right branch\r\n44c8791c ensure vision CI runs on each commit (#4582)\r\n95e82532 Merge branch 'master' into vision\r\ne74a7365 new data loading (#4497)\r\n6f820050 Merge remote-tracking branch 'origin/master' into vision\r\na7d45de1 Initializing a VilBERT model from a pre-trained transformer (#4495)\r\n3833f7a5 Merge branch 'master' into vision\r\n71d7cb4e Merge branch 'master' into vision\r\n31379611 Merge remote-tracking branch 'origin/master' into vision\r\n6cc508d7 Merge branch 'master' into vision\r\nf87df839 Merge remote-tracking branch 'origin/master' into vision\r\n0bbe84b4 An initial VilBERT model for NLVR2 (#4423)\r\n"}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/35318248", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/35318248/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/35318248/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v1.3.0", "id": 35318248, "author": {"login": "AkshitaB", "id": 6500683, "node_id": "MDQ6VXNlcjY1MDA2ODM=", "avatar_url": "https://avatars.githubusercontent.com/u/6500683?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AkshitaB", "html_url": "https://github.com/AkshitaB", "followers_url": "https://api.github.com/users/AkshitaB/followers", "following_url": "https://api.github.com/users/AkshitaB/following{/other_user}", "gists_url": "https://api.github.com/users/AkshitaB/gists{/gist_id}", "starred_url": "https://api.github.com/users/AkshitaB/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AkshitaB/subscriptions", "organizations_url": "https://api.github.com/users/AkshitaB/orgs", "repos_url": "https://api.github.com/users/AkshitaB/repos", "events_url": "https://api.github.com/users/AkshitaB/events{/privacy}", "received_events_url": "https://api.github.com/users/AkshitaB/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTM1MzE4MjQ4", "tag_name": "v1.3.0", "target_commitish": "master", "name": "v1.3.0", "draft": false, "prerelease": false, "created_at": "2020-12-15T22:05:13Z", "published_at": "2020-12-15T22:10:16Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v1.3.0", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v1.3.0", "body": "## What's new\r\n\r\n### Added \ud83c\udf89\r\n\r\n- Added links to source code in docs.\r\n- Added `get_embedding_layer` and `get_text_field_embedder` to the `Predictor` class; to specify embedding layers for non-AllenNLP models.\r\n- Added [Gaussian Error Linear Unit (GELU)](https://pytorch.org/docs/stable/generated/torch.nn.GELU.html) as an Activation.\r\n\r\n### Changed \u26a0\ufe0f\r\n\r\n- Renamed module `allennlp.data.tokenizers.token` to `allennlp.data.tokenizers.token_class` to avoid\r\n  [this bug](https://github.com/allenai/allennlp/issues/4819).\r\n- `transformers` dependency updated to version 4.0.1.\r\n\r\n### Fixed \u2705\r\n\r\n- Fixed a lot of instances where tensors were first created and then sent to a device\r\n  with `.to(device)`. Instead, these tensors are now created directly on the target device.\r\n- Fixed issue with `GradientDescentTrainer` when constructed with `validation_data_loader=None` and `learning_rate_scheduler!=None`.\r\n- Fixed a bug when removing all handlers in root logger.\r\n- `ShardedDatasetReader` now inherits parameters from `base_reader` when required.\r\n- Fixed an issue in `FromParams` where parameters in the `params` object used to a construct a class\r\n  were not passed to the constructor if the value of the parameter was equal to the default value.\r\n  This caused bugs in some edge cases where a subclass that takes `**kwargs` needs to inspect\r\n  `kwargs` before passing them to its superclass.\r\n- Improved the band-aid solution for segmentation faults and the \"ImportError: dlopen: cannot load any more object with static TLS\" \r\n  by adding a `transformers` import.\r\n- Added safety checks for extracting tar files\r\n\r\n## Commits\r\n\r\nd408f416 log import errors for default plugins (#4866)\r\nf2a53310 Adds a safety check for tar files (#4858)\r\n84a36a06 Update transformers requirement from <3.6,>=3.4 to >=4.0,<4.1 (#4831)\r\nfdad31aa Add ability to specify the embedding layer if the model does not use `TextFieldEmbedder` (#4836)\r\n41c52245 Improve the band-aid solution for seg faults and the static TLS error (#4846)\r\n63b6d163 fix FromParams bug (#4841)\r\n6c3238ec rename token.py -> token_class.py (#4842)\r\ncec92098 Several micro optimizations (#4833)\r\n48a48652 Add GELU activation (#4828)\r\n3e623658 Bugfix for attribute inheritance in ShardedDatasetReader (#4830)\r\n458c4c2b fix the way handlers are removed from the root logger (#4829)\r\n5b306585 Fix bug in GradientDescentTrainer when validation data is absent (#4811)\r\nf353c6ce add link to source code in docs (#4807)\r\n0a832713 No Docker auth on PRs (#4802)\r\nad8e8a09 no ssh setup on PRs (#4801)\r\n\r\n"}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/34073790", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/34073790/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/34073790/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v1.2.2", "id": 34073790, "author": {"login": "epwalsh", "id": 8812459, "node_id": "MDQ6VXNlcjg4MTI0NTk=", "avatar_url": "https://avatars.githubusercontent.com/u/8812459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/epwalsh", "html_url": "https://github.com/epwalsh", "followers_url": "https://api.github.com/users/epwalsh/followers", "following_url": "https://api.github.com/users/epwalsh/following{/other_user}", "gists_url": "https://api.github.com/users/epwalsh/gists{/gist_id}", "starred_url": "https://api.github.com/users/epwalsh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/epwalsh/subscriptions", "organizations_url": "https://api.github.com/users/epwalsh/orgs", "repos_url": "https://api.github.com/users/epwalsh/repos", "events_url": "https://api.github.com/users/epwalsh/events{/privacy}", "received_events_url": "https://api.github.com/users/epwalsh/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTM0MDczNzkw", "tag_name": "v1.2.2", "target_commitish": "master", "name": "v1.2.2", "draft": false, "prerelease": false, "created_at": "2020-11-17T18:24:41Z", "published_at": "2020-11-17T18:26:44Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v1.2.2", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v1.2.2", "body": "## What's new\r\n\r\n### Added \ud83c\udf89\r\n\r\n- Added Docker builds for other torch-supported versions of CUDA.\r\n- Adds [`allennlp-semparse`](https://github.com/allenai/allennlp-semparse) as an official, default plugin.\r\n\r\n### Fixed \u2705\r\n\r\n- `GumbelSampler` now sorts the beams by their true log prob.\r\n\r\n## Commits\r\n\r\n023d9bcc Prepare for release v1.2.2\r\n7b0826c1 push commit images for both CUDA versions\r\n3cad5b41 fix AUC test (#4795)\r\nefde092d upgrade ssh-agent action (#4797)\r\nec37dd46 Docker builds for other CUDA versions, improve CI (#4796)\r\n0d8873cf doc link quickfix\r\ne4cc95ce improve plugin section in README (#4789)\r\nd99f7f8a ensure Gumbel sorts beams by true log prob (#4786)\r\n9fe8d900 Makes the transformer cache work with custom kwargs (#4781)\r\n1e7492d7 Update transformers requirement from <3.5,>=3.4 to >=3.4,<3.6 (#4784)\r\nf27ef38b Fixes pretrained embeddings for transformers that don't have end tokens (#4732)"}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/33761246", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/33761246/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/33761246/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v1.2.1", "id": 33761246, "author": {"login": "epwalsh", "id": 8812459, "node_id": "MDQ6VXNlcjg4MTI0NTk=", "avatar_url": "https://avatars.githubusercontent.com/u/8812459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/epwalsh", "html_url": "https://github.com/epwalsh", "followers_url": "https://api.github.com/users/epwalsh/followers", "following_url": "https://api.github.com/users/epwalsh/following{/other_user}", "gists_url": "https://api.github.com/users/epwalsh/gists{/gist_id}", "starred_url": "https://api.github.com/users/epwalsh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/epwalsh/subscriptions", "organizations_url": "https://api.github.com/users/epwalsh/orgs", "repos_url": "https://api.github.com/users/epwalsh/repos", "events_url": "https://api.github.com/users/epwalsh/events{/privacy}", "received_events_url": "https://api.github.com/users/epwalsh/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTMzNzYxMjQ2", "tag_name": "v1.2.1", "target_commitish": "master", "name": "v1.2.1", "draft": false, "prerelease": false, "created_at": "2020-11-11T00:21:31Z", "published_at": "2020-11-11T00:22:48Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v1.2.1", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v1.2.1", "body": "## What's new\r\n\r\n### Added \ud83c\udf89\r\n\r\n- Added an optional `seed` parameter to `ModelTestCase.set_up_model` which sets the random\r\n  seed for `random`, `numpy`, and `torch`.\r\n- Added support for a global plugins file at `~/.allennlp/plugins`.\r\n- Added more documentation about plugins.\r\n- Added sampler class and parameter in beam search for non-deterministic search, with several\r\n  implementations, including `MultinomialSampler`, `TopKSampler`, `TopPSampler`, and\r\n  `GumbelMaxSampler`. Utilizing `GumbelMaxSampler` will give [Stochastic Beam Search](https://api.semanticscholar.org/CorpusID:76662039).\r\n\r\n### Changed \u26a0\ufe0f\r\n\r\n- Pass batch metrics to `BatchCallback`.\r\n\r\n### Fixed \u2705\r\n\r\n- Fixed a bug where forward hooks were not cleaned up with saliency interpreters if there\r\n  was an exception.\r\n- Fixed the computation of saliency maps in the Interpret code when using mismatched indexing.\r\n  Previously, we would compute gradients from the top of the transformer, after aggregation from\r\n  wordpieces to tokens, which gives results that are not very informative.  Now, we compute gradients\r\n  with respect to the embedding layer, and aggregate wordpieces to tokens separately.\r\n- Fixed the heuristics for finding embedding layers in the case of RoBERTa. An update in the\r\n  `transformers` library broke our old heuristic.\r\n- Fixed typo with registered name of ROUGE metric. Previously was `rogue`, fixed to `rouge`.\r\n- Fixed default masks that were erroneously created on the CPU even when a GPU is available.\r\n\r\n## Commits\r\n\r\n04247faa support global plugins file, improve plugins docs (#4779)\r\n9f7cc248 Add sampling strategies to beam search (#4768)\r\nf6fe8c6d pin urllib3 in dev reqs for responses (#4780)\r\n764bbe2e Pass batch metrics to `BatchCallback` (#4764)\r\ndc3a4f67 clean up forward hooks on exception (#4778)\r\nfcc3a70b Fix: typo in metric, rogue -> rouge (#4777)\r\nb89320cd Set the device for an auto-created mask (#4774)\r\n92a844a7 RoBERTa embeddings are no longer a type of BERT embeddings (#4771)\r\n23f0a8a6 Ensure cnn_encoder respects masking (#4746)\r\nb4f1a7ab add seed option to ModelTestCase.set_up_model (#4769)\r\nb7cec515 Made Interpret code handle mismatched cases better (#4733)\r\n9759b15f allow TextFieldEmbedder to have EmptyEmbedder that may not be in input (#4761)"}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/33243179", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/33243179/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/33243179/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v1.2.0", "id": 33243179, "author": {"login": "AkshitaB", "id": 6500683, "node_id": "MDQ6VXNlcjY1MDA2ODM=", "avatar_url": "https://avatars.githubusercontent.com/u/6500683?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AkshitaB", "html_url": "https://github.com/AkshitaB", "followers_url": "https://api.github.com/users/AkshitaB/followers", "following_url": "https://api.github.com/users/AkshitaB/following{/other_user}", "gists_url": "https://api.github.com/users/AkshitaB/gists{/gist_id}", "starred_url": "https://api.github.com/users/AkshitaB/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AkshitaB/subscriptions", "organizations_url": "https://api.github.com/users/AkshitaB/orgs", "repos_url": "https://api.github.com/users/AkshitaB/repos", "events_url": "https://api.github.com/users/AkshitaB/events{/privacy}", "received_events_url": "https://api.github.com/users/AkshitaB/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTMzMjQzMTc5", "tag_name": "v1.2.0", "target_commitish": "master", "name": "v1.2.0", "draft": false, "prerelease": false, "created_at": "2020-10-29T21:32:00Z", "published_at": "2020-10-29T21:37:50Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v1.2.0", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v1.2.0", "body": "## What's new\r\n\r\n### Changed \u26a0\ufe0f\r\n\r\n- Enforced stricter typing requirements around the use of `Optional[T]` types.\r\n- Changed the behavior of `Lazy` types in `from_params` methods. Previously, if you defined a `Lazy` parameter like\r\n  `foo: Lazy[Foo] = None` in a custom `from_params` classmethod, then `foo` would actually never be `None`.\r\n  This behavior is now different. If no params were given for `foo`, it will be `None`.\r\n  You can also now set default values for foo like `foo: Lazy[Foo] = Lazy(Foo)`.\r\n  Or, if you want you want a default value but also want to allow for `None` values, you can\r\n  write it like this: `foo: Optional[Lazy[Foo]] = Lazy(Foo)`.\r\n- Added support for PyTorch version 1.7.\r\n\r\n### Fixed \u2705\r\n\r\n- Made it possible to instantiate `TrainerCallback` from config files.\r\n- Fixed the remaining broken internal links in the API docs.\r\n- Fixed a bug where Hotflip would crash with a model that had multiple TokenIndexers and the input\r\n  used rare vocabulary items.\r\n- Fixed a bug where `BeamSearch` would fail if `max_steps` was equal to 1.\r\n\r\n## Commits\r\n\r\n7f85c74e fix docker build (#4762)\r\ncc9ac0f2 ensure dataclasses not installed in CI (#4754)\r\n812ac570 Fix hotflip bug where vocab items were not re-encoded correctly (#4759)\r\naeb6d362 revert samplers and fix bug when max_steps=1 (#4760)\r\nbaca7545 Make returning token type id default in transformers intra word tokenization. (#4758)\r\n5d6670ce Update torch requirement from <1.7.0,>=1.6.0 to >=1.6.0,<1.8.0 (#4753)\r\n0ad228d4 a few small doc fixes (#4752)\r\n71a98c2a stricter typing for Optional[T] types, improve handling of Lazy params (#4743)\r\n27edfbf8 Add end+trainer callbacks to Trainer.from_partial_objects (#4751)\r\nb792c834 Fix device mismatch bug for categorical accuracy metric in distributed training (#4744)\r\n\r\n"}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/32945236", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/32945236/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/32945236/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v1.2.0rc1", "id": 32945236, "author": {"login": "AkshitaB", "id": 6500683, "node_id": "MDQ6VXNlcjY1MDA2ODM=", "avatar_url": "https://avatars.githubusercontent.com/u/6500683?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AkshitaB", "html_url": "https://github.com/AkshitaB", "followers_url": "https://api.github.com/users/AkshitaB/followers", "following_url": "https://api.github.com/users/AkshitaB/following{/other_user}", "gists_url": "https://api.github.com/users/AkshitaB/gists{/gist_id}", "starred_url": "https://api.github.com/users/AkshitaB/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AkshitaB/subscriptions", "organizations_url": "https://api.github.com/users/AkshitaB/orgs", "repos_url": "https://api.github.com/users/AkshitaB/repos", "events_url": "https://api.github.com/users/AkshitaB/events{/privacy}", "received_events_url": "https://api.github.com/users/AkshitaB/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTMyOTQ1MjM2", "tag_name": "v1.2.0rc1", "target_commitish": "master", "name": "v1.2.0rc1", "draft": false, "prerelease": true, "created_at": "2020-10-22T21:43:53Z", "published_at": "2020-10-22T21:45:38Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v1.2.0rc1", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v1.2.0rc1", "body": "## What's new\r\n\r\n### Added \ud83c\udf89\r\n\r\n- Added a warning when `batches_per_epoch` for the validation data loader is inherited from\r\n  the train data loader.\r\n- Added a `build-vocab` subcommand that can be used to build a vocabulary from a training config file.\r\n- Added `tokenizer_kwargs` argument to `PretrainedTransformerMismatchedIndexer`.\r\n- Added `tokenizer_kwargs` and `transformer_kwargs` arguments to `PretrainedTransformerMismatchedEmbedder`.\r\n- Added official support for Python 3.8.\r\n- Added a script: `scripts/release_notes.py`, which automatically prepares markdown release notes from the\r\n  CHANGELOG and commit history.\r\n- Added a flag `--predictions-output-file` to the `evaluate` command, which tells AllenNLP to write the\r\n  predictions from the given dataset to the file as JSON lines.\r\n- Added the ability to ignore certain missing keys when loading a model from an archive. This is done\r\n  by adding a class-level variable called `authorized_missing_keys` to any PyTorch module that a `Model` uses.\r\n  If defined, `authorized_missing_keys` should be a list of regex string patterns.\r\n- Added `FBetaMultiLabelMeasure`, a multi-label Fbeta metric. This is a subclass of the existing `FBetaMeasure`.\r\n- Added ability to pass additional key word arguments to `cached_transformers.get()`, which will be passed on to `AutoModel.from_pretrained()`.\r\n- Added an `overrides` argument to `Predictor.from_path()`.\r\n- Added a `cached-path` command.\r\n- Added a function `inspect_cache` to `common.file_utils` that prints useful information about the cache. This can also \r\n  be used from the `cached-path` command with `allennlp cached-path --inspect`.\r\n- Added a function `remove_cache_entries` to `common.file_utils` that removes any cache entries matching the given\r\n  glob patterns. This can used from the `cached-path` command with `allennlp cached-path --remove some-files-*`.\r\n- Added logging for the main process when running in distributed mode.\r\n- Added a `TrainerCallback` object to support state sharing between batch and epoch-level training callbacks.\r\n- Added support for .tar.gz in PretrainedModelInitializer.\r\n- Added classes: `nn/samplers/samplers.py` with `MultinomialSampler`, `TopKSampler`, and `TopPSampler` for \r\n  sampling indices from log probabilities\r\n- Made `BeamSearch` registrable.\r\n- Added `top_k_sampling` and `type_p_sampling` `BeamSearch` implementations.\r\n- Pass `serialization_dir` to `Model` and `DatasetReader`.\r\n- Added an optional `include_in_archive` parameter to the top-level of configuration files. When specified, `include_in_archive` should be a list of paths relative to the serialization directory which will be bundled up with the final archived model from a training run.\r\n\r\n### Changed \u26a0\ufe0f\r\n\r\n- Subcommands that don't require plugins will no longer cause plugins to be loaded or have an `--include-package` flag.\r\n- Allow overrides to be JSON string or `dict`.\r\n- `transformers` dependency updated to version 3.1.0.\r\n- When `cached_path` is called on a local archive with `extract_archive=True`, the archive is now extracted into a unique subdirectory of the cache root instead of a subdirectory of the archive's directory. The extraction directory is also unique to the modification time of the archive, so if the file changes, subsequent calls to `cached_path` will know to re-extract the archive.\r\n- Removed the `truncation_strategy` parameter to `PretrainedTransformerTokenizer`. The way we're calling the tokenizer, the truncation strategy takes no effect anyways.\r\n- Don't use initializers when loading a model, as it is not needed.\r\n- Distributed training will now automatically search for a local open port if the `master_port` parameter is not provided.\r\n- In training, save model weights before evaluation.\r\n- `allennlp.common.util.peak_memory_mb` renamed to `peak_cpu_memory`, and `allennlp.common.util.gpu_memory_mb` renamed to `peak_gpu_memory`,\r\n  and they both now return the results in bytes as integers. Also, the `peak_gpu_memory` function now utilizes PyTorch functions to find the memory\r\n  usage instead of shelling out to the `nvidia-smi` command. This is more efficient and also more accurate because it only takes\r\n  into account the tensor allocations of the current PyTorch process.\r\n- Make sure weights are first loaded to the cpu when using PretrainedModelInitializer, preventing wasted GPU memory.\r\n- Load dataset readers in `load_archive`.\r\n- Updated `AllenNlpTestCase` docstring to remove reference to `unittest.TestCase`\r\n\r\n### Removed \ud83d\udc4b\r\n\r\n- Removed `common.util.is_master` function.\r\n\r\n### Fixed \u2705\r\n\r\n- Fixed a bug where the reported `batch_loss` metric was incorrect when training with gradient accumulation.\r\n- Class decorators now displayed in API docs.\r\n- Fixed up the documentation for the `allennlp.nn.beam_search` module.\r\n- Ignore `*args` when constructing classes with `FromParams`.\r\n- Ensured some consistency in the types of the values that metrics return.\r\n- Fix a PyTorch warning by explicitly providing the `as_tuple` argument (leaving\r\n  it as its default value of `False`) to `Tensor.nonzero()`.\r\n- Remove temporary directory when extracting model archive in `load_archive`\r\n  at end of function rather than via `atexit`.\r\n- Fixed a bug where using `cached_path()` offline could return a cached resource's lock file instead\r\n  of the cache file.\r\n- Fixed a bug where `cached_path()` would fail if passed a `cache_dir` with the user home shortcut `~/`.\r\n- Fixed a bug in our doc building script where markdown links did not render properly\r\n  if the \"href\" part of the link (the part inside the `()`) was on a new line.\r\n- Changed how gradients are zeroed out with an optimization. See [this video from NVIDIA](https://www.youtube.com/watch?v=9mS1fIYj1So)\r\n  at around the 9 minute mark.\r\n- Fixed a bug where parameters to a `FromParams` class that are dictionaries wouldn't get logged\r\n  when an instance is instantiated `from_params`.\r\n- Fixed a bug in distributed training where the vocab would be saved from every worker, when it should have been saved by only the local master process.\r\n- Fixed a bug in the calculation of rouge metrics during distributed training where the total sequence count was not being aggregated across GPUs.\r\n- Fixed `allennlp.nn.util.add_sentence_boundary_token_ids()` to use `device` parameter of input tensor.\r\n- Be sure to close the TensorBoard writer even when training doesn't finish.\r\n- Fixed the docstring for `PyTorchSeq2VecWrapper`.\r\n\r\n## Commits\r\n\r\n01644caf Pass serialization_dir to Model, DatasetReader, and support `include_in_archive` (#4713)\r\n1f29f352 Update transformers requirement from <3.4,>=3.1 to >=3.1,<3.5 (#4741)\r\n6bb9ce9a warn about batches_per_epoch with validation loader (#4735)\r\n00bb6c59 Be sure to close the TensorBoard writer (#4731)\r\n3f23938b Update mkdocs-material requirement from <6.1.0,>=5.5.0 to >=5.5.0,<6.2.0 (#4738)\r\n10c11cea Fix typo in PretrainedTransformerMismatchedEmbedder docstring (#4737)\r\n0e64b4d3 fix docstring for PyTorchSeq2VecWrapper (#4734)\r\n006bab48 Don't use PretrainedModelInitializer when loading a model (#4711)\r\nce14bdc0 Allow usage of .tar.gz with PretrainedModelInitializer (#4709)\r\nc14a056d avoid defaulting to CPU device in add_sentence_boundary_token_ids() (#4727)\r\n24519fd9 fix typehint on checkpointer method (#4726)\r\nd3c69f75 Bump mypy from 0.782 to 0.790 (#4723)\r\ncccad29a Updated AllenNlpTestCase docstring (#4722)\r\n3a85e359 add reasonable timeout to gpu checks job (#4719)\r\n1ff0658c Added logging for the main process when running in distributed mode (#4710)\r\nb099b69c Add top_k and top_p sampling to BeamSearch (#4695)\r\nbc6f15ac Fixes rouge metric calculation corrected for distributed training (#4717)\r\nae7cf85b automatically find local open port in distributed training (#4696)\r\n321d4f48 TrainerCallback with batch/epoch/end hooks (#4708)\r\n001e1f76 new way of setting env variables in GH Actions (#4700)\r\nc14ea40e Save checkpoint before running evaluation (#4704)\r\n40bb47ad Load weights to cpu with PretrainedModelInitializer (#4712)\r\n327188b8 improve memory helper functions (#4699)\r\n90f00379 fix reported batch_loss (#4706)\r\n39ddb523 CLI improvements (#4692)\r\nedcb6d34 Fix a bug in saving vocab during distributed training (#4705)\r\n3506e3fd ensure parameters that are actual dictionaries get logged (#4697)\r\neb7f2568 Add StackOverflow link to README (#4694)\r\n17c3b84b Fix small typo (#4686)\r\ne0b2e265 display class decorators in API docs (#4685)\r\nb9a92842 Update transformers requirement from <3.3,>=3.1 to >=3.1,<3.4 (#4684)\r\nd9bdaa95 add build-vocab command (#4655)\r\nce604f1f Update mkdocs-material requirement from <5.6.0,>=5.5.0 to >=5.5.0,<6.1.0 (#4679)\r\nc3b5ed74 zero grad optimization (#4673)\r\n9dabf3fa Add missing tokenizer/transformer kwargs (#4682)\r\n9ac6c76c Allow overrides to be JSON string or dict (#4680)\r\n55cfb47b The truncation setting doesn't do anything anymore (#4672)\r\n990c9c17 clarify conda Python version in README.md\r\n97db5387 official support for Python 3.8 \ud83d\udc0d (#4671)\r\n1e381bb0 Clean up the documentation for beam search (#4664)\r\n11def8ea Update bug_report.md\r\n97fe88d2 Cached path command (#4652)\r\nc9f376bf Update transformers requirement from <3.2,>=3.1 to >=3.1,<3.3 (#4663)\r\ne5e3d020 tick version for nightly releases\r\nb833f905 fix multi-line links in docs (#4660)\r\nd7c06fe7 Expose from_pretrained keyword arguments (#4651)\r\n175c76be fix confusing distributed logging info (#4654)\r\nfbd2ccca fix numbering in RELEASE_GUIDE\r\n2d5f24bd improve how cached_path extracts archives (#4645)\r\n824f97d4 smooth out release process (#4648)\r\nc7b7c008 Feature/prevent temp directory retention (#4643)\r\nde5d68bc Fix tensor.nonzero() function overload warning (#4644)\r\ne8e89d5a add flag for saving predictions to 'evaluate' command (#4637)\r\ne4fd5a0c Multi-label F-beta metric (#4562)\r\nf0e7a78c Create Dependabot config file (#4635)\r\n0e33b0ba Return consistent types from metrics (#4632)\r\n2df364ff Update transformers requirement from <3.1,>=3.0 to >=3.0,<3.2 (#4621)\r\n6d480aae Improve handling of **kwargs in FromParams (#4629)\r\nbf3206a2 Workaround for Python not finding imports in spawned processes (#4630)"}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/30926178", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/30926178/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/30926178/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v1.1.0", "id": 30926178, "author": {"login": "epwalsh", "id": 8812459, "node_id": "MDQ6VXNlcjg4MTI0NTk=", "avatar_url": "https://avatars.githubusercontent.com/u/8812459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/epwalsh", "html_url": "https://github.com/epwalsh", "followers_url": "https://api.github.com/users/epwalsh/followers", "following_url": "https://api.github.com/users/epwalsh/following{/other_user}", "gists_url": "https://api.github.com/users/epwalsh/gists{/gist_id}", "starred_url": "https://api.github.com/users/epwalsh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/epwalsh/subscriptions", "organizations_url": "https://api.github.com/users/epwalsh/orgs", "repos_url": "https://api.github.com/users/epwalsh/repos", "events_url": "https://api.github.com/users/epwalsh/events{/privacy}", "received_events_url": "https://api.github.com/users/epwalsh/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTMwOTI2MTc4", "tag_name": "v1.1.0", "target_commitish": "master", "name": "v1.1.0", "draft": false, "prerelease": false, "created_at": "2020-09-08T17:51:19Z", "published_at": "2020-09-08T20:32:48Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v1.1.0", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v1.1.0", "body": "## Highlights\r\n\r\nVersion 1.1 was mainly focused on bug fixes, but there are a few important new features such as gradient checkpointing with pretrained transformer embedders and official support for automatic mixed precision (AMP) training through the new `torch.amp` module.\r\n\r\n## Details\r\n\r\n### Added\r\n\r\n- `Predictor.capture_model_internals()` now accepts a regex specifying which modules to capture.\r\n- Added the option to specify `requires_grad: false` within an optimizer's parameter groups.\r\n- Added the `file-friendly-logging` flag back to the `train` command. Also added this flag to the `predict`, `evaluate`, and `find-learning-rate` commands.\r\n- Added an `EpochCallback` to track current epoch as a model class member.\r\n- Added the option to enable or disable gradient checkpointing for transformer token embedders via boolean parameter `gradient_checkpointing`.\r\n- Added a method to `ModelTestCase` for running basic model tests when you aren't using config files.\r\n- Added some convenience methods for reading files.\r\n- `cached_path()` can now automatically extract and read files inside of archives.\r\n- Added the ability to pass an archive file instead of a local directory to `Vocab.from_files`.\r\n- Added the ability to pass an archive file instead of a glob to `ShardedDatasetReader`.\r\n- Added a new `\"linear_with_warmup\"` learning rate scheduler.\r\n- Added a check in `ShardedDatasetReader` that ensures the base reader doesn't implement manual distributed sharding itself.\r\n- Added an option to `PretrainedTransformerEmbedder` and `PretrainedTransformerMismatchedEmbedder` to use a scalar mix of all hidden layers from the transformer model instead of just the last layer. To utilize this, just set `last_layer_only` to `False`.\r\n- Training metrics now include `batch_loss` and `batch_reg_loss` in addition to aggregate loss across number of batches.\r\n\r\n### Changed\r\n\r\n- Upgraded PyTorch requirement to 1.6.\r\n- Beam search now supports multi-layer decoders.\r\n- Replaced the NVIDIA Apex AMP module with torch's native AMP module. The default trainer (`GradientDescentTrainer`) now takes a `use_amp: bool` parameter instead of the old `opt_level: str` parameter.\r\n- Not specifying a `cuda_device` now automatically determines whether to use a GPU or not.\r\n- Discovered plugins are logged so you can see what was loaded.\r\n- `allennlp.data.DataLoader` is now an abstract registrable class. The default implementation remains the same, but was renamed to `allennlp.data.PyTorchDataLoader`.\r\n- `BertPooler` can now unwrap and re-wrap extra dimensions if necessary.\r\n\r\n### Removed\r\n\r\n- Removed the `opt_level` parameter to `Model.load` and `load_archive`. In order to use AMP with a loaded model now, just run the model's forward pass within torch's [`autocast`](https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.autocast) context.\r\n\r\n### Fixed\r\n\r\n- Fixed handling of some edge cases when constructing classes with `FromParams` where the class\r\n  accepts `**kwargs`.\r\n- Fixed division by zero error when there are zero-length spans in the input to a\r\n  `PretrainedTransformerMismatchedIndexer`.\r\n- Improved robustness of `cached_path` when extracting archives so that the cache won't be corrupted\r\n  if a failure occurs during extraction.\r\n- Fixed a bug with the `average` and `evalb_bracketing_score` metrics in distributed training.\r\n- Fixed a bug in distributed metrics that caused nan values due to repeated addition of an accumulated variable.\r\n- Fixed how truncation was handled with `PretrainedTransformerTokenizer`.\r\n  Previously, if `max_length` was set to `None`, the tokenizer would still do truncation if the\r\n  transformer model had a default max length in its config.\r\n  Also, when `max_length` was set to a non-`None` value, several warnings would appear\r\n  for certain transformer models around the use of the `truncation` parameter.\r\n- Fixed evaluation of all metrics when using distributed training.\r\n- Added a `py.typed` marker. Fixed type annotations in `allennlp.training.util`.\r\n- Fixed problem with automatically detecting whether tokenization is necessary.\r\n  This affected primarily the Roberta SST model.\r\n- Improved help text for using the --overrides command line flag.\r\n- Removed unnecessary warning about deadlocks in `DataLoader`.\r\n- Fixed testing models that only return a loss when they are in training mode.\r\n- Fixed a bug in `FromParams` that caused silent failure in case of the parameter type being `Optional[Union[...]]`.\r\n- Fixed a bug where the program crashes if `evaluation_data_loader` is a `AllennlpLazyDataset`.\r\n- Reduced the amount of log messages produced by `allennlp.common.file_utils`.\r\n- Fixed a bug where `PretrainedTransformerEmbedder` parameters appeared to be trainable\r\n  in the log output even when `train_parameters` was set to `False`.\r\n- Fixed a bug with the sharded dataset reader where it would only read a fraction of the instances\r\n  in distributed training.\r\n- Fixed checking equality of `ArrayField`s.\r\n- Fixed a bug where `NamespaceSwappingField` did not work correctly with `.empty_field()`.\r\n- Put more sensible defaults on the `huggingface_adamw` optimizer.\r\n- Simplified logging so that all logging output always goes to one file.\r\n- Fixed interaction with the python command line debugger.\r\n- Log the grad norm properly even when we're not clipping it.\r\n- Fixed a bug where `PretrainedModelInitializer` fails to initialize a model with a 0-dim tensor\r\n- Fixed a bug with the layer unfreezing schedule of the `SlantedTriangular` learning rate scheduler.\r\n- Fixed a regression with logging in the distributed setting. Only the main worker should write log output to the terminal.\r\n- Pinned the version of boto3 for package managers (e.g. poetry).\r\n- Fixed issue #4330 by updating the `tokenizers` dependency.\r\n- Fixed a bug in `TextClassificationPredictor` so that it passes tokenized inputs to the `DatasetReader`\r\n  in case it does not have a tokenizer.\r\n- `reg_loss` is only now returned for models that have some regularization penalty configured.\r\n- Fixed a bug that prevented `cached_path` from downloading assets from GitHub releases.\r\n- Fixed a bug that erroneously increased last label's false positive count in calculating fbeta metrics.\r\n- `Tqdm` output now looks much better when the output is being piped or redirected.\r\n- Small improvements to how the API documentation is rendered.\r\n- Only show validation progress bar from main process in distributed training.\r\n\r\n### Commits\r\n\r\ndcc9cdc7 Prepare for release v1.1.0\r\naa750bec fix Average metric (#4624)\r\ne1aa57cf improve robustness of cached_path when extracting archives (#4622)\r\n711afaa7 Fix division by zero when there are zero-length spans in MismatchedEmbedder. (#4615)\r\nbe97943a Improve handling of **kwargs in FromParams (#4616)\r\n187b24e5 add more tutorial links to README (#4613)\r\ne840a589 s/logging/logger/ (#4609)\r\ndbc3c3ff Added batched versions of scatter and fill to util.py (#4598)\r\n2c54cf8b reformat for new version of black (#4605)\r\n2dd335e4 batched_span_select now guarantees element order in each span (#4511)\r\n62f554ff specify module names by a regex in predictor.capture_model_internals() (#4585)\r\nf464aa38 Bump markdown-include from 0.5.1 to 0.6.0 (#4586)\r\nd01cdff9 Update RELEASE_PROCESS.md to include allennlp-models (#4587)\r\n3aedac97 Prepare for release v1.1.0rc4\r\n87a61ad9 Bug fix in distributed metrics (#4570)\r\n71a9a90d upgrade actions to cache@v2 (#4573)\r\nbd9ee6a4 Give better usage info for overrides parameter (#4575)\r\n0a456a75 Fix boolean and categorical accuracy for distributed (#4568)\r\n85112746 add actions workflow for closing stale issues (#4561)\r\nde413065 Static type checking fixes (#4545)\r\n5a07009b Fix RoBERTa SST (#4548)\r\n351941f3 Only pin mkdocs-material to minor version, ignore specific patch version (#4556)\r\n0ac13a4f fix CHANGELOG\r\n3b86f588 Prepare for release v1.1.0rc3\r\n44d28476 Metrics in distributed setting (#4525)\r\n1d619659 Bump mkdocs-material from 5.5.3 to 5.5.5 (#4547)\r\n5b977809 tick version for nightly releases\r\nb32608e3 add gradient checkpointing for transformer token embedders (#4544)\r\nf639336a Fix logger being created twice (#4538)\r\n660fdaf2 Fix handling of max length with transformer tokenizers (#4534)\r\n15e288f5 EpochCallBack for tracking epoch (#4540)\r\n9209bc91 Bump mkdocs-material from 5.5.0 to 5.5.3 (#4533)\r\nbfecdc3e Ensure len(self.evaluation_data_loader) is not called (#4531)\r\n5bc3b732 Fix typo in warning in file_utils (#4527)\r\ne80d7687 pin torch >= 1.6\r\n73220d71 Prepare for release v1.1.0rc2\r\n9415350d Update torch requirement from <1.6.0,>=1.5.0 to >=1.5.0,<1.7.0 (#4519)\r\n146bd9ee Remove link to self-attention modules. (#4512)\r\n24012823 add back file-friendly-logging flag (#4509)\r\n54e5c83e closes #4494 (#4508)\r\nfa39d498 ensure __call__ methods are rendered in docs (#4522)\r\ne53d1858 Bug fix for case when param type is Optional[Union...] (#4510)\r\n14f63b77 Make sure we have a bool tensor where we expect one (#4505)\r\n18a4eb34 add a requires_grad option to param groups (#4502)\r\n6c848dfb Bump mkdocs-material from 5.4.0 to 5.5.0 (#4507)\r\nd73f8a91 More BART changes (#4500)\r\n1cab3bfe Update beam_search.py (#4462)\r\n478bf46c remove deadlock warning in DataLoader (#4487)\r\n714334ad Fix reported loss: Bug fix in batch_loss (#4485)\r\ndb20b1fb use longer tqdm intervals when output being redirected (#4488)\r\n53eeec10 tick version for nightly releases\r\nd693cf1c PathLike (#4479)\r\n2f878322 only show validation progress bar from main process (#4476)\r\n9144918d Fix reported loss (#4477)\r\n5c970833 fix release link in CHANGELOG and formatting in README\r\n4eb97953 Prepare for release v1.1.0rc1\r\nf195440b update 'Models' links in README (#4475)\r\n9c801a3c add CHANGELOG to API docs, point to license on GitHub, improve API doc formatting (#4472)\r\n69d2f03d Clean up Tqdm bars when output is being piped or redirected (#4470)\r\n7b188c93 fixed bug that erronously increased last label's false positive count (#4473)\r\n64db027d Skip ETag check if OSError (#4469)\r\nb9d011ef More BART changes (#4468)\r\n7a563a8f add option to use scalar mix of all transformer layers (#4460)\r\nd00ad668 Minor tqdm and logging clean up (#4448)\r\n6acf2058 Fix regloss logging (#4449)\r\n8c32ddfd Fixing bug in TextClassificationPredictor so that it passes tokenized inputs to the DatasetReader (#4456)\r\nb9a91646 Update transformers requirement from <2.12,>=2.10 to >=2.10,<3.1 (#4446)\r\n181ef5d2 pin boto3 to resolve some dependency issues (#4453)\r\nc75a1ebd ensure base reader of ShardedDatasetReader doesn't implement sharding itself (#4454)\r\n8a05ad43 Update CONTRIBUTING.md (#4447)\r\n5b988d63 ensure only rank 0 worker writes to terminal (#4445)\r\n8482f022 fix bug with SlantedTriangular LR scheduler (#4443)\r\ne46a578e Update transformers requirement from <2.11,>=2.10 to >=2.10,<2.12 (#4411)\r\n8229aca3 Fix pretrained model initialization (#4439)\r\n60deece9 Fix type hint in text_field.py (#4434)\r\n23e549e4 More multiple-choice changes (#4415)\r\n6d0a4fd2 generalize DataLoader (#4416)\r\nacd99952 Automatic file-friendly logging (#4383)\r\n637dbb15 fix README, pin mkdocs, update mkdocs-material (#4412)\r\n9c4dfa54 small fix to pretrained transformer tokenizer (#4417)\r\n84988b81 Log plugins discovered and filter out transformers \"PyTorch version ... available\" log message (#4414)\r\n54c41fcc Adds the ability to automatically detect whether we have a GPU (#4400)\r\n96ff5851 Changes from my multiple-choice work (#4368)\r\neee15ca8 Assign an empty mapping array to empty fields of `NamespaceSwappingField` (#4403)\r\naa2943e5 Bump mkdocs-material from 5.3.2 to 5.3.3 (#4398)\r\n7fa7531c fix __eq__ method of ArrayField (#4401)\r\ne104e441 Add test to ensure data loader yields all instances when batches_per_epoch is set (#4394)\r\nb6fd6978 fix sharded dataset reader (#4396)\r\n30e5dbfc Bump mypy from 0.781 to 0.782 (#4395)\r\nb0ba2d4c update version\r\n1d07cc75 Bump mkdocs-material from 5.3.0 to 5.3.2 (#4389)\r\nffc51843 ensure Vocab.from_files and ShardedDatasetReader can handle archives (#4371)\r\n20afe6ce Add Optuna integrated badge to README.md (#4361)\r\nba79f146 Bump mypy from 0.780 to 0.781 (#4390)\r\n85e531c2 Update README.md (#4385)\r\nc2ecb7a2 Add a method to ModelTestCase for use without config files (#4381)\r\n6852deff pin some doc building requirements (#4386)\r\nbf422d56 Add github template for using your own python run script (#4380)\r\nebde6e85 Bump overrides from 3.0.0 to 3.1.0 (#4375)\r\ne52b7518 ensure transformer params are frozen at initialization when train_parameters is false (#4377)\r\n3e8a9ef6 Add link to new template repo for config file development (#4372)\r\n4f70bc93 tick version for nightly releases\r\n63a5e158 Update spacy requirement from <2.3,>=2.1.0 to >=2.1.0,<2.4 (#4370)\r\nef7c75b8 reduce amount of log messages produced by file_utils (#4366)"}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/29944439", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/29944439/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/29944439/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v1.1.0rc4", "id": 29944439, "author": {"login": "AkshitaB", "id": 6500683, "node_id": "MDQ6VXNlcjY1MDA2ODM=", "avatar_url": "https://avatars.githubusercontent.com/u/6500683?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AkshitaB", "html_url": "https://github.com/AkshitaB", "followers_url": "https://api.github.com/users/AkshitaB/followers", "following_url": "https://api.github.com/users/AkshitaB/following{/other_user}", "gists_url": "https://api.github.com/users/AkshitaB/gists{/gist_id}", "starred_url": "https://api.github.com/users/AkshitaB/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AkshitaB/subscriptions", "organizations_url": "https://api.github.com/users/AkshitaB/orgs", "repos_url": "https://api.github.com/users/AkshitaB/repos", "events_url": "https://api.github.com/users/AkshitaB/events{/privacy}", "received_events_url": "https://api.github.com/users/AkshitaB/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTI5OTQ0NDM5", "tag_name": "v1.1.0rc4", "target_commitish": "master", "name": "v1.1.0rc4", "draft": false, "prerelease": true, "created_at": "2020-08-20T18:46:09Z", "published_at": "2020-08-20T18:50:56Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v1.1.0rc4", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v1.1.0rc4", "body": "## Changes since `v1.1.0rc3`\r\n\r\n### Added\r\n\r\n- Added a workflow to GitHub Actions that will automatically close unassigned stale issues and\r\n  ping the assignees of assigned stale issues.\r\n\r\n### Fixed\r\n\r\n- Fixed a bug in distributed metrics that caused nan values due to repeated addition of an accumulated variable.\r\n\r\n## Commits\r\n\r\n87a61ad9 Bug fix in distributed metrics (#4570)\r\n71a9a90d upgrade actions to cache@v2 (#4573)\r\nbd9ee6a4 Give better usage info for overrides parameter (#4575)\r\n0a456a75 Fix boolean and categorical accuracy for distributed (#4568)\r\n85112746 add actions workflow for closing stale issues (#4561)\r\nde413065 Static type checking fixes (#4545)\r\n5a07009b Fix RoBERTa SST (#4548)\r\n351941f3 Only pin mkdocs-material to minor version, ignore specific patch version (#4556)"}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/29623439", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/29623439/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/29623439/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v1.1.0rc3", "id": 29623439, "author": {"login": "epwalsh", "id": 8812459, "node_id": "MDQ6VXNlcjg4MTI0NTk=", "avatar_url": "https://avatars.githubusercontent.com/u/8812459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/epwalsh", "html_url": "https://github.com/epwalsh", "followers_url": "https://api.github.com/users/epwalsh/followers", "following_url": "https://api.github.com/users/epwalsh/following{/other_user}", "gists_url": "https://api.github.com/users/epwalsh/gists{/gist_id}", "starred_url": "https://api.github.com/users/epwalsh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/epwalsh/subscriptions", "organizations_url": "https://api.github.com/users/epwalsh/orgs", "repos_url": "https://api.github.com/users/epwalsh/repos", "events_url": "https://api.github.com/users/epwalsh/events{/privacy}", "received_events_url": "https://api.github.com/users/epwalsh/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTI5NjIzNDM5", "tag_name": "v1.1.0rc3", "target_commitish": "master", "name": "v1.1.0rc3", "draft": false, "prerelease": true, "created_at": "2020-08-12T20:15:41Z", "published_at": "2020-08-12T20:18:44Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v1.1.0rc3", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v1.1.0rc3", "body": "## Changes since `v1.1.0rc2`\r\n\r\n### Fixed\r\n\r\n- Fixed how truncation was handled with `PretrainedTransformerTokenizer`.\r\n  Previously, if `max_length` was set to `None`, the tokenizer would still do truncation if the\r\n  transformer model had a default max length in its config.\r\n  Also, when `max_length` was set to a non-`None` value, several warnings would appear\r\n  for certain transformer models around the use of the `truncation` parameter.\r\n- Fixed evaluation of all metrics when using distributed training.\r\n\r\n## Commits\r\n\r\n0ac13a4f fix CHANGELOG\r\n3b86f588 Prepare for release v1.1.0rc3\r\n44d28476 Metrics in distributed setting (#4525)\r\n1d619659 Bump mkdocs-material from 5.5.3 to 5.5.5 (#4547)\r\n5b977809 tick version for nightly releases\r\nb32608e3 add gradient checkpointing for transformer token embedders (#4544)\r\nf639336a Fix logger being created twice (#4538)\r\n660fdaf2 Fix handling of max length with transformer tokenizers (#4534)\r\n15e288f5 EpochCallBack for tracking epoch (#4540)\r\n9209bc91 Bump mkdocs-material from 5.5.0 to 5.5.3 (#4533)\r\nbfecdc3e Ensure len(self.evaluation_data_loader) is not called (#4531)\r\n5bc3b732 Fix typo in warning in file_utils (#4527)\r\ne80d7687 pin torch >= 1.6"}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/29173631", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/29173631/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/29173631/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v1.1.0rc2", "id": 29173631, "author": {"login": "epwalsh", "id": 8812459, "node_id": "MDQ6VXNlcjg4MTI0NTk=", "avatar_url": "https://avatars.githubusercontent.com/u/8812459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/epwalsh", "html_url": "https://github.com/epwalsh", "followers_url": "https://api.github.com/users/epwalsh/followers", "following_url": "https://api.github.com/users/epwalsh/following{/other_user}", "gists_url": "https://api.github.com/users/epwalsh/gists{/gist_id}", "starred_url": "https://api.github.com/users/epwalsh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/epwalsh/subscriptions", "organizations_url": "https://api.github.com/users/epwalsh/orgs", "repos_url": "https://api.github.com/users/epwalsh/repos", "events_url": "https://api.github.com/users/epwalsh/events{/privacy}", "received_events_url": "https://api.github.com/users/epwalsh/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTI5MTczNjMx", "tag_name": "v1.1.0rc2", "target_commitish": "master", "name": "v1.1.0rc2", "draft": false, "prerelease": true, "created_at": "2020-07-31T16:56:24Z", "published_at": "2020-07-31T17:03:21Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v1.1.0rc2", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v1.1.0rc2", "body": "## What's new since `v1.1.0rc1`\r\n\r\n### Changed\r\n\r\n- Upgraded PyTorch requirement to 1.6.\r\n- Replaced the NVIDIA Apex AMP module with torch's native AMP module. The default trainer (`GradientDescentTrainer`)\r\n  now takes a `use_amp: bool` parameter instead of the old `opt_level: str` parameter.\r\n\r\n### Fixed\r\n\r\n- Removed unnecessary warning about deadlocks in `DataLoader`.\r\n- Fixed testing models that only return a loss when they are in training mode.\r\n- Fixed a bug in `FromParams` that caused silent failure in case of the parameter type being `Optional[Union[...]]`.\r\n\r\n### Added\r\n\r\n- Added the option to specify `requires_grad: false` within an optimizer's parameter groups.\r\n- Added the `file-friendly-logging` flag back to the `train` command. Also added this flag to the `predict`, `evaluate`, and `find-learning-rate` commands.\r\n\r\n### Removed\r\n\r\n- Removed the `opt_level` parameter to `Model.load` and `load_archive`. In order to use AMP with a loaded\r\n  model now, just run the model's forward pass within torch's [`autocast`](https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.autocast)\r\n  context.\r\n\r\n## Commits\r\n\r\n73220d71 Prepare for release v1.1.0rc2\r\n9415350d Update torch requirement from <1.6.0,>=1.5.0 to >=1.5.0,<1.7.0 (#4519)\r\n146bd9ee Remove link to self-attention modules. (#4512)\r\n24012823 add back file-friendly-logging flag (#4509)\r\n54e5c83e closes #4494 (#4508)\r\nfa39d498 ensure __call__ methods are rendered in docs (#4522)\r\ne53d1858 Bug fix for case when param type is Optional[Union...] (#4510)\r\n14f63b77 Make sure we have a bool tensor where we expect one (#4505)\r\n18a4eb34 add a requires_grad option to param groups (#4502)\r\n6c848dfb Bump mkdocs-material from 5.4.0 to 5.5.0 (#4507)\r\nd73f8a91 More BART changes (#4500)\r\n1cab3bfe Update beam_search.py (#4462)\r\n478bf46c remove deadlock warning in DataLoader (#4487)\r\n714334ad Fix reported loss: Bug fix in batch_loss (#4485)\r\ndb20b1fb use longer tqdm intervals when output being redirected (#4488)\r\n53eeec10 tick version for nightly releases\r\nd693cf1c PathLike (#4479)\r\n2f878322 only show validation progress bar from main process (#4476)\r\n9144918d Fix reported loss (#4477)\r\n5c970833 fix release link in CHANGELOG and formatting in README"}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/28561753", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/28561753/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/28561753/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v1.1.0rc1", "id": 28561753, "author": {"login": "epwalsh", "id": 8812459, "node_id": "MDQ6VXNlcjg4MTI0NTk=", "avatar_url": "https://avatars.githubusercontent.com/u/8812459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/epwalsh", "html_url": "https://github.com/epwalsh", "followers_url": "https://api.github.com/users/epwalsh/followers", "following_url": "https://api.github.com/users/epwalsh/following{/other_user}", "gists_url": "https://api.github.com/users/epwalsh/gists{/gist_id}", "starred_url": "https://api.github.com/users/epwalsh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/epwalsh/subscriptions", "organizations_url": "https://api.github.com/users/epwalsh/orgs", "repos_url": "https://api.github.com/users/epwalsh/repos", "events_url": "https://api.github.com/users/epwalsh/events{/privacy}", "received_events_url": "https://api.github.com/users/epwalsh/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTI4NTYxNzUz", "tag_name": "v1.1.0rc1", "target_commitish": "master", "name": "v1.1.0rc1", "draft": false, "prerelease": true, "created_at": "2020-07-14T20:54:41Z", "published_at": "2020-07-14T21:00:22Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v1.1.0rc1", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v1.1.0rc1", "body": "This is the first pre-release candidate for version 1.1. There will probably be at least more candidate before the true 1.1 release.\r\n\r\n## What's new since v1.0.0\r\n\r\n### Fixed\r\n\r\n- Reduced the amount of log messages produced by `allennlp.common.file_utils`.\r\n- Fixed a bug where `PretrainedTransformerEmbedder` parameters appeared to be trainable\r\n  in the log output even when `train_parameters` was set to `False`.\r\n- Fixed a bug with the sharded dataset reader where it would only read a fraction of the instances\r\n  in distributed training.\r\n- Fixed checking equality of `ArrayField`s.\r\n- Fixed a bug where `NamespaceSwappingField` did not work correctly with `.empty_field()`.\r\n- Put more sensible defaults on the `huggingface_adamw` optimizer.\r\n- Simplified logging so that all logging output always goes to one file.\r\n- Fixed interaction with the python command line debugger.\r\n- Log the grad norm properly even when we're not clipping it.\r\n- Fixed a bug where `PretrainedModelInitializer` fails to initialize a model with a 0-dim tensor\r\n- Fixed a bug with the layer unfreezing schedule of the `SlantedTriangular` learning rate scheduler.\r\n- Fixed a regression with logging in the distributed setting. Only the main worker should write log output to the terminal.\r\n- Pinned the version of boto3 for package managers (e.g. poetry).\r\n- Fixed issue #4330 by updating the `tokenizers` dependency.\r\n- Fixed a bug in `TextClassificationPredictor` so that it passes tokenized inputs to the `DatasetReader`\r\n  in case it does not have a tokenizer.\r\n- `reg_loss` is only now returned for models that have some regularization penalty configured.\r\n- Fixed a bug that prevented `cached_path` from downloading assets from GitHub releases.\r\n- Fixed a bug that erronously increased last label's false positive count in calculating fbeta metrics.\r\n- `Tqdm` output now looks much better when the output is being piped or redirected.\r\n- Small improvements to how the API documentation is rendered.\r\n\r\n### Added\r\n\r\n- A method to ModelTestCase for running basic model tests when you aren't using config files.\r\n- Added some convenience methods for reading files.\r\n- Added an option to `file_utils.cached_path` to automatically extract archives.\r\n- Added the ability to pass an archive file instead of a local directory to `Vocab.from_files`.\r\n- Added the ability to pass an archive file instead of a glob to `ShardedDatasetReader`.\r\n- Added a new `\"linear_with_warmup\"` learning rate scheduler.\r\n- Added a check in `ShardedDatasetReader` that ensures the base reader doesn't implement manual\r\n  distributed sharding itself.\r\n- Added an option to `PretrainedTransformerEmbedder` and `PretrainedTransformerMismatchedEmbedder` to use a\r\n  scalar mix of all hidden layers from the transformer model instead of just the last layer. To utilize\r\n  this, just set `last_layer_only` to `False`.\r\n- `cached_path()` can now read files inside of archives.\r\n\r\n### Changed\r\n\r\n- Not specifying a `cuda_device` now automatically determines whether to use a GPU or not.\r\n- Discovered plugins are logged so you can see what was loaded.\r\n- `allennlp.data.DataLoader` is now an abstract registrable class. The default implementation\r\nremains the same, but was renamed to `allennlp.data.PyTorchDataLoader`.\r\n- `BertPooler` can now unwrap and re-wrap extra dimensions if necessary.\r\n- New `transformers` dependency. Only version >=3.0 now supported.\r\n\r\n## Commits\r\n\r\n4eb97953 Prepare for release v1.1.0rc1\r\nf195440b update 'Models' links in README (#4475)\r\n9c801a3c add CHANGELOG to API docs, point to license on GitHub, improve API doc formatting (#4472)\r\n69d2f03d Clean up Tqdm bars when output is being piped or redirected (#4470)\r\n7b188c93 fixed bug that erronously increased last label's false positive count (#4473)\r\n64db027d Skip ETag check if OSError (#4469)\r\nb9d011ef More BART changes (#4468)\r\n7a563a8f add option to use scalar mix of all transformer layers (#4460)\r\nd00ad668 Minor tqdm and logging clean up (#4448)\r\n6acf2058 Fix regloss logging (#4449)\r\n8c32ddfd Fixing bug in TextClassificationPredictor so that it passes tokenized inputs to the DatasetReader (#4456)\r\nb9a91646 Update transformers requirement from <2.12,>=2.10 to >=2.10,<3.1 (#4446)\r\n181ef5d2 pin boto3 to resolve some dependency issues (#4453)\r\nc75a1ebd ensure base reader of ShardedDatasetReader doesn't implement sharding itself (#4454)\r\n8a05ad43 Update CONTRIBUTING.md (#4447)\r\n5b988d63 ensure only rank 0 worker writes to terminal (#4445)\r\n8482f022 fix bug with SlantedTriangular LR scheduler (#4443)\r\ne46a578e Update transformers requirement from <2.11,>=2.10 to >=2.10,<2.12 (#4411)\r\n8229aca3 Fix pretrained model initialization (#4439)\r\n60deece9 Fix type hint in text_field.py (#4434)\r\n23e549e4 More multiple-choice changes (#4415)\r\n6d0a4fd2 generalize DataLoader (#4416)\r\nacd99952 Automatic file-friendly logging (#4383)\r\n637dbb15 fix README, pin mkdocs, update mkdocs-material (#4412)\r\n9c4dfa54 small fix to pretrained transformer tokenizer (#4417)\r\n84988b81 Log plugins discovered and filter out transformers \"PyTorch version ... available\" log message (#4414)\r\n54c41fcc Adds the ability to automatically detect whether we have a GPU (#4400)\r\n96ff5851 Changes from my multiple-choice work (#4368)\r\neee15ca8 Assign an empty mapping array to empty fields of `NamespaceSwappingField` (#4403)\r\naa2943e5 Bump mkdocs-material from 5.3.2 to 5.3.3 (#4398)\r\n7fa7531c fix __eq__ method of ArrayField (#4401)\r\ne104e441 Add test to ensure data loader yields all instances when batches_per_epoch is set (#4394)\r\nb6fd6978 fix sharded dataset reader (#4396)\r\n30e5dbfc Bump mypy from 0.781 to 0.782 (#4395)\r\nb0ba2d4c update version\r\n1d07cc75 Bump mkdocs-material from 5.3.0 to 5.3.2 (#4389)\r\nffc51843 ensure Vocab.from_files and ShardedDatasetReader can handle archives (#4371)\r\n20afe6ce Add Optuna integrated badge to README.md (#4361)\r\nba79f146 Bump mypy from 0.780 to 0.781 (#4390)\r\n85e531c2 Update README.md (#4385)\r\nc2ecb7a2 Add a method to ModelTestCase for use without config files (#4381)\r\n6852deff pin some doc building requirements (#4386)\r\nbf422d56 Add github template for using your own python run script (#4380)\r\nebde6e85 Bump overrides from 3.0.0 to 3.1.0 (#4375)\r\ne52b7518 ensure transformer params are frozen at initialization when train_parameters is false (#4377)\r\n3e8a9ef6 Add link to new template repo for config file development (#4372)\r\n4f70bc93 tick version for nightly releases\r\n63a5e158 Update spacy requirement from <2.3,>=2.1.0 to >=2.1.0,<2.4 (#4370)\r\nef7c75b8 reduce amount of log messages produced by file_utils (#4366)"}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/26691389", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/26691389/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/26691389/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v1.0.0", "id": 26691389, "author": {"login": "schmmd", "id": 954798, "node_id": "MDQ6VXNlcjk1NDc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/954798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/schmmd", "html_url": "https://github.com/schmmd", "followers_url": "https://api.github.com/users/schmmd/followers", "following_url": "https://api.github.com/users/schmmd/following{/other_user}", "gists_url": "https://api.github.com/users/schmmd/gists{/gist_id}", "starred_url": "https://api.github.com/users/schmmd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/schmmd/subscriptions", "organizations_url": "https://api.github.com/users/schmmd/orgs", "repos_url": "https://api.github.com/users/schmmd/repos", "events_url": "https://api.github.com/users/schmmd/events{/privacy}", "received_events_url": "https://api.github.com/users/schmmd/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTI2NjkxMzg5", "tag_name": "v1.0.0", "target_commitish": "master", "name": "v1.0.0", "draft": false, "prerelease": false, "created_at": "2020-06-16T18:12:25Z", "published_at": "2020-06-16T18:29:23Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v1.0.0", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v1.0.0", "body": "The 1.0 version of AllenNLP is the culmination of more than 500 commits over the course of several months of work from our engineering team. The AllenNLP library has had wide-reaching appeal so far in its lifetime, and this 1.0 release represents an important maturity milestone. While we will continue to move fast to keep up with the ever-changing state of the art, we will be increasingly conscious of the effect future API changes have on our existing user base.\r\n\r\nThis release touches almost every aspect of the library, ranging from improving documentation to adding new natural-language processing components, to adjusting our APIs so they serve the community for the long haul.  While we cannot summarize everything in these release notes, here are some of the main milestones for the 1.0 release.\r\n\r\n1. We are releasing several new models, such as:\r\n  a. TransformerQA, a reading comprehension model ([paper](https://arxiv.org/abs/1810.04805), [demo](https://demo.allennlp.org/reading-comprehension))\r\n  b. An improved coreference model, with a 17% absolute improvement ([architecture paper](https://www.aclweb.org/anthology/N18-2108/)/[embedder paper](https://arxiv.org/abs/1907.10529), [demo](https://demo.allennlp.org/coreference-resolution))\r\n  c. The NMN reading comprehension model ([paper](https://arxiv.org/abs/1912.04971), [demo](https://demo.allennlp.org/reading-comprehension))\r\n  d. The RoBERTa models for textual entailment, or NLI ([paper](https://www.semanticscholar.org/paper/RoBERTa%3A-A-Robustly-Optimized-BERT-Pretraining-Liu-Ott/077f8329a7b6fa3b7c877a57b81eb6c18b5f87de), [demo](https://demo.allennlp.org/textual-entailment))\r\n\r\n2. We have new introductory material in the form of [an interactive guide](https://allennlp-guide.apps.allenai.org/), showing how to use library components and our experiment framework.  The guide's goal is to provide a comprehensive introduction to AllenNLP for people with a good understanding of machine learning, Python, and some PyTorch.\r\n\r\n3. We have improved performance across the library.\r\n  a. Switching to native PyTorch data loading, which is not only much faster but also allows the three main parts of the library (data, model, and training) to interoperate with any native PyTorch code.\r\n  b. Enabled support for 16-bit floating point through Apex.\r\n  c. Multi-GPU training now utilizes a separate Python process for each GPU. These workers communicate using PyTorch's `distributed` module. This is more efficient than the old system which used a single Python process and was therefore limited by the GIL.\r\n\r\n4. We separated our models into a model repository ([allennlp-models](https://github.com/allenai/allennlp-models)), so we have a lean core library with fewer dependencies.\r\n\r\n5. We dramatically simplified how AllenNLP code corresponds to AllenNLP configuration files, which also makes the library easy to use from raw Python.\r\n\r\nBut changes are not limited to these. Some other highlights are that we have:\r\n\r\n1. Support for gradient accumulation.\r\n1. Improved configurability of the trainer so you can inject your own call on each batch.\r\n1. Seamless support for using word-piece tokenization on pre-tokenized text.\r\n1. A sampler that creates batches with roughly equal numbers of tokens.\r\n1. Unified support for Huggingface's transformer library.\r\n1. Support for token type IDs throughout the library.\r\n1. Nightly releases of the library to pip.\r\n1. BLEU and ROUGE metrics.\r\n\r\n# Updates since `v1.0.0rc6`\r\n\r\n## Fixed\r\n\r\n- Lazy dataset readers now work correctly with multi-process data loading.\r\n- Fixed race conditions that could occur when using a dataset cache.\r\n\r\n## Added\r\n\r\n- A bug where where all datasets would be loaded for vocab creation even if not needed.\r\n- A parameter to the `DatasetReader` class: `manual_multi_process_sharding`. This is similar\r\n  to the `manual_distributed_sharding` parameter, but applies when using a multi-process\r\n  `DataLoader`.\r\n\r\n## Commits\r\n\r\n29f3b6c3 Prepare for release v1.0.0\r\na8b840df fix some formatting issues in README (#4365)\r\nd3ed6197 fix Makefile\r\nc5549105 quick doc fixes (#4364)\r\nb764bef5 simplify dataset classes, fix multi-process lazy loading (#4344)\r\n884a6149 Bump mkdocs-material from 5.2.3 to 5.3.0 (#4359)\r\n6a124d80 ensure 'from_files' vocab doesn't load instances (#4356)\r\n87c23e4a Fix handling of \"datasets_for_vocab_creation\" param (#4350)\r\nc3755d16 update CHANGELOG\r\n\r\n# Upgrade guide from `v0.9.0`\r\n\r\nThere are too many changes to be exhaustive, but here is a list of the most common issues:\r\n\r\n* You can continue to use the `allennlp` command line, but if you want to invoke it through Python, use `python -m allennlp <command>` instead of `python -m allennlp.run <command>`.\r\n* `\"bert_adam\"` is now `\"adamw\"`.\r\n* We no longer support the `\"gradient_accumulation_batch_size\"` parameter to the trainer. Use `\"num_gradient_accumulation_steps\"` instead.\r\n\r\n## Using the `transformers` library\r\n\r\nAllenNLP 1.0 replaces the mash-mash of transformer libraries and dependencies that we had in `v0.9.0`, and replaces it with one implementation that uses https://github.com/huggingface/transformers under the hood. For cases where you can work directly with the word pieces that are used by the transformers, use `\"pretrained_transformer\"` for tokenizers, indexers, and embedders. If you want to use tokens from pre-tokenized text, use `\"\"pretrained_transformer_mismatched\"`. The latter turns the text into word pieces, embeds them with the transformer, and then combines word pieces to produce an embedding for the original tokens.\r\n\r\nThe parameters `requires_grad` and `top_layer_only` are no longer supported. If you are converting an old model that used to use `\"bert-pretrained\"`, this is important! `requires_grad` used to be `False` by default, so it would not train the transformer itself. This saves memory and time at the cost of performance. The new code does not support this setting, and will always train the transformer. You can prevent this by setting `requires_grad` to `False` in a parameter group when setting up the optimizer.\r\n\r\nYou no longer need to specify `do_lowercase`, as this is handled automatically now. \r\n\r\n## Config file changes\r\n\r\nIn 1.0, we simplified how `FromParams` works. As a result, some things in the config files need to change to work with 1.0:\r\n\r\n* The way Vocabulary options are specified in config files has changed. See #3550. If you want to load a vocabulary from files, you should specify `\"type\": \"from_files\"`, and use the key `\"directory\"` instead of `\"directory_path\"`.\r\n* When instantiating a `BasicTextFieldEmbedder` `from_params`, you used to be able to have embedder names be top-level keys in the config file (e.g., `\"embedder\": {\"elmo\": ELMO_PARAMS, \"tokens\": TOKEN_PARAMS}`).  We changed this a long time ago to prefer wrapping them in a `\"token_embedders\"` key, and this is now required (e.g., `\"embedder\": {\"token_embedders\": {\"elmo\": ELMO_PARAMS, \"tokens\": TOKEN_PARAMS}}`).\r\n* The `TokenCharactersEncoder` now requires you to specify the `vocab_namespace` for the underlying embedder.  It used to default to `\"token_characters\"`, matching the `TokenCharactersIndexer` default, but making that work required some custom magic that wasn't worth the complexity.  So instead of `\"token_characters\": {\"type\": \"character_encoding\", \"embedding\": {\"embedding_dim\": 25}, \"encoder\": {...}}`, you need to change this to: `\"token_characters\": {\"type\": \"character_encoding\", \"embedding\": {\"embedding_dim\": 25, \"vocab_namespace\": \"token_characters\"}, \"encoder\": {...}}`\r\n* Regularization now needs another key in a config file. Instead of specifying regularization as `\"regularizer\": [[regex1, regularizer_params], [regex2, regularizer_params]]`, it now must be specified as `\"regularizer\": {\"regexes\": [[regex1, regularizer_params], [regex2, regularizer_params]]}`.\r\n* We changed initialization in a similar way to regularization. Instead of specifying initialization as `\"initializer\": [[regex1, initializer_params], [regex2, initializer_params]]`, it now must be specified as `\"initializer\": {\"regexes\": [[regex1, initializer_params], [regex2, initializer_params]]}`. Also, you used to be able to have `initializer_params` be `\"prevent\"`, to prevent initialization of matching parameters.  This is now done with a separate key passed to the initializer: `\"initializer\": {\"regexes\": [..], \"prevent_regexes\": [regex1, regex2]}.\r\n* `num_serialized_models_to_keep` and `keep_serialized_model_every_num_seconds` used to be able to be passed as top-level parameters to the `trainer`, but now they must always be passed to the `checkpointer` instead.  For example, if you had `\"trainer\": {\"num_serialized_models_to_keep\": 1}`, it now needs to be `\"trainer\": {\"checkpointer\": {\"num_serialized_models_to_keep\": 1}}`. Also, the default for that setting is now `2`, so AllenNLP will no longer fill up your hard drive!\r\n* Tokenizer specification changed because of #3361. Instead of something like `\"tokenizer\": {\"word_splitter\": {\"type\": \"spacy\"}}`, you now just do `\"tokenizer\": {\"type\": \"spacy\"}` (more technically: the `WordTokenizer` has now been removed, with the things we used to call `WordSplitters` now just moved up to be top-level `Tokenizers` themselves).\r\n* The `namespace_to_cache` argument to `ElmoTokenEmbedder` has been removed as a config file option.  You can still pass `vocab_to_cache` to the constructor of this class, but this functionality is no longer available from a config file.  If you used this and are really put out by this change, let us know, and we'll see what we can do.\r\n\r\n## Iterators \u2794 DataLoaders\r\n\r\nAllennlp now uses PyTorch's API for data iteration, rather than our own custom one. This means that `train_data`, `validation_data`, `iterator` and `validation_iterator` arguments to the `Trainer` have been removed and replaced with `data_loader` and `validation_dataloader`.\r\n\r\n*Previous config files which looked like:*\r\n```\r\n{\r\n  \"iterator\": {\r\n    \"type\": \"bucket\",\r\n    \"sorting_keys\": [[\"tokens\"], [\"num_tokens\"]],\r\n    \"padding_noise\": 0.1\r\n    ...\r\n  }\r\n}\r\n```\r\n*Now become:*\r\n\r\n```\r\n{\r\n  \"data_loader\": {\r\n    \"batch_sampler\" {\r\n      \"type\": \"bucket\",\r\n      // sorting keys are no longer required! They can be inferred automatically.\r\n      \"padding_noise\": 0.1\r\n      ...\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n## Multi-GPU\r\n\r\nAllennlp now uses `DistributedDataParallel` for parallel training, rather than `DataParallel`. With `DistributedDataParallel`, each worker (GPU) runs in it's own process. As such, each process also has its own `Trainer`, which now takes a single GPU ID only.\r\n\r\n*Previous config files which looked like:*\r\n```\r\n{\r\n  \"trainer\": {\r\n    \"cuda_device\": [0, 1, 2, 3],\r\n    \"num_epochs\": 20,\r\n    ...\r\n  }\r\n}\r\n```\r\n*Now become:*\r\n\r\n```\r\n{\r\n  \"distributed\": {\r\n    \"cuda_devices\": [0, 1, 2, 3],\r\n  },\r\n  \"trainer\": {\r\n    \"num_epochs\": 20,\r\n    ...\r\n  }\r\n}\r\n```"}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/27473525", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/27473525/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/27473525/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v1.0.0rc6", "id": 27473525, "author": {"login": "epwalsh", "id": 8812459, "node_id": "MDQ6VXNlcjg4MTI0NTk=", "avatar_url": "https://avatars.githubusercontent.com/u/8812459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/epwalsh", "html_url": "https://github.com/epwalsh", "followers_url": "https://api.github.com/users/epwalsh/followers", "following_url": "https://api.github.com/users/epwalsh/following{/other_user}", "gists_url": "https://api.github.com/users/epwalsh/gists{/gist_id}", "starred_url": "https://api.github.com/users/epwalsh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/epwalsh/subscriptions", "organizations_url": "https://api.github.com/users/epwalsh/orgs", "repos_url": "https://api.github.com/users/epwalsh/repos", "events_url": "https://api.github.com/users/epwalsh/events{/privacy}", "received_events_url": "https://api.github.com/users/epwalsh/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTI3NDczNTI1", "tag_name": "v1.0.0rc6", "target_commitish": "master", "name": "v1.0.0rc6", "draft": false, "prerelease": true, "created_at": "2020-06-11T21:40:30Z", "published_at": "2020-06-11T21:44:35Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v1.0.0rc6", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v1.0.0rc6", "body": "## Fixed\r\n\r\n- A bug where `TextField`s could not be duplicated since some tokenizers cannot be deep-copied.\r\n  See https://github.com/allenai/allennlp/issues/4270.\r\n- Our caching mechanism had the potential to introduce race conditions if multiple processes\r\n  were attempting to cache the same file at once. This was fixed by using a lock file tied to each\r\n  cached file.\r\n- `get_text_field_mask()` now supports padding indices that are not `0`.\r\n- A bug where `predictor.get_gradients()` would return an empty dictionary if an embedding layer had trainable set to false\r\n- Fixes `PretrainedTransformerMismatchedIndexer` in the case where a token consists of zero word pieces.\r\n- Fixes a bug when using a lazy dataset reader that results in a `UserWarning` from PyTorch being printed at\r\n  every iteration during training.\r\n- Predictor names were inconsistently switching between dashes and underscores. Now they all use underscores.\r\n- `Predictor.from_path` now automatically loads plugins (unless you specify `load_plugins=False`) so\r\n  that you don't have to manually import a bunch of modules when instantiating predictors from\r\n  an archive path.\r\n- `allennlp-server` automatically found as a plugin once again.\r\n\r\n## Added\r\n\r\n- A `duplicate()` method on `Instance`s and `Field`s, to be used instead of `copy.deepcopy()`\r\n- A batch sampler that makes sure each batch contains approximately the same number of tokens (`MaxTokensBatchSampler`)\r\n- Functions to turn a sequence of token indices back into tokens\r\n- The ability to use Huggingface encoder/decoder models as token embedders\r\n- Improvements to beam search\r\n- ROUGE metric\r\n- Polynomial decay learning rate scheduler\r\n- A `BatchCallback` for logging CPU and GPU memory usage to tensorboard. This is mainly for debugging\r\n  because using it can cause a significant slowdown in training.\r\n- Ability to run pretrained transformers as an embedder without training the weights\r\n\r\n## Changed\r\n\r\n- Similar to our caching mechanism, we introduced a lock file to the vocab to avoid race\r\n  conditions when saving/loading the vocab from/to the same serialization directory in different processes.\r\n- Changed the `Token`, `Instance`, and `Batch` classes along with all `Field` classes to \"slots\" classes. This dramatically reduces the size in memory of instances.\r\n- SimpleTagger will no longer calculate span-based F1 metric when `calculate_span_f1` is `False`.\r\n- CPU memory for every worker is now reported in the logs and the metrics. Previously this was only reporting the CPU memory of the master process, and so it was only\r\n  correct in the non-distributed setting.\r\n- To be consistent with PyTorch `IterableDataset`, `AllennlpLazyDataset` no longer implements `__len__()`.\r\n  Previously it would always return 1.\r\n- Removed old tutorials, in favor of [the new AllenNLP Guide](https://guide.allennlp.org)\r\n- Changed the vocabulary loading to consider new lines for Windows/Linux and Mac.\r\n\r\n## Commits\r\n\r\nd98d13b5 add 'allennlp_server' to default plugins (#4348)\r\n33d0cd8c fix file utils test (#4349)\r\nf4d330a2 Update vocabulary load to a system-agnostic newline (#4342)\r\n2012fea9 remove links to tutorials in API docs (#4346)\r\n3d8ce442 Fixes spelling in changelog\r\n73289bc8 Consistently use underscores in Predictor names (#4340)\r\n2d03c413 Allow using pretrained transformers without fine-tuning them (#4338)\r\n8f68d69b load plugins from Predictor.from_path (#4333)\r\n5c6cc3a2 Bump mkdocs-material from 5.2.2 to 5.2.3 (#4341)\r\n7ab7551b Removing old tutorials, pointing to the new guide in the README (#4334)\r\n902d36a5 Fix bug with lazy data loading, un-implement __len__ on AllennlpLazyDataset (#4328)\r\n11b57996 log metrics in alphabetical order (#4327)\r\n7d66b3e7 report CPU memory usage for each worker (#4323)\r\n06bac68b make Instance, Batch, and all field classes \"slots\" classes (#4313)\r\n2b2d1413 Bump mypy from 0.770 to 0.780 (#4316)\r\na038c01a Update transformers requirement from <2.11,>=2.9 to >=2.9,<2.12 (#4315)\r\n345459e9 Stop calculating span-based F1 metric when `calculate_span_f1` is `False`. (#4302)\r\nfc47bf6a Deals with the case where a word doesn't have any word pieces assigned (#4301)\r\n11a08ae7 Making Token class a \"slots\" class (#4312)\r\n32bccfbd Fix a bug where predictor.get_gradients() would return an empty...  (#4305)\r\n33a49454 ensure CUDA available in GPU checks workflow (#4310)\r\nd51ffa11 Update transformers requirement from <2.10,>=2.9 to >=2.9,<2.11 (#4282)\r\n75c07ab5 Merge branch 'master' of github.com:allenai/allennlp\r\n8c9421da fix Makefile\r\n77b432f6 Update README.md (#4309)\r\n720ad434 A few small fixes in the README.md (#4307)\r\na7265c04 move tensorboard memory logging to BatchCallback (#4306)\r\n91d0fa1a remove setup.cfg (#4300)\r\n5ad7a33a Support for bart in allennlp-models (#4169)\r\n25134f2b add lock file within caching and vocab saving/loading mechanisms (#4299)\r\n58dc84ea add 'Feature request' label to template\r\n9526f007 Update issue templates (#4293)\r\n79999ec0 Adds a \"duplicate()\" method on instances and fields (#4294)\r\n8ff47d34 Set version to rc6"}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/26922178", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/26922178/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/26922178/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v1.0.0rc5", "id": 26922178, "author": {"login": "dirkgr", "id": 920638, "node_id": "MDQ6VXNlcjkyMDYzOA==", "avatar_url": "https://avatars.githubusercontent.com/u/920638?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dirkgr", "html_url": "https://github.com/dirkgr", "followers_url": "https://api.github.com/users/dirkgr/followers", "following_url": "https://api.github.com/users/dirkgr/following{/other_user}", "gists_url": "https://api.github.com/users/dirkgr/gists{/gist_id}", "starred_url": "https://api.github.com/users/dirkgr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dirkgr/subscriptions", "organizations_url": "https://api.github.com/users/dirkgr/orgs", "repos_url": "https://api.github.com/users/dirkgr/repos", "events_url": "https://api.github.com/users/dirkgr/events{/privacy}", "received_events_url": "https://api.github.com/users/dirkgr/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTI2OTIyMTc4", "tag_name": "v1.0.0rc5", "target_commitish": "master", "name": "", "draft": false, "prerelease": false, "created_at": "2020-05-26T23:17:16Z", "published_at": "2020-05-26T23:19:58Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v1.0.0rc5", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v1.0.0rc5", "body": "### Fixed\r\n\r\n- Fix bug where `PretrainedTransformerTokenizer` crashed with some transformers (#4267)\r\n- Make `cached_path` work offline.\r\n- Tons of docstring inconsistencies resolved.\r\n- Nightly builds no longer run on forks.\r\n- Distributed training now automatically figures out which worker should see which instances\r\n- A race condition bug in distributed training caused from saving the vocab to file from the master process while other processing might be reading those files.\r\n- Unused dependencies in `setup.py` removed\r\n\r\n### Added\r\n\r\n- Additional CI checks to ensure docstrings are consistently formatted.\r\n- Ability to train on CPU with multiple processes by setting `cuda_devices` to a list of negative integers in your training config. For example: `\"distributed\": {\"cuda_devices\": [-1, -1]}`. This is mainly to make it easier to test and debug distributed training code.\r\n- Documentation for when parameters don't need config file entries\r\n\r\n### Changed\r\n\r\n- The `allennlp test-install` command now just ensures the core submodules can\r\nbe imported successfully, and prints out some other useful information such as the version, PyTorch version, and the number of GPU devices available.\r\n- All of the tests moved from `allennlp/tests` to `tests` at the root level, and\r\n`allennlp/tests/fixtures` moved to `test_fixtures` at the root level. The PyPI source and wheel distributions will no longer include tests and fixtures.\r\n\r\n### Commits\r\n\r\n7dcc60be Update version for release v1.0.0rc5\r\nf421e911 clean up dependencies (#4290)\r\na9be961c Bump mkdocs-material from 5.2.0 to 5.2.2 (#4288)\r\n69fc5b46 Update saliency_interpreter.py (#4286)\r\ne52fea28 Makes the EpochCallback work the same way as the BatchCallback (#4277)\r\n6574823c Make special token inference logic more robust (#4267)\r\n24617c02 Bump overrides from 2.8.0 to 3.0.0 (#4249)\r\nf7d96730 Bump mkdocs-material from 5.1.6 to 5.2.0 (#4257)\r\n5198a5cd Document when parameters do not need an entry in a config file (#4275)\r\n4ee2735a update contribution guidelines (#4271)\r\ndacbb757 wait for non-master workers to finish reading vocab before master worker saves it (#4274)\r\nf27475aa Enable multi-process training on CPU (#4272)\r\n7e683ddb Workers in the distributed scenario need to see different instances (#4241)\r\n9c51d6c8 move test and fixtures to root level and simplify test-install command (#4264)\r\n65a146d2 Clean up the command to create a commit list. (#4263)\r\n88683d46 switch to tokenless codecov upload (#4261)\r\nb41d4487 Add a CHANGELOG (#4260)\r\n7d71398b make 'cached_path' work offline (#4253)\r\nfc810670 move py2md back to scripts (#4251)\r\n4de68a42 Improves API docs and docstring consistency (#4244)\r\n1b0d231d tick version to rc5\r\n"}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/26533643", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/26533643/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/26533643/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v1.0.0rc4", "id": 26533643, "author": {"login": "epwalsh", "id": 8812459, "node_id": "MDQ6VXNlcjg4MTI0NTk=", "avatar_url": "https://avatars.githubusercontent.com/u/8812459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/epwalsh", "html_url": "https://github.com/epwalsh", "followers_url": "https://api.github.com/users/epwalsh/followers", "following_url": "https://api.github.com/users/epwalsh/following{/other_user}", "gists_url": "https://api.github.com/users/epwalsh/gists{/gist_id}", "starred_url": "https://api.github.com/users/epwalsh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/epwalsh/subscriptions", "organizations_url": "https://api.github.com/users/epwalsh/orgs", "repos_url": "https://api.github.com/users/epwalsh/repos", "events_url": "https://api.github.com/users/epwalsh/events{/privacy}", "received_events_url": "https://api.github.com/users/epwalsh/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTI2NTMzNjQz", "tag_name": "v1.0.0rc4", "target_commitish": "master", "name": "Version 1.0.0 Release Candidate 4", "draft": false, "prerelease": true, "created_at": "2020-05-14T20:59:39Z", "published_at": "2020-05-14T21:01:15Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v1.0.0rc4", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v1.0.0rc4", "body": "## Commits\r\n\r\n3f193367 gitignore evalb binary (#4235)\r\n189624df Make some arguments to evaluate() optional, add docstring (#4237)\r\n52274202 remove pre-commit (#4236)\r\n6701e593 Bump mkdocs-material from 5.1.5 to 5.1.6 (#4221)\r\ne3d72fcb Use the new tokenizers (#3868)\r\n592c6534 Add a more informative exception when there's no GPU available. (#4230)\r\n114751f7 Attach nltk and HF caches to docker containers in CI (#4232)\r\n7d9b72cd remove unused path in test image (#4229)\r\n89238d22 Remove unused path. (#4226)\r\nb461f3f3 fix new linting errors (#4227)\r\n0bcab362 consolidate testing decorators (#4213)\r\n72061b15 ensure Docker images get the right name and tag (#4214)\r\nedf91ac2 improve Docker workflows (#4210)\r\nb916720b Add wordpiece_mask to default to bool tensor (#4206)\r\n82bf58ae Switch to pytest style test classes, use plain asserts (#4204)\r\n743d2d83 Separate linting from formatting in CI, always run all steps of workflow (#4202)\r\n4e47894f Add linke to allennlp-models in README (#4196)\r\n1895743a Bump mkdocs-material from 5.1.4 to 5.1.5 (#4195)\r\nb6e0ba96 attach allennlp cache to Docker images and fix apex test (#4197)\r\nc4583034 Fix Bug in evaluate script (#4199)\r\n967660a4 remove namespace plugin mechanism (#4188)\r\nc09833c3 Remove allennlp sparse_clip_grad and replace with torch clip_grad_norm_. (#4159)\r\n42a4e638 Adds links in readme to stable and latest docs (#4186)\r\nd67e7213 Fix heuristic in util.get_token_ids_from_text_field_tensors (#4184)\r\n2602c8ff improve error message for Vocab.get_token_index (#4185)\r\n31616dec tweak torch requirement (#4166)\r\ne99de854 Improve Docker-based workflows (#4183)\r\nab421894 update GPU checks CI (#4182)\r\ne56992bc Add failing from_archive test (#4156)\r\ncbe74586 Add self-hosted runner for GPU checks (#4180)\r\n2544e598 Find the right embedding layer for mismatched cases (#4179)\r\n74c84049 Bump mkdocs-material from 5.1.3 to 5.1.4 (#4174)\r\n0f8346de Fix XLNet token type number (#4125)\r\nca9118f2 ensure docs can build in PR workflow (#4178)\r\n6038fd1a remove verbose mode of linters (#4176)\r\n7cbeb6c2 Display activation functions as modules. (#4045)\r\nbe53f071 add other missing param to ReduceOnPlateau LR Scheduler (#4177)\r\n706bf528 DOCS: Clean up the docs for commands (#4145)\r\n82cae1b4 add missing param `threshold_mode` (#4173)\r\n52ae7928 Use new env var in the allennlp-models build (#4172)\r\n08874e95 remove elmo command (#4168)\r\n4a6023b3 Fix logging (#4164)\r\n26e313b7 tick version to rc4 for nightly releases\r\n11f4307d Reduce number of warnings seen after running tests (#4153)\r\naf890b29 add test for version convention (#4157)\r\nd9dd5038 ensure typing backport uninstalled first (#4162)\r\n9d8862a6 Move default predictors (#4154)\r\nb0c7ac76 Uninstall typing before running pip again (#4161)\r\n0fe28393 Fixes an error with pip (#4158)\r\n69e7511f Bump mkdocs-material from 5.1.1 to 5.1.3 (#4150)\r\ndfe8b1ce clean up scripts dir (#4152)\r\n27e374ac Fix `from_params` when the class has no registered impl (#4090)\r\nbc2435d4 Update RELEASE_PROCESS.md (#4151)"}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/25936733", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/25936733/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/25936733/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v1.0.0rc3", "id": 25936733, "author": {"login": "epwalsh", "id": 8812459, "node_id": "MDQ6VXNlcjg4MTI0NTk=", "avatar_url": "https://avatars.githubusercontent.com/u/8812459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/epwalsh", "html_url": "https://github.com/epwalsh", "followers_url": "https://api.github.com/users/epwalsh/followers", "following_url": "https://api.github.com/users/epwalsh/following{/other_user}", "gists_url": "https://api.github.com/users/epwalsh/gists{/gist_id}", "starred_url": "https://api.github.com/users/epwalsh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/epwalsh/subscriptions", "organizations_url": "https://api.github.com/users/epwalsh/orgs", "repos_url": "https://api.github.com/users/epwalsh/repos", "events_url": "https://api.github.com/users/epwalsh/events{/privacy}", "received_events_url": "https://api.github.com/users/epwalsh/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTI1OTM2NzMz", "tag_name": "v1.0.0rc3", "target_commitish": "master", "name": "Version 1.0.0 Release Candidate 3", "draft": false, "prerelease": true, "created_at": "2020-04-27T20:10:08Z", "published_at": "2020-04-27T20:10:54Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v1.0.0rc3", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v1.0.0rc3", "body": "## Commits\r\n\r\n4720199f docs workflow quick fix\r\n24e11a87 add shortcuts for 'stable' and 'latest' to docs (#4138)\r\nb04d3171 hard-code version (#4142)\r\nc2e1be9c Fix broken link pointing to GitHub Actions (#4144)\r\nd709e59f modified the make_output_human_readable method in basic_classifier for allennlp-demo (#4038)\r\n6ea6c594 test logging errors work-around (#4139)\r\n4b8edc5e Some tokenizers don's have padding tokens (#4131)\r\n5e8b2ba5 Update torch requirement from <=1.4.0,>1.3.1 to >1.3.1,<1.6.0 (#4118)\r\n2a063d0e Remove unused Dockerfiles (#4137)\r\n1384a70c document release process (#4133)\r\nb3fcf600 Push 'latest' tag versions of Docker images (#4134)"}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/25828311", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/25828311/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/25828311/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v1.0.0.rc2", "id": 25828311, "author": {"login": "epwalsh", "id": 8812459, "node_id": "MDQ6VXNlcjg4MTI0NTk=", "avatar_url": "https://avatars.githubusercontent.com/u/8812459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/epwalsh", "html_url": "https://github.com/epwalsh", "followers_url": "https://api.github.com/users/epwalsh/followers", "following_url": "https://api.github.com/users/epwalsh/following{/other_user}", "gists_url": "https://api.github.com/users/epwalsh/gists{/gist_id}", "starred_url": "https://api.github.com/users/epwalsh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/epwalsh/subscriptions", "organizations_url": "https://api.github.com/users/epwalsh/orgs", "repos_url": "https://api.github.com/users/epwalsh/repos", "events_url": "https://api.github.com/users/epwalsh/events{/privacy}", "received_events_url": "https://api.github.com/users/epwalsh/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTI1ODI4MzEx", "tag_name": "v1.0.0.rc2", "target_commitish": "master", "name": "Version 1.0.0 Release Candidate 2", "draft": false, "prerelease": true, "created_at": "2020-04-23T21:35:59Z", "published_at": "2020-04-23T21:38:44Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v1.0.0.rc2", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v1.0.0.rc2", "body": "## Commits \r\n\r\n\r\n8f8288be Fix version suffix in release job and make deps caching more robust (#4130)\r\n66651d10 Bump pre-commit from 2.2.0 to 2.3.0 (#4127)\r\nfc824503 Fix typo in `InitializerApplicator` docstring (#4100)\r\n42b93872 Simplify GitHub Actions job graph (#4129)\r\n7b51c222 Fix cron schedule (#4128)\r\n583ea413 pypi release workflow HOT FIX\r\n5d589939 add nightly release schedule (#4123)\r\nf5a4f4ff Update README with new CI links / badge (#4116)\r\n8be8e854 Ensure all deps are up-to-date in CI (#4120)\r\n98a29533 Simplify pull request workflow (#4117)\r\n0056b9c1 fix docs output directory (#4113)\r\nb4c7d76c Adds release workflow, fixes logging test and docs workflow (#4112)\r\n0638cbb2 Fix param name in `GradientDescentTrainer` doc (#4091)\r\nece5c59d Bump mkdocs-material from 5.1.0 to 5.1.1 (#4103)\r\n78325af1 Update README.md (#4110)\r\n15e6b031 Fix docs workflow (#4107)\r\n10fdf274 More fixes to GitHub Actions (#4106)\r\nc9b7cbd2 Fixes for GitHub Actions (#4105)\r\n4eed5d42 Add custom 'logging.Logger' that has warn_once(), debug_once(), etc m\u2026 (#4062)\r\n24de9b17 Add main CI workflow to GH Actions (#4097)\r\n6cb944c4 Correctly computes the number of training steps (#4099)\r\nb4e10dd1 Make sure the nightly build doesn't produce bogus version numbers\r\n1beddcdf a few more doc fixes (#4078)\r\n51ef5b50 improve sorting_key error message (#4083)\r\n039522a4 Merge branch 'release-v1.0.0.rc1'\r\n3ead0540 Merge branch 'master' of https://github.com/allenai/allennlp\r\n3f5eabb2 Bump version numbers to v1.0.0.rc2-unreleased\r\n25ba3a26 Fix error message when top level config key missing (#4081)"}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/25592282", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/25592282/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/25592282/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v1.0.0.rc1", "id": 25592282, "author": {"login": "dirkgr", "id": 920638, "node_id": "MDQ6VXNlcjkyMDYzOA==", "avatar_url": "https://avatars.githubusercontent.com/u/920638?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dirkgr", "html_url": "https://github.com/dirkgr", "followers_url": "https://api.github.com/users/dirkgr/followers", "following_url": "https://api.github.com/users/dirkgr/following{/other_user}", "gists_url": "https://api.github.com/users/dirkgr/gists{/gist_id}", "starred_url": "https://api.github.com/users/dirkgr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dirkgr/subscriptions", "organizations_url": "https://api.github.com/users/dirkgr/orgs", "repos_url": "https://api.github.com/users/dirkgr/repos", "events_url": "https://api.github.com/users/dirkgr/events{/privacy}", "received_events_url": "https://api.github.com/users/dirkgr/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTI1NTkyMjgy", "tag_name": "v1.0.0.rc1", "target_commitish": "master", "name": "Version 1.0.0 Release Candidate 1", "draft": false, "prerelease": true, "created_at": "2020-04-16T22:30:12Z", "published_at": "2020-04-16T23:15:30Z", "assets": [{"url": "https://api.github.com/repos/allenai/allennlp/releases/assets/19896740", "id": 19896740, "node_id": "MDEyOlJlbGVhc2VBc3NldDE5ODk2NzQw", "name": "allennlp-1.0.0rc1-py3-none-any.whl", "label": null, "uploader": {"login": "dirkgr", "id": 920638, "node_id": "MDQ6VXNlcjkyMDYzOA==", "avatar_url": "https://avatars.githubusercontent.com/u/920638?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dirkgr", "html_url": "https://github.com/dirkgr", "followers_url": "https://api.github.com/users/dirkgr/followers", "following_url": "https://api.github.com/users/dirkgr/following{/other_user}", "gists_url": "https://api.github.com/users/dirkgr/gists{/gist_id}", "starred_url": "https://api.github.com/users/dirkgr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dirkgr/subscriptions", "organizations_url": "https://api.github.com/users/dirkgr/orgs", "repos_url": "https://api.github.com/users/dirkgr/repos", "events_url": "https://api.github.com/users/dirkgr/events{/privacy}", "received_events_url": "https://api.github.com/users/dirkgr/received_events", "type": "User", "site_admin": false}, "content_type": "application/octet-stream", "state": "uploaded", "size": 1467904, "download_count": 32, "created_at": "2020-04-16T23:15:16Z", "updated_at": "2020-04-16T23:15:24Z", "browser_download_url": "https://github.com/allenai/allennlp/releases/download/v1.0.0.rc1/allennlp-1.0.0rc1-py3-none-any.whl"}, {"url": "https://api.github.com/repos/allenai/allennlp/releases/assets/19896741", "id": 19896741, "node_id": "MDEyOlJlbGVhc2VBc3NldDE5ODk2NzQx", "name": "allennlp-1.0.0rc1.tar.gz", "label": null, "uploader": {"login": "dirkgr", "id": 920638, "node_id": "MDQ6VXNlcjkyMDYzOA==", "avatar_url": "https://avatars.githubusercontent.com/u/920638?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dirkgr", "html_url": "https://github.com/dirkgr", "followers_url": "https://api.github.com/users/dirkgr/followers", "following_url": "https://api.github.com/users/dirkgr/following{/other_user}", "gists_url": "https://api.github.com/users/dirkgr/gists{/gist_id}", "starred_url": "https://api.github.com/users/dirkgr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dirkgr/subscriptions", "organizations_url": "https://api.github.com/users/dirkgr/orgs", "repos_url": "https://api.github.com/users/dirkgr/repos", "events_url": "https://api.github.com/users/dirkgr/events{/privacy}", "received_events_url": "https://api.github.com/users/dirkgr/received_events", "type": "User", "site_admin": false}, "content_type": "application/x-gzip", "state": "uploaded", "size": 1395034, "download_count": 15, "created_at": "2020-04-16T23:15:24Z", "updated_at": "2020-04-16T23:15:26Z", "browser_download_url": "https://github.com/allenai/allennlp/releases/download/v1.0.0.rc1/allennlp-1.0.0rc1.tar.gz"}], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v1.0.0.rc1", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v1.0.0.rc1", "body": "### Breaking changes:\r\n* WordSplitter has been replaced by Tokenizer. See #3361. (this also requires a config file change; see below)\r\n* Models are now available at https://github.com/allenai/allennlp-models.\r\n* `Model.decode()` was renamed to `Model.make_output_human_readable()`\r\n* `trainer.cuda_device` no longer takes a list, use `distributed.cuda_devices`. See #3529.\r\n* TODO: As of #3529 dataset readers used in the distributed setting need to shard instances to separate processes internally. We should fix this before releasing a final version.\r\n* Dataset caching is now handled entirely with a parameter passed to the dataset reader, not with command-line arguments.  If you used the caching arguments to `allennlp train`, instead just add a `\"cache_directory\"` key to your dataset reader parameters.\r\n* Sorting keys in the bucket iterator have changed; now you just give a field name that you want to sort by, and that's it.  We also implemented a heuristic to figure out what the right sorting key is, so in almost all cases you can just remove the `sorting_keys` parameter from your config file and our code will do the right thing.  Some cases where our heuristic might get it wrong are if you have a `ListField[TextField]` and you want the size of the list to be the padding key, or if you have an `ArrayField` with a long but constant dimension that shouldn't be considered in sorting.\r\n* The argument order to `Embedding()` changed (because we made an argument optional and had to move it; pretty unfortunate, sorry).  It used to be `Embedding(num_embeddings, embedding_dim)`.  Now it's `Embedding(embedding_dim, num_embeddings)`.\r\n\r\n### Config file changes:\r\nIn 1.0, we made some significant simplifications to how `FromParams` works, so you basically never see it in your code and don't ever have to think about adding custom `from_params` methods.  We just follow argument annotations and have the config file always exactly match the argument names in the constructors that are called.\r\n\r\nThis meant that we needed to remove cases where we previously allowed mismatches between argument names and keys that show up in config files.  There are a number of places that were affected by this:\r\n\r\n* The way Vocabulary options are specified in config files has changed. See #3550. If you want to load a vocabulary from files, you should specify `\"type\": \"from_files\"`, and use the key `\"directory\"` instead of `\"directory_path\"`.\r\n* When instantiating a `BasicTextFieldEmbedder` `from_params`, you used to be able to have embedder names be top-level keys in the config file (e.g., `\"embedder\": {\"elmo\": ELMO_PARAMS, \"tokens\": TOKEN_PARAMS}`).  We changed this a long time ago to prefer wrapping them in a `\"token_embedders\"` key, and this is now required (e.g., `\"embedder\": {\"token_embedders\": {\"elmo\": ELMO_PARAMS, \"tokens\": TOKEN_PARAMS}}`).\r\n* The `TokenCharactersEncoder` now requires you to specify the `vocab_namespace` for the underlying embedder.  It used to default to `\"token_characters\"`, matching the `TokenCharactersIndexer` default, but making that work required some custom magic that wasn't worth the complexity.  So instead of `\"token_characters\": {\"type\": \"character_encoding\", \"embedding\": {\"embedding_dim\": 25}, \"encoder\": {...}}`, you need to change this to: `\"token_characters\": {\"type\": \"character_encoding\", \"embedding\": {\"embedding_dim\": 25, \"vocab_namespace\": \"token_characters\"}, \"encoder\": {...}}`\r\n* Regularization now needs another key in a config file.  Instead of specifying regularization as `\"regularizer\": [[regex1, regularizer_params], [regex2, regularizer_params]]`, it now must be specified as `\"regularizer\": {\"regexes\": [[regex1, regularizer_params], [regex2, regularizer_params]]}`.\r\n* Initialization was changed in a similar way to regularization.  Instead of specifying initialization as `\"initializer\": [[regex1, initializer_params], [regex2, initializer_params]]`, it now must be specified as `\"initializer\": {\"regexes\": [[regex1, initializer_params], [regex2, initializer_params]]}`.  Also, you used to be able to have `initializer_params` be `\"prevent\"`, to prevent initialization of matching parameters.  This is now done with a separate key passed to the initializer: `\"initializer\": {\"regexes\": [..], \"prevent_regexes\": [regex1, regex2]}.\r\n* `num_serialized_models_to_keep` and `keep_serialized_model_every_num_seconds` used to be able to be passed as top-level parameters to the `trainer`, but now they must always be passed to the `checkpointer` instead.  For example, if you had `\"trainer\": {\"num_serialized_models_to_keep\": 1}`, it now needs to be `\"trainer\": {\"checkpointer\": {\"num_serialized_models_to_keep\": 1}}`.\r\n* Tokenizer specification changed because of #3361.  Instead of something like `\"tokenizer\": {\"word_splitter\": {\"type\": \"spacy\"}}`, you now just do `\"tokenizer\": {\"type\": \"spacy\"}` (more technically: the `WordTokenizer` has now been removed, with the things we used to call `WordSplitters` now just moved up to be top-level `Tokenizers` themselves).\r\n* The `namespace_to_cache` argument to `ElmoTokenEmbedder` has been removed as a config file option.  You can still pass `vocab_to_cache` to the constructor of this class, but this functionality is no longer available from a config file.  If you used this and are really put out by this change, let us know, and we'll see what we can do.\r\n\r\n\r\n### Changes:\r\n* `allennlp make-vocab` and `allennlp dry-run` are deprecated, replaced with a `--dry-run` flag which can be passed to `allennlp train`. We did this so that people might find it easier to actually use the dry run features, as they don't require any config changes etc.\r\n* When constructing objects using our `FromParams` pipeline, we now inspect superclass constructors when the concrete class has a `**kwargs` argument, adding arguments from the superclass.  This means, e.g., that we can add parameters to the base `DatasetReader` class that are immediately accessible to all subclasses, without code changes.  All that this requires is that you take a `**kwargs` argument in your constructor, and you call `super().__init__(**kwargs)`.  See #3633.\r\n* The order of the `num_embeddings` and `embedding_dim` constructor arguments for `Embedding` has changed. Additionally, passing a `pretrained_file` now initializes the embedding regardless of whether it was constructed from a config file, or directly instantiated in code. see #3763  \r\n* Our default logging behavior is now much less verbose.\r\n\r\n### Notable bug fixes\r\nThere are far too many small fixes to be listed here, but these are some notable fixes:\r\n* NER interpretations have never actually reproduced the result in the original AllenNLP Interpret paper.  This version finds and fixes that problem, which was that the loss masking code to make the model compute correct gradients for a single span prediction was not checked in. See #3971.\r\n\r\n\r\n# Upgrading Guide\r\n\r\n\r\n## Iterators -> DataLoaders\r\nAllennlp now uses Pytorch's API for data iteration, rather than our own custom one. This means that `train_data`, `validation_data`, `iterator` and `validation_iterator` arguments to the `Trainer` are now deprecated, having been replaced with `data_loader` and `validation_dataloader`.\r\n\r\n*Previous config files which looked like:*\r\n```\r\n{\r\n  \"iterator\": {\r\n    \"type\": \"bucket\",\r\n    \"sorting_keys\": [[\"tokens\"], [\"num_tokens\"]],\r\n\t\"padding_noise\": 0.1\r\n    ...\r\n  }\r\n}\r\n```\r\n*Now become:*\r\n\r\n```\r\n{\r\n  \"data_loader\": {\r\n\t\"batch_sampler\" {\r\n\t\t\"type\": \"bucket\",\r\n\t\t// sorting keys are no longer required! They can be inferred automatically.\r\n\t\t\"padding_noise\": 0.1\r\n\t}\r\n}\r\n```\r\n\r\n\r\n\r\n## Multi-GPU\r\n\r\nAllennlp now uses `DistributedDataParallel` for parallel training, rather than `DataParallel`. With `DistributedDataParallel`, each worker (GPU) runs in it's own process. As such, each process also has its own `Trainer`, which now takes a single GPU ID only.\r\n\r\n*Previous config files which looked like:*\r\n```\r\n{\r\n  \"trainer\": {\r\n    \"cuda_device\": [0, 1, 2, 3],\r\n    \"num_epochs\": 20,\r\n    ...\r\n  }\r\n}\r\n```\r\n*Now become:*\r\n\r\n```\r\n{\r\n  \"distributed\": {\r\n    \"cuda_devices\": [0, 1, 2, 3],\r\n  },\r\n  \"trainer\": {\r\n    \"num_epochs\": 20,\r\n    ...\r\n  }\r\n}\r\n```\r\nIn addition, if it is important that your dataset is correctly sharded such that one epoch strictly corresponds to one pass over the data, your dataset reader should contain the following logic to read instances on a per-worker basis:\r\n\r\n```python\r\nrank = torch.distributed.get_rank()\r\nworld_size = torch.distributed.get_world_size()\r\nfor idx, inputs in enumerate(data_file):\r\n\tif idx % world_size == rank:\r\n\t\tyield self.text_to_instance(inputs)\r\n\t\r\n```"}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/20247077", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/20247077/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/20247077/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v0.9.0", "id": 20247077, "author": {"login": "matt-gardner", "id": 3291951, "node_id": "MDQ6VXNlcjMyOTE5NTE=", "avatar_url": "https://avatars.githubusercontent.com/u/3291951?v=4", "gravatar_id": "", "url": "https://api.github.com/users/matt-gardner", "html_url": "https://github.com/matt-gardner", "followers_url": "https://api.github.com/users/matt-gardner/followers", "following_url": "https://api.github.com/users/matt-gardner/following{/other_user}", "gists_url": "https://api.github.com/users/matt-gardner/gists{/gist_id}", "starred_url": "https://api.github.com/users/matt-gardner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/matt-gardner/subscriptions", "organizations_url": "https://api.github.com/users/matt-gardner/orgs", "repos_url": "https://api.github.com/users/matt-gardner/repos", "events_url": "https://api.github.com/users/matt-gardner/events{/privacy}", "received_events_url": "https://api.github.com/users/matt-gardner/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTIwMjQ3MDc3", "tag_name": "v0.9.0", "target_commitish": "master", "name": "v0.9.0", "draft": false, "prerelease": false, "created_at": "2019-09-25T13:47:34Z", "published_at": "2019-09-25T14:34:32Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v0.9.0", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v0.9.0", "body": "## Main features\r\n\r\n- [AllenNLP Interpret](https://allennlp.org/interpret).  This lets you interpret the predictions of any AllenNLP model, using gradient-based visualization and attack techniques.  You can (1) explore existing interpretations for models that we have implemented at [demo.allennlp.org](https://demo.allennlp.org); (2) easily add interpretations for your own model, either programmatically or in a live demo; and (3) easily add new interpretation methods that can be used with any AllenNLP model.\r\n- Compatibility with `pytorch-transformers`, so you can use RoBERTa or whatever else as your base encoder.\r\n\r\n## Also of note\r\n\r\n- A new, more flexible seq2seq abstraction is available (though, honestly, I think we all agree that fairseq or OpenNMT are better for seq2seq models still).\r\n- When specifying types for registrable items, you can now use a fully-qualified path, like `\"my_package.models.my_new_fancy_classifier\"`, instead of needing to pass `--include-package` everywhere.\r\n\r\n## Complete commit list\r\n\r\n052353ed (tag: v0.9.0) bump version number to v0.9.0\r\nff0d44a5 (origin/master, origin/HEAD) reversing NER for interpet UI (#3283)\r\n3b220112 Composed Sequence to Sequence Abstraction (#2913)\r\nb85f29c6 Fix F1Measure returning true positives, false positives, et al. only for the first class (#3279)\r\n64143c41 upgrade to latest pylint (#3266)\r\nd09042e4 Fix crash when hotflip gets OOV input (#3277)\r\n2a950223 Revert batching for input reduction (#3276)\r\n052e8d32 Reduce number of samples in smoothgrad (#3273)\r\n76d248f8 Reduce hotflip vocab size, batch input reduction beam search (#3270)\r\n9a67546e fix empty sequence bug (#3271)\r\n87fb294e Update question.md (#3267)\r\ndaed8359 Fix wrong partition to types in DROP evaluation (#3263)\r\n41a47767 Unidirectional LM doesn't return backward loss. (#3256)\r\n3e0bad4a Minor fixes for interpret code (#3260)\r\n05be16a8 allow implicit package imports (#3253)\r\n48de8667 Assorted fixes for run_with_beaker.py (#3248)\r\nc732cbf1 Add additive attention & unittest (#3238)\r\n07364c67 Make Instance in charge of when to re-index (#3239)\r\n7b50b69f Replace staticmethods with classmethods (#3229)\r\n7cfaab47 Add ERROR callback event (#2983)\r\nce504073 Revert \"Use an NVIDIA base image. (#3177)\" (#3222)\r\nb1caa9e5 Use an NVIDIA base image. (#3177)\r\n4625a9d2 Improve check_links.py CI script (#3141)\r\n5e2206df Add a reference to Joe Barrow's blog\r\n27ebcf6b added infer_type_and_cast flags (#3209)\r\nbbaf1fc0 Benchmark iterator, avoid redundant queue, remove managers. (#3119)\r\n78ee3d85 Targeted hotflip attacks and beam search for input reduction (#3206)\r\nf2824fdc Predictors for demo LMs, update for coref predictor (#3202)\r\nd78ac70a Language model classes for making predictions (both masked LM and next token LM) (#3201)\r\n8c06c4b2 Adding a LanguageModelHead abstraction (#3200)\r\n370d5126 Dataset readers for masked language modeling and next-token-language-modeling (#3147)\r\n1eaa1ff9 Link to Discourse in README\r\n030e28c9 Revert \"Revert \"Merge branch 'matt-gardner-transformer-embedder'\"\"\r\n6e1e3713 Revert \"Merge branch 'matt-gardner-transformer-embedder'\"\r\n4c7fa737 Merge branch 'matt-gardner-transformer-embedder'\r\n07bdc4ae Merge branch 'transformer-embedder' of https://github.com/matt-gardner/allennlp into matt-gardner-transformer-embedder\r\n993034f0 Minor fixes so PretrainedTransformerIndexer works with roberta (#3203)\r\n70e92e85 doc\r\ned93e523 pylint\r\n195bf0c3 override method\r\n6ec74aaf Added a TokenEmbedder for use with pytorch-transformers\r\nfb9a9718 code for mixed bert embedding layers (#3199)\r\n0e872a0d Clarify that scalar_mix_parameters takes unnormalized weights (#3198)\r\n23efadd9 upgrade to pytorch 1.2 (#3182)\r\n155a94e5 Add DropEmAndF1 metric to __init__.py (#3191)\r\n7738cb58 Add exist_ok parameter to registrable.register decorator. (#3190)\r\nce6dc726 Add example of initializing weights from pretrained model to doc (#3188)\r\n817814bf Update documentation for bert_pooler.py (#3181)\r\n112d8d0b Bump version numbers to v0.9.0-unreleased"}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/19444833", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/19444833/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/19444833/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v0.8.5", "id": 19444833, "author": {"login": "joelgrus", "id": 1308313, "node_id": "MDQ6VXNlcjEzMDgzMTM=", "avatar_url": "https://avatars.githubusercontent.com/u/1308313?v=4", "gravatar_id": "", "url": "https://api.github.com/users/joelgrus", "html_url": "https://github.com/joelgrus", "followers_url": "https://api.github.com/users/joelgrus/followers", "following_url": "https://api.github.com/users/joelgrus/following{/other_user}", "gists_url": "https://api.github.com/users/joelgrus/gists{/gist_id}", "starred_url": "https://api.github.com/users/joelgrus/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/joelgrus/subscriptions", "organizations_url": "https://api.github.com/users/joelgrus/orgs", "repos_url": "https://api.github.com/users/joelgrus/repos", "events_url": "https://api.github.com/users/joelgrus/events{/privacy}", "received_events_url": "https://api.github.com/users/joelgrus/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTE5NDQ0ODMz", "tag_name": "v0.8.5", "target_commitish": "master", "name": "v0.8.5", "draft": false, "prerelease": false, "created_at": "2019-08-21T17:17:34Z", "published_at": "2019-08-21T18:10:25Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v0.8.5", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v0.8.5", "body": "This is (almost certainly) the last release to support versions of PyTorch earlier than 1.2.0. PyTorch 1.2.0 introduces some breaking changes that require changes to the allennlp library (as well as some new features that will allow us to make allennlp better), and so you should expect that the next release will require `torch>=1.2.0`\r\n\r\nThis introduces some changes to the `CallbackTrainer`, which moves much of the ancillary training behavior into configurable Callbacks. Its API should still be considered somewhat experimental; in particular, we are open to feedback on its design decisions. There are no current plans to get rid of the classic `Trainer`, although it is likely to get more and more unwieldy as we add new training functionality to the library.\r\n\r\nThis also includes the \"srl_bert\" model as featured in the allennlp demo, as well as many other fixes.\r\n\r\nFull list of commits:\r\n\r\nc8d73270 (tag: v0.8.5) bump version number to v0.8.5\r\n9d8d36af quoref metric and evaluator (#3153)\r\n7bacfad5 Set pytorch-transformer to 1.1.0 (#3171)\r\n18daa298 Fix pearson correlation.py (#3101)\r\ne641543e Add a test for the subcommands docstring help outputs (#3172)\r\nb6af6ebf bug fix for default tokenization of knowledge graph entities (#3170)\r\n0f6b3b89 Make SrlBert model use SrlEvalMetric (#3168)\r\nadad1bc3 Switch SemanticRoleLabeler metric to SrlEvalScorer. (#3164)\r\n8fffd831 Add missing train command cache options (#3160)\r\n770791a2 switch for DataIterators whether smaller batches should be skipped (#3140)\r\n111db19d Create method to save instances to cache file. (#3131)\r\ndac486ef Fixing NaN warning with single element tensors (#3158)\r\n6bbf82ef Move matplotlib import into function (#3157)\r\n3ef43c96 Upgrade minimum spacy to 2.1.0 (#3152)\r\n1d6e1667 Fixing Conda download and install link on Readme (#3151)\r\nf111d8a7 Pretrained transformer indexer (#3146)\r\nfa1ff674 Add support for running preemptible workloads on beaker (#3143)\r\n0bd33194 Add regularization parameter to Models (#3120)\r\nbf968c69 add keep_as_dict option to Params.pop, use in Vocab and automat\u2026 (#3075)\r\n217022f4 Adding a PretrainedTransformerTokenizer (#3145)\r\nf9e2029f Update HTTP links to HTTPS where possible (#3142)\r\n9093f475 Add dropout option for BERT Pooler (#3109)\r\n23a089c8 pin pytorch away from 1.2 until we fix the tests (#3128)\r\n0be4b488 fix UpdateMovingAverage.from_params (#3126)\r\n9db00422 Upgrade conllu from 0.11 to 1.3.1 (#3115)\r\n6746d12c pass cache_directory and cache_prefix to non-default trainers (#3077)\r\n031bbf95 allenNLP broken link (#3086)\r\n0caf3640 Adding cached_path to input file of the predictor (#3098)\r\n417a7572 Adding ability to choose validation DatasetReader with \"predict\" (#3033)\r\n30c4271f Close tensorboard's event files properly at the end of the training (#3085)\r\n428c1511 fix MetadataField.batch_tensors (#3084)\r\n88a61e13 [Embedding] Forward given padding_index param to embedding() (#2504)\r\n9ed9e2c8 remove executable permission for submods (#3080)\r\na1476c04 add equality check for index field; allennlp interpret (#3073)\r\n5014d022 remove deprecated function call in hotflip (#3074)\r\n014fe318 Improve dict missing key code (#3071)\r\n9166c184 AllenNLP Interpret Basic Version (#3032)\r\n7728b12f Minor WTQ ERM model and  dataset reader fixes for demo (#3068)\r\nec30c902 remove dropout from test fixtures (#2889)\r\n1cd21937 Revert \"Lr scheduler bug\" (#3065)\r\n083f3430 Lr scheduler bug (#2905)\r\n5c64f9d0 warn about truncation only once (#3052)\r\nebe91139 Add options for focal loss (#3036)\r\nc22ed57a Spacy token indexer (#3040)\r\na33436db Multilabel bug (#3021)\r\ndd3476f1 simplify callback trainer (#3029)\r\n0663e0bb Fixes to ERM decoding script (#3041)\r\n64d16acf fix shape comments (#3025)\r\n354a19b4 Update documents to sentence_splitter.py (#3023)\r\n427996d4 Fix forward in EndpointSpanExtractor (#3042)\r\n38a10735 Update README.md\r\n715422c7 Fix error in BooleanAccuracy when total count is 0 (#2991)\r\n9e52e0f6 Remove separate start type prediction in state machines (#3030)\r\nc2c4b64d Remove awscli from dependencies (#3024)\r\nae72049f Make per-batch logging quieter. (#3020)\r\n57870f5a removing unnecessary data iteration (#3027)\r\ne71618de Allow option to only reset some states in _EncoderBase (#2967)\r\n15a9cbe5 PassThroughIterator (#3015)\r\n70fa4aac Typo in initializers.py (#3016)\r\nc85dcfcf fix behavior when num_serialized_models_to_keep is 0 (#2880)\r\n2cce412e fix type in vocab config (#2977)\r\n7943b2fb Expose the spacy language model for the word splitter in the se\u2026 (#3008)\r\n9b929b28 Fixes inconsistent resetting of metrics with Validate and TrackMetrics callbacks  (issue #3001) (#3002)\r\n0a267397 Modified ActionSpaceWalker to use DomainLanguage (#3006)\r\n9a13ab57 (vikigenius/master) do not evaluate after training if non-default trainer (#2997)\r\n30ffaa54 Removed target_token_indexers documentation (#2990)\r\n7b465c69 Clarify docs for CosineWithRestarts (#2953)\r\nfb878721 Add never_split signature (#2463)\r\nb6a2abbf correct the wrong parameter note(target_namespace) (#2987)\r\ncf247c6a Add model parameters / modules inspection helper. (#2466)\r\n0fbd1ca8 WTQ dataset reader and models moved from iterative-search-semparse (#2764)\r\n7e08298e Fix wordpiece indexer truncation (#2931)\r\n03aa838e fix incorrect logging in viterbi decode (#2982)\r\n51b74b1a use starting offsets in the srl model so output is wellformed (#2972)\r\nbcd10700 Add cuda_device to Predictor.from_path (#2974)\r\n459633c9 rename callbacks (#2966)\r\n0f1bc3b9 DeprecationWarning removed for op-level token_embedders (#2955)\r\na655ad52 experimental: add backoff (#2968)\r\n2a59be3a (epwalsh/master) Change registered names of scheduler callbacks (#2964)\r\n2a884501 update sphinx version (#2959)\r\n165b282b Bert srl model (#2961)\r\n3dc99a77 Add missing param in CallbackTrainer.__init__ docstring (#2960)\r\n9e6e0af6 callback based trainer (#2817)\r\n1b656dd7 Allowing for bulk adding of tokens to vocab (#2948)\r\nc9eb2d0e Replace current default stopword list with spaCy's. (#2940)\r\n6a3d3a8d ensure regularizers are only computed for parameters requiring gradients (#2887)\r\nacfbb8c0 Replace s3 path style to virtual host style (#2873)\r\nac72c888 Fix Model.load fail if model_params is str (#2805)\r\ne7b00130 Linear assignment depricated fix (#2950)\r\n01602c86 Link to Wikitables and ATIS data. (#2947)\r\neaebe02f Update issue templates to request full stacktrace (#2876)\r\nda16ad13 Multilingual parser and Cross-lingual ELMo (#2628)\r\n92ee4215 Fix for cyclic import problems (issue #2935) (#2938)\r\n5e3c4cdd token_type_ids fix for window reshaping (#2942)\r\n44ba4900 Use unsigned s3 requests when missing credentials (#2939)\r\nc0f44f74 Fixed minor error while calculating span accuracy (#2923)\r\n8e180cb9 Pad coreference model input to 5 (#2933)\r\n9a0e01fa fix trainable and requires_grad kwargs (#2932)\r\n5f377834 Bert srl (#2854)\r\n5b2066bd Fix Invalid Index Reference for labels in Vocabulary (#2926)\r\nc629093b CopyNet: replace in-place tensor operation with out-of-place equivalent (#2925)\r\n89700de2 Change image to docker_image (#2918)\r\n53938826 Change blueprint to image in run_with_beaker (#2903)\r\n6afba9a2 Fix TextField padding when there are no tokens (#2843)\r\n954a02f9 (vikigenius/release-v0.8.4, upstream/release-v0.8.4) Bump version numbers to v0.8.5-unreleased\r\n"}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/17692644", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/17692644/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/17692644/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v0.8.4", "id": 17692644, "author": {"login": "schmmd", "id": 954798, "node_id": "MDQ6VXNlcjk1NDc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/954798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/schmmd", "html_url": "https://github.com/schmmd", "followers_url": "https://api.github.com/users/schmmd/followers", "following_url": "https://api.github.com/users/schmmd/following{/other_user}", "gists_url": "https://api.github.com/users/schmmd/gists{/gist_id}", "starred_url": "https://api.github.com/users/schmmd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/schmmd/subscriptions", "organizations_url": "https://api.github.com/users/schmmd/orgs", "repos_url": "https://api.github.com/users/schmmd/repos", "events_url": "https://api.github.com/users/schmmd/events{/privacy}", "received_events_url": "https://api.github.com/users/schmmd/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTE3NjkyNjQ0", "tag_name": "v0.8.4", "target_commitish": "master", "name": "v0.8.4", "draft": false, "prerelease": false, "created_at": "2019-05-30T19:52:44Z", "published_at": "2019-05-30T22:14:32Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v0.8.4", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v0.8.4", "body": "This is a minor release to distribute the numerous small improvements and bugfixes to our users.\r\n\r\n# List of Commits\r\n\r\n030ef755 bump version number to v0.8.4\r\nfbc28cef Fix typo in Model.extend_embedder_vocab docstring (#2806)\r\n26ac3499 fix dependency conflicts (by removing moto + disabled tests that use it) (#2902)\r\n69c1c9fc Shift extra_weight for embedding extension to right device (#2896)\r\nd6bb6c87 Make DROP EM and F1 calculation length aware (#2866)\r\nd800e823 truncate type ids (#2875)\r\nc2609453 updated links for pretrained NER models (#2872)\r\n20844259 Add ability to generate BERT embeddings using a sliding window (#2493) (#2537)\r\n093847a7 updated fine-grained-ner-model-elmo URL (#2868)\r\n93e86a96 Add output path to drop eval (#2860)\r\nb6693107 Log random seeds to disk. (#2859)\r\nb20c12ba Use optimal matching for the drop eval and add appropriate tests. (#2853)\r\nc59a3a8b Update auc.py (#2775)\r\n7bdd575c Remove multiple GPU warning (#2837)\r\n8e0953ab Fix typo in tagger README file (#2836)\r\ne3febb99 Fix test evaluation after training in the general case (#2835)\r\naa1524ee Fix typo in a comment (#2826)\r\n7c4311b1 Unwrap masks to tensors as well in AUC metric (#2813)\r\n2452492f fix batched_index_select (#2765)\r\n425f2af3 add `token_min_padding_length` to `TokenIndexer` (#2615)\r\ne70eb7bb Link pretrained model in tutorial. (#2814)\r\n48f46ea9 Number predictions. (#2709)\r\naeafe1c5 Bert for classification (#2787)\r\n63be47a2 Improve handling of empty ListFields. (#2697)\r\n30460e61 Use a hash function that is constant across program invocations. (#2802)\r\n16c40ec5 Delete .DS_Store (#2803)\r\n24f90a82 Adding NamedTuple case for Tensors to be moved to GPU. (#2799)\r\na701a0ae Fix out-of-bound checking in BidirectionalEndpointSpanExtractor for empty sequences. (#2763)\r\n9fc6f9eb Add SrlEvalMetric (#2743)\r\n42ada05d fixing regression in drop script and adding a test case for the same (#2796)\r\nd9baa6f7 Changing model name in config file to match registered name (#2794)\r\n6acfc6ca Repro for #2776 and backwards compatible fix. (#2784)\r\n648a36f7 Increase max_runs for NumericallyAugmentedQaNetTest. (#2778)\r\n3170f6f3 Make FBetaMeasure work with batch size of 1. (#2777)\r\nb97bca02 Make _replace_none properly handle \"None\" in parameter lists (#2774)\r\n7c0d5d53 Fix bug in QaNetEncoder, update pretrained model (#2773)\r\n95b439da Remove misleading dropout from training configs where RNN has only one layer (#2768)\r\nc059b151 Remove misleading dropout from bidaf.jsonnet (#2767)\r\n3c226042 Extras checks (#2754)\r\nc6ddb9d3 Update comment on ELMo NER model to match current configuration (#2761)\r\n979817dd add print-results command (#2744)\r\n282f12ae fix abstract type hint (#2753)\r\n90839d0b Dummy commit to test Google Cloud Build removal.\r\nb6453dc1 Directly capture output in EVALB metric (#2755)\r\n20772152 Use numpy's shuffle for reproducibility. (#2729)\r\n53b3d37b Text Classification Predictor (#2745)\r\n12cee922 lowercase stopword set in word filter (#2749)\r\nf8d2ec5b Moving WTQ language updates from iterative-search-semparse (#2637)\r\nb9072218 Add caching functionality to the base DatasetReader class (#2726)\r\n75f7992c Document interaction between UNK token and max_vocab_size (#2740)\r\n7e2e592d Allow FromParams to construct Union types (#2734)\r\n016ea34e Fixing typo in ElmoTokenEmbedder documentation (#2741)\r\n57956e78 Enable span-level F1 metric in SimpleTagger (#2735)\r\n00e95b8b Added NAQANet to pretrained.py and DEFAULT_PREDICTORS (#2731)\r\n9dec0202 [Feature Enhanced] Generalize F1-measure to FBeta-measure (#2275)\r\ne2d153f8 Add perplexity metric to LanguageModel (#2548)\r\n4422d53d QaNetEncoder Multi-GPU (#2692)\r\n4c98095b Fix example in RegularizerApplicator.from_params docstring (#2733)\r\ndeae1db2 Add coref_resolved method to CorefPredictor (#2296)\r\n7d34ca3b Fix to include day count in training duration for metrics.json  (#2718)\r\n58f386c5 clarify that seq2vecencoder is required for basic classifier (#2712)\r\n74634e34 Add missing docstring for Embedding.pretrained_file (#2710)\r\n08d66bf2 Fix typo in output parameter name in docstring (#2708)\r\n79e2cf7b Fix ArrayField.to_tensor's dtype when it's a scalar (#2676)\r\n48294831 Fix tagger tutorial (#2705)\r\na9d957b7 Update pylint to 1.9.4 (#2698)\r\n73bc049b Adding NAQANET training config and fixing test format in evaluation (#2695)\r\ne5adfd73 Ignore 2 root node types in PTB parsing reader (#2675)\r\n53a46ab7 call parse_cuda_device inside check_for_gpu (#2646)\r\n1bca7f60 Correct The Pearson Correlation Formula In Docs (#2683)\r\n0f417503 Catch some references to old library versions. (#2672)\r\nefdc9f3c unpin msgpack, pin spacy to > 2.0.18 (#2671)"}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/16129415", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/16129415/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/16129415/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v0.8.3", "id": 16129415, "author": {"login": "joelgrus", "id": 1308313, "node_id": "MDQ6VXNlcjEzMDgzMTM=", "avatar_url": "https://avatars.githubusercontent.com/u/1308313?v=4", "gravatar_id": "", "url": "https://api.github.com/users/joelgrus", "html_url": "https://github.com/joelgrus", "followers_url": "https://api.github.com/users/joelgrus/followers", "following_url": "https://api.github.com/users/joelgrus/following{/other_user}", "gists_url": "https://api.github.com/users/joelgrus/gists{/gist_id}", "starred_url": "https://api.github.com/users/joelgrus/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/joelgrus/subscriptions", "organizations_url": "https://api.github.com/users/joelgrus/orgs", "repos_url": "https://api.github.com/users/joelgrus/repos", "events_url": "https://api.github.com/users/joelgrus/events{/privacy}", "received_events_url": "https://api.github.com/users/joelgrus/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTE2MTI5NDE1", "tag_name": "v0.8.3", "target_commitish": "master", "name": "allennlp 0.8.3", "draft": false, "prerelease": false, "created_at": "2019-03-29T20:31:43Z", "published_at": "2019-03-29T23:44:27Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v0.8.3", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v0.8.3", "body": "# Highlighted Changes\r\n\r\n* This release includes code for working with the DROP dataset, including the official evaluation script, a `DatasetReader`, and the `NAQANet` model. (#2559, #2556 and #2560)\r\n* We added a no-op trainer that allows you to create AllenNLP model archives for programmatic baselines, alternatively trained models, etc. (#2610)\r\n\r\n# Breaking Changes\r\n\r\n1. In #2607 we changed the (default) SpacyWordSplitter to return allennlp `Token`s (which are compact, efficient `NamedTuple`s) rather than `spacy.tokens.Token`s. This was done primarily to decrease memory usage for large datasets; secondarily to play nicer with the multiprocess dataset reader. \r\n\r\nThis is a breaking change in two ways, neither of which should affect most users:\r\n\r\n* in theory everyone should be programming to the `Token` abstraction that's shared between both implementations, but it's possible that someone could be relying on having the actual spacy token, in which case they would need need to configure their word splitter with `keep_spacy_tokens=True`.\r\n\r\n* a NamedTuple can't have different constructor parameters and field names. Our previous Token implementation used e.g. pos as the name of the constructor argument but then pos_ as the name of the field. Converting this to a namedtuple meant that the constructor argument now also has to be pos_. If you were for some reason generating your own tokens manually (which the wikitables dataset reader was doing) you would need to make the corresponding changes to that code; if you were only creating Tokens using our `Tokenizer`s, then there's no difference to you.\r\n\r\nIt's quite likely that neither of these changes will affect even a single user, but in theory they *could*.\r\n\r\n# List of Commits\r\n\r\nbaef9537 bump version number to v0.8.3\r\n0abefe25 Fix docstrings after inspection (#2655)\r\na80aac7e Move register to typical location. (#2662)\r\ne1d70bb7 Add missing paren (#2661)\r\n2bf0779f Fixed ELMO command's lack of encoding specification when reading from\u2026 (#2614)\r\ne138d6cb TextCat Reader skip_label_indexing Fix (#2653)\r\n6e1ee2e1 config_allennlp.py uses open(file_path) where file_path is a URL (#2654)\r\n263d3401 Upgrade Dockerfile to stretch. (#2647)\r\nfab4b15e Fix quarel explanations (#2648)\r\n37a078af make things backward compatible with spacy 2.0 (#2644)\r\ne79b713d add dependency parser config (#2639)\r\n305bd35c final_state_tuple is a Tuple (#2645)\r\na4a43066 Checkpointer should check in case user deleted a serialized model (#2531)\r\n3c889cae Update outdated doc (#2641)\r\n12626ac4 fix sampled softmax tests (#2061)\r\nc06e9045 add option \"-k\" to limit tests in test-install command (#2635)\r\n1357c7e8 Remove reference to install_requirements.sh from the README (#2633)\r\n0bbd359f Add workaround for missing linking in spacy 2.1, remove install_requirements.sh (#2632)\r\n1b07b481 Bump up spacy version pin to 2.1 (#2626)\r\ne3038a3f bug fixes in drop evaluation and more test cases (#2594)\r\nf8b10a94 Add a no-op trainer. (#2610)\r\n9e72ee03 Fix TextClassificationJsonReader handling of unlabeled instances (#2621)\r\n01065364 Add text classification model (#2591)\r\n43b384d4 Move some scripts to `allennlp/allennlp/tools` (#2584)\r\nfe80f9fb Fix 'cuda_device' docstring in Trainer.__init__ (#2613)\r\nf19c0ee2 Enable Pruner class to keep different number of items for different entries in minibatch. (#2511)\r\n3cdb7e21 Ensure contiguous initial state tensors in `_EncoderBase(stateful=True)` (#2451)\r\nca998b26 Feature Request: Add a `dtype` parameter to `ArrayField` (#2609)\r\nff908456 change pins to bounds (#2490)\r\n0fffb9b0 Allow the transition from M to M in the BMES constraint type (#2611)\r\n0542c5a6 make spacy word splitter return allennlp Tokens (now NamedTuples) by default (#2607)\r\n9e3f4052 Only log the keys in the \"extras\" dictionary when instantiating objects from_params (#2608)\r\n720d306b Handle edge cases in beam search (#2557)\r\n79936e5e Re-use .allennlp when running Docker commands (#2593)\r\n55458f51 fix bugs in naqanet (#2604)\r\nc163b638 Fixed memory error in `make_vocab` on big dataset. (#2606)\r\nb61d511d context manager that allows predictor to capture model internals (#2581)\r\n3e0fcf0b Update README.md (#2601)\r\n18312a0c Seq2seq dataset reader improvements (#2599)\r\n1adb3e8a Interactive beam search (#2513)\r\n0f7bcf52 Add support for overriding list elements (#2585)\r\n9437b614 disable tutorial test (#2580)\r\n6ea273ef Allow checkpointer to be initialized from params (#2491)\r\nb0ea7ab6 Make tutorial use GPU if available. (#2570)\r\n41174daa Fix unit test to work with GPUs. (#2574)\r\n32defc3b fix a bug in augmented_lstm.py (#2534)\r\ncdbac6db Fix min padding length in pretrained NER predictors (#2541)\r\nd0f7170b Make `load_archive` operate on serialization directories. (#2554)\r\n31af01e0 Add missing requirement to setup.py (#2564)\r\nc54fcc62 Add NAQANet model for DROP (#2560)\r\n97f3578b add initializer to copynet (#2558)\r\nbbb67e9f Add dataset reader for DROP (#2556)\r\n4d5eadee Add official DROP evaluation script (#2559)\r\n3d5560fa missing `=overrides` argument when instantiate Params despite a second time (#2553)\r\n64a8e130 Scope DeprecationWarning errors to just allennlp-internal stuff (#2549)\r\n321cf915 Clarify `data_parallel` implementation. (#2488)\r\n540ebac7 Propose a deprecation policy. (#2424)\r\n6d8da973 make archival take an optional output path (#2510)\r\nfefc4390 Restore tensorboard epoch metrics to pre-refactoring behavior (#2532)\r\n0205c26f Bump version numbers to v0.8.3-unreleased\r\n"}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/15647280", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/15647280/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/15647280/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v0.8.2", "id": 15647280, "author": {"login": "schmmd", "id": 954798, "node_id": "MDQ6VXNlcjk1NDc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/954798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/schmmd", "html_url": "https://github.com/schmmd", "followers_url": "https://api.github.com/users/schmmd/followers", "following_url": "https://api.github.com/users/schmmd/following{/other_user}", "gists_url": "https://api.github.com/users/schmmd/gists{/gist_id}", "starred_url": "https://api.github.com/users/schmmd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/schmmd/subscriptions", "organizations_url": "https://api.github.com/users/schmmd/orgs", "repos_url": "https://api.github.com/users/schmmd/repos", "events_url": "https://api.github.com/users/schmmd/events{/privacy}", "received_events_url": "https://api.github.com/users/schmmd/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTE1NjQ3Mjgw", "tag_name": "v0.8.2", "target_commitish": "master", "name": "", "draft": false, "prerelease": false, "created_at": "2019-02-19T15:41:09Z", "published_at": "2019-02-19T16:59:36Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v0.8.2", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v0.8.2", "body": "# List of Commits\r\n\r\n4bac9b55 bump version number to v0.8.2\r\n3a5373fd upgrade huggingface pretrained bert (#2524)\r\n2f2b481c Adding bag_of_embeddings as an alternate name for the boe encoder (Issue #2268) (#2521)\r\n9f87aa56 fix typo (#2508)\r\n751d2e1e Remove extended_vocab argument from extend_embedder_vocab API. (#2505)\r\ncf6eff22 Allow vocab+embedding extension in evaluate command. (#2501)\r\n39413f22 Add necessary implicit embedding extension for transfer-modules api and vocab extension (#2431)\r\n47feb35e remove notebooks and juptyer dependency (#2499)\r\n56c11e53 Revert \"Less restrictive token characters indexer (#2494)\" (#2497)\r\n89056f1e Less restrictive token characters indexer (#2494)\r\n43dc4cb4 upgrade pytorch-pretrained-bert (#2495)\r\nb9a40036 Add momentum schedulers to trainer (#2469)\r\n19d0f592 Add support to transfer submodules, and in different modes. (#2471)\r\nce83cb41 Variational dropout in Augmented LSTM (#2344)\r\n2e7acb05 Mark QaNet test as flaky (#2481)\r\n23ea5908 Add support for pretrained embedding extension in fine-tuning. (#2395)\r\ndbd70851 WIP: Make warnings errors and filter library warnings from pytest (#2479)\r\n234fb18f make bag_of_word_counts token embedder ignore padding and UNK tokens (#2432)\r\ne5a74abc [Feature Enhanced] Support sentence pair in BERT (#2279)\r\n6f5f5657 Fix CORS error for react in config explorer (#2476)\r\n08a8c5e5 Add QaNet model (#2446)\r\ne4174868 Fixes unnecessary parameter clone in moving average. (#2468)\r\n9a2f1982 Add support to kwargs in TimeDistributed (#2439)\r\n9a5ea1a2 Ten times faster than before in GPU get_best_span of bidaf (#2465)\r\n84eb4d1a More help info for `archive_surgery.py` (#2327)\r\na8e0f15f Make `mask` in `PassThroughEncoder` work (#2448)\r\n2b52492e Fix path issue for certain cases when using --include-package (#2464)\r\n35f7f169 Update `Running AllenNLP` in README.md (#2447)\r\ndddcbf24 Cleanup global logging after training (#2458)\r\n5560466e [Feature Enhanced]Add FeedforwardEncoder for Sequences (#2444)\r\n5ff923c5 Generalize LR scheduler (#2345)\r\n508cb730 Instantiate the class `Activation` properly when testing FeedForward. (#2443)\r\ndc901a6a rename test case (#2441)\r\n00c877b9 Fix edgecase of potentially missing best-val-metrics in metrics.json on training recovery (#2352)\r\n4465a6e4 Support exponential moving average in the default trainer. (#2406)\r\nb6cc9d39 Optimizing the memory usage in `multi_head_self_attention` and `masked_softmax` (#2405)\r\n90f39f91 Mark Atis Parser tests as flaky. Fix pylint. (#2430)\r\n8b07c42a Improve error message for \"undocumented modules\" (#2427)\r\n585c19e4 Fixing BERT mask size (#2429)\r\nb7d56ae0 Fix typo when loading train state (#2425)\r\ncb9651a4 Add AUC metric. (#2358)\r\n9b01d4ca fix params duplicate bug (#2421)\r\nc1aace7e Get rid of hardcoded Vocabulary class name. (#2418)\r\n55b9bd08 serialization in the tutorial (#2412)\r\nd6e3140f Replace scripts with entry_points.console_scripts (#2232)\r\n3baec620 Remove outdated reference to custom extensions (#2401)\r\n012e42dd Ensure vocab is popped off params (#2409)\r\na9b34750 Fix docstring for PretrainedBertIndexer's pretrained_model param (#2399)\r\n11d83278 Fix typing for python 3.7 (#2393)\r\n7525c610 Remove scattering for multi-GPU training. (#2200)\r\nd0a5a40a Remove unnecessary line (#2385)\r\n9719b5c7 Allow embedding extension to load from pre-trained embeddings file. (#2387)\r\n174f5395 enable pickling for vocabulary (#2391)\r\n2d29736b Add support for extending Embeddings/Embedders with Extended Vocabulary. (#2374)\r\n7cc8ba04 Rollback PR #2308 \"Check train/dev/test-file-path\" (#2386)\r\n4fe8fa0d move model to cuda in tests, add comment (#2384)\r\n8eb2d75c Text classification JSON dataset reader (#2366)\r\ne9b0acac Make DomainLanguage handle executing action sequences directly, with side arguments (#2375)\r\n122a21a5 changed log-to-console flags (#2381)\r\n385e66e8 pieces for multitask learning (#2369)\r\nae63a2ae New WikiTables language (#2350)\r\nfc91f3e6 Sparse Gradient Clipping (#2312)\r\n7da19bc8 Disable requires_grad for Transformer ELMo parser. (#2336)\r\ne9287d4d Bag of words token embedder (#2365)\r\ne75b19b8 Use f-string (#2370)\r\ne2af6b44 Install package in dockerfile (#2305)\r\n9649f3a2 fix a typo (#2367)\r\ne08ade81 trainer refactor (#2304)\r\nbe57ecc3 Add snippet for using `LanguageModelTokenEmbedder`. (#2359)\r\nabc10ed9 Update create_a_configuration.md howto by removing old allennlp configure (#2330)\r\n083f49ea Bump up pytorch-pretrained-bert to v0.4.0 (#2349)\r\naa8ed3b7 Added StackedBiDirectional to list of encoders (#2339)\r\n51ac8142 Add optional additional tokens to ELMo character indexer (#2364)\r\n6bd59751 sanitize environment variables that can't be unicode encoded (#2357)\r\n0f6796aa Fix very minor docstring typo in elmo.py (#2361)\r\n0409371e New NLVR language (#2319)\r\n632b14ce Warn default value of min_padding_length (#2309)\r\n4c5de577 adjust call to lisp_to_nested_expression (#2347)\r\n43ea97cc QuaRel Domain Language (#2321)\r\n23687601 Remove pylint ignores for backslashes. (#2331)\r\ne6ad6e9a Evaluate on a token-weighted basis. (#2183)\r\n71ebcd84 add infer_and_cast (#2324)\r\n059b0572 remove custom extensions (#2332)\r\n5f9fb419 Stacked Bi-Directional LSTM to be compatible with the Seq2Vec wrapper (#2318)\r\n93250f05 Skip Custom Highway LSTM tests, since they're broken (#2307)\r\n7ecf772d Fix backslash exceptions. (#2322)\r\n5bce1d54 Display default values in help message for allennlp command (#2323)\r\n36d74006 Added links for some tutorials and organized the How-to alphabetically. (#2315)\r\nf1600591 Add raw prefix to avoid warnings from regex (#2310)\r\n8a7f808b Check train/dev/test-file-path before process (#2308)\r\n7d3b130e Masked Flip (#2299)\r\nb0e29566 Add CopyNet seq2seq model (#2237)\r\na4670adb Grammar induction from a python executor (#2281)\r\n088f0bb6 Turn BidirectionalLM into a more-general LanguageModel class (#2264)\r\nf76dc70b Bump version numbers to v0.8.2-unreleased\r\n"}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/14844804", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/14844804/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/14844804/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v0.8.1", "id": 14844804, "author": {"login": "schmmd", "id": 954798, "node_id": "MDQ6VXNlcjk1NDc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/954798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/schmmd", "html_url": "https://github.com/schmmd", "followers_url": "https://api.github.com/users/schmmd/followers", "following_url": "https://api.github.com/users/schmmd/following{/other_user}", "gists_url": "https://api.github.com/users/schmmd/gists{/gist_id}", "starred_url": "https://api.github.com/users/schmmd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/schmmd/subscriptions", "organizations_url": "https://api.github.com/users/schmmd/orgs", "repos_url": "https://api.github.com/users/schmmd/repos", "events_url": "https://api.github.com/users/schmmd/events{/privacy}", "received_events_url": "https://api.github.com/users/schmmd/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTE0ODQ0ODA0", "tag_name": "v0.8.1", "target_commitish": "master", "name": "v0.8.1", "draft": false, "prerelease": false, "created_at": "2019-01-07T23:40:17Z", "published_at": "2019-01-08T17:29:13Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v0.8.1", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v0.8.1", "body": "# Highlighted changes\r\n\r\n* We now include a script to easily update pre-existing model archives (#2223).\r\n* We fixed an issue that caused issues using AllenNLP from within iPython (#2257).\r\n\r\n# List of Commits\r\n\r\nb0191969 Don't deprecate bidirectional-language-model name (#2297)\r\n01913fb4 Update the base image in the Dockerfiles. (#2298)\r\n1bec3f96 Add __repr__ to Vocabulary (#2293)\r\n73f0e5b9 Update the `find-lr` subcommand help text. (#2289)\r\ne13aae41 fix #2285 (#2286)\r\n2f662cf4 Fix spelling in tutorial README (#2283)\r\ne6b0f213 script for doing archive surgery (#2223)\r\n259ce322 Add a Contributions section to README.md (#2277)\r\ne394b7a9 Fix type annotation for .forward(...) in tutorial (#2122)\r\ncf6a7d7d Fix bug in uniform_unit_scaling #2239 (#2273)\r\n511c846c Deprecate bidirectional-language-model for bidirectional_language_model (#2253)\r\nb5141b23 fix doc (#2213)\r\n0d54a7a3 Add a quick README for training transformer ELMo. (#2231)\r\n07d193ee clarify the \"num_output_representations\" more clear (#2256)\r\nee02ed0f Fix multiGPU peak gpu memory test (#2254)\r\nfb587833 Check for existing vocabulary before creating a new one (#2240) (#2261)\r\n3cff7d35 Remove deprecated SpanPruner (#2248)\r\na98481c8 Remove deprecated batch_average argument to sequence ce with logits (#2247)\r\ncf671280 Remove EpochTrackingBucketIterator (#2249)\r\n2f56765b fix get_output_dim() for ElmoTokenEmbedder (#2234)\r\n42e0815e Fix circular dependency. (#2257)\r\nb5b9b401 add some docstrings about parameter `force` (#2226)\r\ne53b4f45 Allow loading model from path with ~ HOME (#2215)\r\n3f0953d1 change mislabeled variable in description (#2242)\r\n1938a5af fix completely masked case in BooleanAccuracy (#2230)\r\nfe626580 Corrected documentation (#2229)\r\ne2f66c0d Simplifies Subsequent Mask  (#2224)\r\nce060bad Bidirectional LM Embedder (#2138)"}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/14622468", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/14622468/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/14622468/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v0.8.0", "id": 14622468, "author": {"login": "schmmd", "id": 954798, "node_id": "MDQ6VXNlcjk1NDc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/954798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/schmmd", "html_url": "https://github.com/schmmd", "followers_url": "https://api.github.com/users/schmmd/followers", "following_url": "https://api.github.com/users/schmmd/following{/other_user}", "gists_url": "https://api.github.com/users/schmmd/gists{/gist_id}", "starred_url": "https://api.github.com/users/schmmd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/schmmd/subscriptions", "organizations_url": "https://api.github.com/users/schmmd/orgs", "repos_url": "https://api.github.com/users/schmmd/repos", "events_url": "https://api.github.com/users/schmmd/events{/privacy}", "received_events_url": "https://api.github.com/users/schmmd/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTE0NjIyNDY4", "tag_name": "v0.8.0", "target_commitish": "master", "name": "v0.8.0", "draft": false, "prerelease": false, "created_at": "2018-12-19T17:49:23Z", "published_at": "2018-12-20T00:14:13Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v0.8.0", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v0.8.0", "body": "# Major New Features\r\n\r\n* PyTorch 1.0 support\r\n\r\n# List of Commits\r\n\r\ne142042c Update pretrained.py\r\n4cc4b6b6 Remove constraint_type parameter from CRF Tagger (#2208)\r\neff25a30 Adding metadata and debug info for the NLVR demo (#2214)\r\n050b7671 Update predicting_paper_venues_pt2.md\r\nbb000a01 Allow NLVR predictor to handle string inputs for the structured representation (#2209)\r\n6aaf76ae Adding DatasetReader for the bAbI tasks and Qangaroo (#2194)\r\n3fd224fc fix lowercase-ization in bert indexer (#2205)\r\na2084fd3 Remove last_dim_softmax as it's deprecated and scheduled for removal. (#2207)\r\nd1bae5ce Remove deprecated dataset_readers.{nlvr, wikitables} components. (#2206)\r\n06637629 Fix (most?) warnings from BasicTextFieldEmbedder (#2117)\r\nc6663579 Adding an initializer to initialize a model to pretrained weights (#2043)\r\n30612985 Fix loaded model not taken to cuda device (#2190)\r\n7e22b861 add CPU/GPU usage in metrics.json (#2136)\r\n12d32972 Make bz2 and lzma optional dependencies (#2196)\r\na558f7f4 minor spelling tweaks (#2192)\r\n89cef2f0 Clean up temporary archive directory at exit (#2184)\r\n0ae699a5 empty time stamp should be a str instead of int 0 (#2153)\r\n8e861e75 (upstream/add_memory_usage_into_metricsA) pytorch 1.0 (#2165)\r\nd0d6310b add default argument for SpacyWordSplitter (#2174)\r\na89aebae Minor fixes to help evaluate ELMo port. (#2172)\r\nda761e8f pre commit hook to verify file sizes (#2151)\r\n41c71960 instantiate multiprocessing.log_to_stderr lazily (#2166)\r\n021471a9 Transformer ELMo (#2119)\r\n6c33005c fix bert input order (#2145)\r\na8adc6c1 Fixes broken link and rephrasing visualizing_model_internals.md (#2162)\r\n140c3ec5 make max dialog length configurable through json file (#2160)\r\n2e412e33 fix mismatches (#2157)\r\na75cb0a9 Update the elmo command help text. (#2143)\r\n7dbd7d34 add [CLS] and [SEP] tags to bert indexer (#2142)\r\n411b5a2f upgrade pytorch-pretrained-bert + fix kwarg (#2130)"}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/14323020", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/14323020/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/14323020/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v0.7.2", "id": 14323020, "author": {"login": "schmmd", "id": 954798, "node_id": "MDQ6VXNlcjk1NDc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/954798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/schmmd", "html_url": "https://github.com/schmmd", "followers_url": "https://api.github.com/users/schmmd/followers", "following_url": "https://api.github.com/users/schmmd/following{/other_user}", "gists_url": "https://api.github.com/users/schmmd/gists{/gist_id}", "starred_url": "https://api.github.com/users/schmmd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/schmmd/subscriptions", "organizations_url": "https://api.github.com/users/schmmd/orgs", "repos_url": "https://api.github.com/users/schmmd/repos", "events_url": "https://api.github.com/users/schmmd/events{/privacy}", "received_events_url": "https://api.github.com/users/schmmd/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTE0MzIzMDIw", "tag_name": "v0.7.2", "target_commitish": "master", "name": "v0.7.2", "draft": false, "prerelease": false, "created_at": "2018-12-03T18:54:13Z", "published_at": "2018-12-03T21:37:05Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v0.7.2", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v0.7.2", "body": "# Major New Features\r\n\r\n* You can now run `allennlp configure` to launch a GUI tool that helps you build a model configuration.\r\n* You can now use a BERT embedder within you model.\r\n\r\n# List of Commits\r\n\r\nca7e3cf3 release config explorer (#2118)\r\n27830443 pin msgpack (#2125)\r\n3eecd1aa more informative message for BasicTextFieldEmbedder mismatched keys (#2124)\r\n3a7e0659 fix epoch tracking (#2121)\r\ndb0096f1 Fix more BasicTextFieldEmbedder warnings (#2114)\r\nf757f7a0 Fix epoch tracking bucket iterator warnings. (#2108)\r\n44269a1d Rename DATASET_CACHE to CACHE_DIRECTORY. (#2000)\r\n3842820c Fix span pruner warning. (#2113)\r\n92c71a17 Upgrade flask to remove werkzeug warning. (#2107)\r\n240974fc Pack batches more tightly with maximum_samples_per_batch. (#2111)\r\n42d076ee modify BERT embedder to deal with higher order inputs (#2104)\r\ndcd1d25e Resolve some of the token_embedders warnings. (#2100)\r\n2648d952 Add BLEU metric to simple_seq2seq (#2063)\r\n582d0e4f add BERT token embedder  (#2067)\r\nd78daa44 Fix EVALB compilation by cd to directory path instead of binary path (#2090)\r\n6b16222b calypso transformer (#2049)\r\n5890111b Separate calculation of num_tokens per TokenIndexer (#2080)\r\n193bb04d Specify path at which to compile evalb. (#2027)\r\naf902a3f Add support of tokenized input for coref and srl predictors (#2076)\r\ne3e8e1cb Fix typo in IntraSentenceAttentionEncoder docstring (#2072)\r\nd5040896 Add wikitables predicator to the default predicators (#2071)\r\n2a2d9c94 Improve server_simple by adding predict_batch and adding GPU option (#2064)\r\n19c784fa Add BLEU metric (#2001)\r\n6ecd1932 Fix logging statement so it has the proper scope. (#2059)\r\n888c11a3 add flaky decorator + increase tolerance (#2060)\r\n86da8809 add sampled softmax loss (#2042)\r\n07b57491 Enable multi-gpu training in find_learning_rate.py (#2045)\r\n43243acf Update issue templates\r\nde7c013e Sentence splitter (#2036)\r\n0701dbdc Dynamic stopwords (#2037)\r\naa1b774e Test sql decoding (#2030)\r\na5c2d9eb Catch nan loss in training loop (#2029)\r\n82bbee7d (warnings) Add link to EMNLP tutorial from tutorial README.\r\n68cbfb80 modify training_configs related issue #1954 (#1997)\r\n481c1817 Fix ArrayField.to_tensor not working with a scalar (#2008)\r\nbe36374e change stopping factor default (#2021)\r\nb919f5ae Add logging statement when EVALB is compiled. (#2018)\r\n3ca69420 Support stdin for prediction jsonl. (#2003)\r\nbae77581 Change log level to clean up allennlp command. (#2004)\r\n1406a856 Added default predictor for bimpm model (#2014)\r\n77517992 Ignore hidden files in vocabulary/ (#2002)\r\n6af83e7c Prevent inspect_cache from swallowing exception. (#1999)\r\ne4f9131c Untyped grammar (#1986)\r\na4b885cd Add scalar_mix_parameters to Elmo.from_params (#1992)\r\n1f782d33 Add --force option to find-lr command (#1991)\r\n021f8bb1 use wordsplit with taggers (#1981)\r\nc262ef5a Match vocab generation in currently online Event2Mind model. (#1978)\r\nbc97ce87 combine_tensors_and_multiply_with_batch_size_one_and_seq_len_one (#1980)\r\n5aa1c8f0 Fix for import_submodules (#1976)\r\n02317e12 add min_padding_length to TokenCharactersIndexer (#1954) (#1967)\r\n39e16c41 delete swp file (#1975)\r\nad729e37 Link tutorial site from tutorials/README.md. (#1972)\r\n2a8bd638 Extend GPU fixes from vidurj and MaxMotovilov (#1944)\r\n"}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/13666976", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/13666976/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/13666976/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v0.7.1", "id": 13666976, "author": {"login": "schmmd", "id": 954798, "node_id": "MDQ6VXNlcjk1NDc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/954798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/schmmd", "html_url": "https://github.com/schmmd", "followers_url": "https://api.github.com/users/schmmd/followers", "following_url": "https://api.github.com/users/schmmd/following{/other_user}", "gists_url": "https://api.github.com/users/schmmd/gists{/gist_id}", "starred_url": "https://api.github.com/users/schmmd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/schmmd/subscriptions", "organizations_url": "https://api.github.com/users/schmmd/orgs", "repos_url": "https://api.github.com/users/schmmd/repos", "events_url": "https://api.github.com/users/schmmd/events{/privacy}", "received_events_url": "https://api.github.com/users/schmmd/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTEzNjY2OTc2", "tag_name": "v0.7.1", "target_commitish": "master", "name": "v0.7.1", "draft": false, "prerelease": false, "created_at": "2018-10-25T22:12:07Z", "published_at": "2018-11-12T16:24:56Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v0.7.1", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v0.7.1", "body": "This is a minor release.\r\n\r\n# List of Commits\r\n\r\n5512a8fb Add config for non-elmo constituency parser, rename existing parser config (#1965)\r\ndedc4cef Try compiling EVALB in metric if it doesn't exist (#1964)\r\n26f09cff Disable tqdm when there isn't a TTY (#1927)\r\n9a3e4b6b Add Windows support info to README.md (#1962)\r\na3bd4759 Update elmo.md\r\n19d106e7 minor bug fix in get_agenda (#1959)\r\n4e440970 Add scalar_mix_parameters to ElmoTokenEmbedder.from_params (#1956)\r\n02640024 Add scalar_mix_parameters to ElmoTokenEmbedder (#1955)\r\nce0bc55b Add training config for bidaf with elmo (#1953)\r\nc6fb86d2 Various fixes related to the variable free wikitables world (#1917)\r\naeb2fc30 Fix small typo (#1939)\r\nd089d52f Add a dimension check to DialogQA, fix example configuration (#1934)\r\n3e2d7959 Remove the report from pylint. (#1932)\r\n53a555c4 Update training_and_evaluating.md (#1837)\r\nafc36eb1 Add a --force command to train (#1913)\r\n947bd165 make api more pythonic (#1926)\r\n0e82106c clean up simple_seq2seq tests (#1928)\r\nb529f6df Generalize beam search, improve simple_seq2seq (#1841)\r\nb0ade1bf add matplotlib to setup.py (#1919)\r\n188e06d7 Passing the 'stateful' parameter to the 'PytorchSeq2SeqWrapper' (#1925)\r\n360c3e10 Text2sql model (#1905)\r\nf29839e0 fix BiMPM url in MODEL.md (#1923)\r\n404b5296 Enable setting scalar mix parameters for ScalarMix and Elmo (#1921)\r\n9fcc7956 Learning Rate Finder (#1776)\r\n63836c42 Adding support for list, tuple, and set in from_params (#1914)\r\nf224c628 Agenda improvements (#1897)\r\n2ec52a5d Uptick cffi to 1.11.5 (#1846)\r\n0e47d160 bidirectional LM + cnn highway encoder + gated cnn encoder (#1787)\r\n158a29c1 Tentative port of `LMDatatsetReader` (#1881)\r\nae7b9a73 Move 'What is AllenNLP' from README to docs. (#1909)\r\na94a23eb More closely emulate original Event2Mind implementation. (#1903)\r\nd3a8f4f6 Track dev loss in ATIS model (#1907)\r\nc4505659 Update README.md\r\n3753b0b2 Multilayer decoder for semantic parsing framework (#1902)\r\n7a707ea2 Make `SlantedTriangular` a little more robust (#1751)\r\n4a22c292 # added optimizer parameter (#1766)\r\n27fab848 Add more configuration options for ATIS semantic parser (#1821)\r\ndc66c8f9 Fix QuaRel reference (#1899)\r\nae9c9c83 try to do some type inference on variables (#1898)\r\n1691cb3a Run ActionSpaceWalker on the new variable free language for WikiTables (#1860)\r\n0d9ad655 Var free grammar (#1893)\r\n24e5547b feature-enhancement: make trainer registrable (#1884)\r\n91bfb4c0 Global grammar values (#1888)\r\nffab3201 strings in sql/prelinked entities (#1876)\r\n043ff07c allow Model to use custom Vocabulary subclasses\r\nc1dcd0ff Reflects the updated code (#1873)\r\n371fd807 input_size of phrase_layer: 1144 -> 1124 (#1875)"}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/12827058", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/12827058/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/12827058/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v0.7.0", "id": 12827058, "author": {"login": "joelgrus", "id": 1308313, "node_id": "MDQ6VXNlcjEzMDgzMTM=", "avatar_url": "https://avatars.githubusercontent.com/u/1308313?v=4", "gravatar_id": "", "url": "https://api.github.com/users/joelgrus", "html_url": "https://github.com/joelgrus", "followers_url": "https://api.github.com/users/joelgrus/followers", "following_url": "https://api.github.com/users/joelgrus/following{/other_user}", "gists_url": "https://api.github.com/users/joelgrus/gists{/gist_id}", "starred_url": "https://api.github.com/users/joelgrus/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/joelgrus/subscriptions", "organizations_url": "https://api.github.com/users/joelgrus/orgs", "repos_url": "https://api.github.com/users/joelgrus/repos", "events_url": "https://api.github.com/users/joelgrus/events{/privacy}", "received_events_url": "https://api.github.com/users/joelgrus/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTEyODI3MDU4", "tag_name": "v0.7.0", "target_commitish": "release-v0.7.0", "name": "v0.7.0", "draft": false, "prerelease": false, "created_at": "2018-10-05T21:58:33Z", "published_at": "2018-10-05T23:01:59Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v0.7.0", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v0.7.0", "body": "# Major new features\r\n\r\n- A [new framework](https://github.com/allenai/allennlp/blob/master/tutorials/getting_started/semantic_parsing.md) for training state-machine-based models, and several examples of using this for semantic parsing.  This still has a few rough edges, but we've successfully used it for enough models now that we're comfortable releasing it.\r\n- A [model](http://demo.allennlp.org/open-information-extraction) for neural open information extraction\r\n- A [re-implementation](https://github.com/allenai/allennlp/blob/master/allennlp/models/graph_parser.py) of a [graph-based semantic dependency parser](https://www.semanticscholar.org/paper/Simpler-but-More-Accurate-Semantic-Dependency-Dozat-Manning/03c3070e0d39513bb8d324d9f0ca602f7d827d6a).\r\n- A [MultiProcessDataIterator](https://github.com/allenai/allennlp/blob/master/allennlp/data/iterators/multiprocess_iterator.py) for loading data on multiple threads on the CPU (we haven't actually used this much, though - if you have trouble with it, let us know).\r\n\r\n# Breaking Changes\r\n\r\n1. Previously if you were working on a GPU, you would specify a `cuda_device` at the time you called `instance.as_tensor_dict()`, and the tensors would be generated on the GPU initially. As we started to develop code for generating instances in parallel across multiple processes, we became concerned that over-generation of instances could potentially exhaust the GPU memory. \r\n\r\nAccordingly, now `instance.as_tensor_dict()` (and all the `field.as_tensor` operations that underlie it) always return tensors on the CPU, and then the `Trainer` (or the evaluation loop, or whoever) moves them to the GPU right before sending them to the model.\r\n\r\nMost likely this won't affect you (other than making your training loop a tiny bit slower), but if you've been creating your own custom `Field`s or `Iterator`s, they'll require small changes as in #1731 \r\n\r\n# List of commits\r\n\r\n7ddc7f1 Automatically map function names to aliases for NLTK's logic parser (#1870)\r\n3989000 Bug fix binary expression in updates to grammar (#1869)\r\n3d78e46 Tutorial for the semantic parsing framework (#1853)\r\nc530fde Update MODELS.md for Event2Mind. (#1866)\r\n8ff8324 Add QuaRel semantic parser (#1857)\r\n8236624 Compile and fix warning for regex in SQL action seq formatting (#1864)\r\n5172c85 Create a markdown file that enumerates available models (#1802)\r\n2111428 Use test-install rather than verify.py (#1849)\r\nc635bc4 Graph parser for semantic dependencies (#1743)\r\nc728951 Integrate new table context in variable free world (#1832)\r\n3de6943 Avoid deprecation warnings (#1861)\r\na7da2ab Fast grammar generation (#1852)\r\nd8b13e0 Simplified GrammarStatelet, made a new LambdaGrammarStatelet class for WikiTables (#1829)\r\n1d50292 Use current_log_probs instead of log_probs in debug_info (#1855)\r\n0934512 move ftfy to right section, fix req. check script (#1858)\r\n6183c90 Remove wget in wikitables tests by using requests (#1854)\r\n358c36b Raise RuntimeError if java is not available (#1856)\r\n99308f6 Structured sql data (#1845)\r\n53b166e Executor for WikiTables variable free language (#1762)\r\n38c87e0 fix network issue (#1844)\r\n0b852fb Add Open AI tokenizer, and ability to add special tokens to token indexer (#1836)\r\nf7c4195 Faster data loading for SQL Context (#1838)\r\na1ec53d initial text2sql grammar with unconstrained context (#1834)\r\na6b4c30 Add per node beam size option to beam search (#1835)\r\n63ba3fb make some sql context code more generic (#1831)\r\ne84e496 Support adding to vocabulary from pretrained embeddings (#1822)\r\n9c7d0d0 sql data updates (#1827)\r\nb1db1c9 ATIS Predictor (#1818)\r\n53bba3d SQL Executor (#1815)\r\n8be358e Seq2Seq Test Cleanup (#1814)\r\n02e2930 Model can store extra pretained embeddings (#1817)\r\nf65ced5 Add an example of using ELMo interactively (#1771)\r\n0459261 Atis model action refactored (#1792)\r\n546242f Fixes for seq2seq model (#1808)\r\n63fcada Make num_start_types optional in transition functions (#1811)\r\n7833447 Load the pre-trained embeddings for all NER tokens (#1806)\r\nc78bb36 Add a correctness test for Open AI transformer (#1801)\r\ne64373c Update README.md\r\n01d2906 Add tags_to_spans_function param to SpanBasedF1Measure. (#1783)\r\n3906981 Moving the WikiTables executor into semparse.executors (#1786)\r\n7647de8 Fixing the table format for the WikiTables executor (#1785)\r\n6039ac0 SQL Coverage script (#1750)\r\n0b44bce Rename SOURCE_COMMIT to ALLENNLP_SOURCE_COMMIT. (#1784)\r\n9306e97 Add minimal configuration to existing models. (#1770)\r\nae72f79 Better multi-word predicates in Open IE predictors (#1759)\r\ncca99b9 Add a predictor for Event2Mind. (#1779)\r\n63dbdf1 one tutorial to rule them all (#1613)\r\n8759ea3 Event2Mind (#1679)\r\nc5501c7 fix trainer initialization (#1761)\r\n898cfed BiattentiveClassificationNetwork with ELMo and without GloVe (#1767)\r\n421f9a4 add simplest possible pretrained model interface (#1768)\r\nb9cbfd6 Add an example of how to read ELMo embeddings with h5py (#1772)\r\ne4f41e7 use managers for queues (#1769)\r\n64f253f Add a requirements check to scripts/verify.py (#1699)\r\nb5087e7 multiprocess dataset reader and iterator (#1760)\r\n1761684 Fixed bug in _get_combination_and_multiply for batch size equal to 1 (#1764)\r\nffe037d Update README.md (#1763)\r\n606a61a output metrics.json every epoch (#1755)\r\n49f43ec Remove large openie model file (#1756)\r\n609babe Add logging of learning rates to tensorboard (#1745)\r\neda2ba5 Set up `SlantedTriangular` for first batch (#1744)\r\n1d81d8b Grammar for a variable free language for WikiTableQuestions (#1709)\r\n4674b01 Make bmes_tags_to_spans support ill-formed spans. (#1710)\r\n4c99f8e Text2sql reader (#1738)\r\n8867f2f create tensors on cpu, move them to gpu later (#1731)\r\n0664893 Discriminative fine-tuning, gradual unfreezing, slanted triangular learning rates (ULMFiT) (#1636)\r\n1532886 Rename and organize tutorials (#1741)\r\n72f7b4b Remove bucket iterator shuffle warning. (#1742)\r\n8bbde0d Fix index_with bug in basic iterator (#1715)\r\n3f54fc8 make openai transformer byte pair indexer add to the vocab (#1705)\r\n7bf930f Add Open Information Extraction (#1726)\r\n6c1607e bump gevent to 1.3.6 (#1732)\r\n934ee17 Update training_and_evaluating.md (#1721)\r\n72c9e98 Sql text utils (#1717)\r\nec25acd Update README.md\r\n647e53e Removing contents of requirements.txt file (#1729)\r\na585994 Update configuration.md (#1722)\r\na61aa67 Moving allennlp.nn.decoding to allennlp.state_machines (#1714)\r\n0b7bb20 Fixes and updates to README.md (#1718)\r\nc47318b Add configs for tasks in ELMo paper, with and without ELMo (#1626)\r\nf2884ad require torch 0.4.1 (#1708)\r\nef72e2e Save learning rate scheduler to training state (#1650)\r\n6cb7005 Add average parameter to sequence cross entropy (#1702)\r\n5e68d04 Rename SpanPruner -> Pruner, remove -infs (#1703)\r\n"}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/12642384", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/12642384/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/12642384/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v0.6.1", "id": 12642384, "author": {"login": "schmmd", "id": 954798, "node_id": "MDQ6VXNlcjk1NDc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/954798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/schmmd", "html_url": "https://github.com/schmmd", "followers_url": "https://api.github.com/users/schmmd/followers", "following_url": "https://api.github.com/users/schmmd/following{/other_user}", "gists_url": "https://api.github.com/users/schmmd/gists{/gist_id}", "starred_url": "https://api.github.com/users/schmmd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/schmmd/subscriptions", "organizations_url": "https://api.github.com/users/schmmd/orgs", "repos_url": "https://api.github.com/users/schmmd/repos", "events_url": "https://api.github.com/users/schmmd/events{/privacy}", "received_events_url": "https://api.github.com/users/schmmd/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTEyNjQyMzg0", "tag_name": "v0.6.1", "target_commitish": "master", "name": "v0.6.1", "draft": false, "prerelease": false, "created_at": "2018-08-30T23:07:53Z", "published_at": "2018-08-31T00:19:25Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v0.6.1", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v0.6.1", "body": "This release includes a new dependency parser model, a QUAC model, and a new NLI model, as well as many bugfixes and small improvements.\r\n\r\n4920249 bump version number to v0.6.1\r\nec2d5a1 skip moto tests, unpin dependencies (#1697)\r\n863ded8 Add option to keep sentence boundaries in Elmo (#1695)\r\ne027478 Upgrade Flask to 0.12.4 (fixes bug) (#1694)\r\n335d899 Make SpanBasedF1Measure support BMES  (#1692)\r\nd16f6c0 Add a default predictor for biaffine-parser. (#1677)\r\n4b6f8d1 Remove inplace modification in Covariance (#1691)\r\na0506a7 remove print statement (#1690)\r\n89729e0 Add BMES constrain to is_transition_allowed function (#1688)\r\n3df54c8 Removing last_dim_*softmax (#1687)\r\nd1f6748 minor memory improvements in _joint_likelihood() of ConditionalRandomField with advanced indexing (#1686)\r\n2a45f44 Add Covariance and PearsonCorrelation metrics (#1684)\r\n1b31320 Add MeanAbsoluteError metric (#1683)\r\ne9710c8 Fix links in docs and improve check_links.py (#1680)\r\ncbeef92 Predictor for QUAC models (#1674)\r\n279f325 SQL semantic parser entity improvements (#1658)\r\n45e6a0f pin boto3 + awscli (#1671)\r\nfcdbbd3 require Python 3.6.1 (#1667)\r\n5a305e3 improve API, update tests (#1664)\r\n6c7c807 Adding decoder to bimpm and improve demo server. (#1665)\r\n9caac66 Fix lazy dataset reader bug in ModelTestCase (#1668)\r\n0b3ebcf Reading comprehension model for QUAC dataset (#1625)\r\n994b996 Make CrfTagger work with non-BIO tagging tasks (#1661)\r\nc31d5f9 Fix conll2000 data reading (#1657)\r\nabfb32d Refactoring how actions are handled in the semantic parsing code (#1294)\r\n75abebb Standardize tagging datasets (#1656)\r\n362060a add back sniff test for parser (#1654)\r\n5e13d24 Fix Conll2003 reader docs to reflect true label namespace names (#1655)\r\nfea0d0a Upgrade flask to avoid security vulnerability. (#1653)\r\nd27770a Make replace_masked_values more efficient by using masked_fill (#1651)\r\n4ade6e4 Fix module docstring for training.learning_rate_schedulers (#1649)\r\n301f2d6 Implement cosine with restarts (#1647)\r\n681a9cf make pos selection an option in dataset reader, use in predictor (#1648)\r\nbca6c2a make max_vocab_size default to None for a given namespace in Vocabulary._extend (#1643)\r\n1d43188 Add learning rate to logs (#1641)\r\nb70e026 Make LinearAttention and LinearMatrixAttention memory-efficient (#1632)\r\nbe76b5c Add missing --recover flag to train docs (#1640)\r\n2e47ac4 typing is part of standard library from python3.6 (gives errors on python3.7) (#1638)\r\n4df4638 Data reader for QUAC (#1624)\r\nbf75c9b Allow configurable label namespaces (#1621)\r\n4c6731b Pip install the library in editable mode (-e) (#1592)\r\n14aee14 fix calculation of estimated time remaining (#1631)\r\n8c89b08 Parser decoding fix 2 (#1619)\r\n7a9975e Fix bimpm config file for names num_perspective(s). (#1627)\r\n0a5aea7 atis dataset reader (#1577)\r\n659bf25 Allow files to be downloaded from S3 (#1620)\r\n76a65a8 BiMPM model (#1594)\r\n58119c0 make fine-tune not expand vocabulary by default (#1623)\r\n3107a0c Don't error if fine-tune serialization dir already exists (#1622)\r\n82686c1 Add IOB1 as allowed CRF constraint (#1615)\r\n59132d2 Avoid divide by zero in CategoricalAccuracy (#1617)"}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/11611864", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/11611864/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/11611864/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v0.6.0", "id": 11611864, "author": {"login": "joelgrus", "id": 1308313, "node_id": "MDQ6VXNlcjEzMDgzMTM=", "avatar_url": "https://avatars.githubusercontent.com/u/1308313?v=4", "gravatar_id": "", "url": "https://api.github.com/users/joelgrus", "html_url": "https://github.com/joelgrus", "followers_url": "https://api.github.com/users/joelgrus/followers", "following_url": "https://api.github.com/users/joelgrus/following{/other_user}", "gists_url": "https://api.github.com/users/joelgrus/gists{/gist_id}", "starred_url": "https://api.github.com/users/joelgrus/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/joelgrus/subscriptions", "organizations_url": "https://api.github.com/users/joelgrus/orgs", "repos_url": "https://api.github.com/users/joelgrus/repos", "events_url": "https://api.github.com/users/joelgrus/events{/privacy}", "received_events_url": "https://api.github.com/users/joelgrus/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTExNjExODY0", "tag_name": "v0.6.0", "target_commitish": "master", "name": "v0.6.0", "draft": false, "prerelease": false, "created_at": "2018-08-15T16:29:47Z", "published_at": "2018-08-15T17:56:03Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v0.6.0", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v0.6.0", "body": "AllenNLP v.0.6.0 has been upgraded to use PyTorch 0.4.1. Accordingly, it should now run on Python 3.7.\r\n\r\nIt contains a handful of breaking changes, most of which probably won't affect you.\r\n\r\n# Breaking changes:\r\n\r\n## 1. HOCON -> Jsonnet for Configuration files\r\nAlthough our experiment configurations look like JSON, they were technically [HOCON](https://github.com/lightbend/config/blob/master/HOCON.md) (which was a superset of JSON). In this release we changed the format to [Jsonnet](https://jsonnet.org/), which is a *different* superset of JSON.\r\n\r\nIf your configuration files are \"JSON with comments\", this change should not affect you. Your configuration files are valid jsonnet and will work fine as is. We believe this described 99+% of people using allennlp.\r\n\r\nIf you are using advanced features of HOCON, then these changes will be breaking for you. Probably the two most common issues will be\r\n\r\n### unquoted strings\r\n\r\nJSON requires strings to be quoted. HOCON doesn't. Jsonnet does. So in the off chance that you have not been putting your strings in quotes, you'll need to start putting them in quotes.\r\n\r\n### environment variables\r\n\r\nHOCON allows you to substitute in environment variables, like\r\n\r\n```\r\n    \"root_directory\": ${HOME}\r\n```\r\n\r\nJsonnet only allows substitution of explicit variables, using a syntax like\r\n\r\n```\r\n    \"root_directory\": std.extVar(\"HOME\")\r\n```\r\n\r\nthese are in fact variables fed to the Jsonnet parser (not environment variables); however, the allennlp code will read all the environment variables and feed them to the parser\r\n\r\n### the elimination of `ConfigTree`\r\n\r\n(you probably don't care about this)\r\n\r\npreviously the AllenNLP `Params` object was a wrapper around a `pyhocon` `ConfigTree`, which is basically a fancy `dict`. After this change, `Params.params` is just a plain `dict` instead of a `ConfigTree`, so if you have code that relies on it being a `ConfigTree`, that code will break. This is very unlikely to affect you.\r\n\r\n### why did we make this change?\r\n\r\nThere is a bug in the Python HOCON parser that incorrectly handles backslashes in strings. This created issues involving initializer regexes being serialized and deserialized incorrectly. Once we determined that the bug was not simple enough for us to easily fix, we chose this as the next best solution.\r\n\r\n(in addition, jsonnet has some nice features involving templates that you might find useful in your experiments)\r\n\r\n## 2. Change to the `Predictor API`\r\n\r\nThe API for the `_json_to_instance` method of the `Predictor` used to be `(json: JsonDict) -> Tuple[Instance, JsonDict]`, where the returned JsonDict contained information from the input which you wanted to be returned in the predictor. This is now not allowed, and the `_json_to_instance` method returns only an `Instance`, meaning any additional information must be routed through your model via the use of `MetadataFields`. This change was to make `Predictors` agnostic of where `Instances` they process come from, allowing us to generate predictions from an original dataset using a `DatasetReader` to generate instances.\r\n\r\nThis means you can now do:\r\n`allennlp predict /path/to/original/dataset --use-dataset-reader`, rather than having to format your data as .jsonl files.\r\n\r\n## 3. Automatic implementation of `from_params`\r\n\r\nIt used to be the case that if you implemented your own `Model` or `DatasetReader` or whatever, you were required to implement a `from_params` classmethod that unpacked a `Params` object and called the constructor with the relevant values. In most cases this method was just boilerplate that didn't do anything interesting -- it popped off strings and strings and ints and ints and so on. And it opened you up to a class of subtle bugs if your `from_params` popped parameters with a different default value than the constructor used.\r\n\r\nIn the latest version, any class that inherits from `FromParams` (which automatically includes all `Registrable` classes) gets for free a `from_params` method that does the \"right thing\". If you need complex logic to instantiate your class from a JSON config, you'll still have to write your own method, but in most cases you won't need to.\r\n\r\nThere are some `from_params` methods that take additional parameters; for example, every `Model` constructor requires a `Vocabulary`, which will need to be supplied by its `from_params` method. To support this, the automatic `from_params` allows extra *keyword-only* arguments.  That is, if you are calling the `from_params` method yourself (which you probably aren't), you have to do\r\n\r\n```\r\nYourModel.from_params(params, vocab=vocab)\r\n```\r\n\r\nif you try to supply the extra arguments positionally (which you could when all of the from_params were defined explicitly), you will get an error. This is the \"breaking\" component of the change.\r\n\r\n## 4. changes to `TokenIndexers`\r\n\r\npreviously the interface for `TokenIndexer` was\r\n\r\n```\r\nTokenIndexer.token_to_indices(self, token: Token, vocabulary: Vocabulary) -> TokenType:\r\n```\r\n\r\nthis assumption (one token) -> (one or more indices) turned out to be not general enough. there are cases where you want to generate indices that depend on multiple tokens, and where you want to generate multiple sets of (related) indices from one input text.  accordingly, we changed the API to\r\n\r\n```\r\nTokenIndexer.tokens_to_indices(self, tokens: List[Token], vocabulary: Vocabulary, index_name: str) -> Dict[str, List[TokenType]]:\r\n```\r\n\r\nthis is some real library-innards stuff, and it is unlikely to affect you or your code unless you have been writing your own `TokenIndexer` subclasses or `Field` subclasses (which is not most users). If this does describe you, look at the changes to `TextField` to see how to update your code.\r\n\r\n--------\r\n\r\nother changes:\r\n\r\n95401255 Tree decoding fix (#1606)\r\n4eaeff72 Fix use of scalar tensors in ConllCorefScores (#1604)\r\n982ceddf Small typo fixes in tutorial (#1603)\r\n45fff838 use empty list for no package not empty string (#1602)\r\ne0e5f4a8 revert conllu changes (#1600)\r\n49626cc1 filter out numpy 'size changed' warnings (#1601)\r\n4fca0287 (include-package-fix) Log number of parameters in optimizers (#1598)\r\nb79d5001 Add file friendly logging for elmo. (#1593)\r\n152b590c Output details when running check-links. (#1569)\r\n068407e3 make --include-package include all submodules (#1586)\r\n12b74e5a Add some debugging echo commands to pip. (#1579)\r\n9194b30b copy bin/ into image (#1587)\r\nbf760b04 be friendlier to windows users (#1572)\r\n4fa4dc23 fix and pin conllu dependency == 1.0 (#1581)\r\n6b37dd2b Turn off shuffling during evaluation (#1578)\r\n07bfc312 Demo features for the dependency parser (#1560)\r\n025b5e7c Remove a step from verify. (#1565)\r\nd2c02742 Don't use a hard-coded temp directory. (#1564)\r\n15e36458 openai transformer LM embedder (#1525)\r\n87b32bb3 Expose iterator shuffling through Trainer config (#1557)\r\n52f44e24 Add parsimonious for SQL parsing to setup.py (#1558)\r\n089d16dc SQL action sequences and Atis World (#1524)\r\n6f0fec13 make passing different feedforward modules more flexible (#1555)\r\ndcba726a WIP: Skip tests that require Java in test-install (#1551)\r\n8438f91f Remove the unused NltkWordSplitter and punkt model. (#1548)\r\n09c2cc56 Dependency parser predictor (#1538)\r\nc2e70ca7 upgrade to pytorch 0.4.1 + make work with python 3.7 (but still 3.6 also) (#1543)\r\nc000ae22 made checklist updates more efficient (#1552)\r\n2ec4c5c2 re-work dependency parser to use HEAD sentinel inside model (#1544)\r\n10ac9ede Update install_requirements.sh\r\n1c2a0deb Remove requirements_test.txt (merge into requirements.txt) (#1541)\r\n2154e726 Allow server start without field-names. (#1523)\r\ne32f4865 fix BasicTextFieldEmbedder.from_params to reflect the constructor (#1474)\r\ndc1ff363 Fix the reported broken links. (#1533)\r\ne049afc0 fix ud reader in case of implicit references (#1529)\r\nad265f85 Add output-file option in evaluate to save the computed metrics (#1512)\r\nc37ff2ca update config files to jsonnet format (#1479)\r\nf3fce4c9 Move cache breaker to the end. (#1527)\r\nc9385e78 Fixed broken link (#1508)\r\n8cf893e7 Add a scirpt to report broken links in all markdowns. (#1522)\r\nbe69e52a Parser improvements (#1515)\r\ne4b86b0e Show warning before ignoring key with unseparable batches from model.forward. (#1520)\r\n2c9abf9a Minor change of a comment (#1500)\r\n0722d7f1 DenseSparseAdam + CRF Feedforward layer (#1519)\r\n1402b7c0 Add to_file method in Params and default preference ordering. (#1517)\r\ne0581b61 Preserve best metrics (#1504)\r\nde0d3f73 Dependency parser (#1465)\r\nbe0f0c25 Remove extra .params (#1513)\r\n7df8275e Text field updates to support multiple arrays in TokenIndexer (#1506)\r\n88c381a1 Make usage of make-vocab, dry-run consistent with train and allow 'extend' to be used by both (#1487)\r\n34a92d0d Update using_as_a_library_pt1.md (#1509)\r\n8e5ee656 fix dumb domain filtering (#1505)\r\n8a208203 Ensure Contiguous Hidden State Tensors in Encoders  (#1493)\r\n66b2c1c3 Bio to bioul (#1497)\r\nf4eef6ee [for discussion] change token_to_indices -> tokens_to_indices (in preparation for byte pair encoding) (#1499)\r\n9ec3aa64 Fix start of tqdm logging in training. (#1492)\r\nee003d24 Fix SpanBasedF1Measure allowed label encodings comment (#1501)\r\nd307a253 Add IOB1 support to SpanBasedF1Metric (#1494)\r\n9c21696d fixing a bug in trainer for histograms (#1498)\r\n5f2f539a Add option to have tie breaking in Categorical Accuracy (#1485)\r\n74577103 Update elmo.py (#1496)\r\n5fc7a00d Fix SpanBasedF1Measure for tags without conll labels (#1491)\r\n01ddd126 make tables nice in validation summary (#1490)\r\nba6f3451 Crf ner tweaks (#1488)\r\nf5bbe592 Move param import (#1484)\r\ne50b1027 fine grained ner reader (#1483)\r\nd9e98610 don't call create_kwargs  for a class that has no constructor (#1481)\r\n52c08355 instantiate default activation functions in constructor (#1478)\r\n580dc8b0 (mostly) remove from_params (#1191)\r\nff41dda4 Implementation of ESIM model (#1469)\r\ne2edc9b2 unwrap tensors in avg metric (#1463)\r\n77298a97 Fix logging of no-grad parameters. (#1448)\r\nbef52ed7 Fix call to vocab.token_from_index -> self.label_namespace (#1459)\r\n7cc3db13 fix Vocabulary.from_params to accept a dict for max_vocab_size (#1460)\r\n59ecd3b7 Fix conll2003.from_params incorrect default (#1453)\r\nf09ff87e Allow to use a different validation iterator from training iterator (#1455)\r\na56fa40f remove RegistrableVocabulary (#1454)\r\nf136ae00 Fix a typo in embedding_tokens notebook. (#1449)\r\nd4ee5db1 Make bucket iterator respect maximum_samples_per_batch (#1446)\r\nf0ed1d45 Few feature additions (#1438)\r\n74a30d08 update the look and feel of the config explorer (#1412)\r\nfa34344b refactor iterators (#1157)\r\n43fc89ee Enables Predict to use dataset readers from models (#1434)\r\nd2e3035f enable mypy on tests (#1437)\r\n7664b121 Add support for selective finetune (freeze parameters by regex from config file) (#1427)\r\n8855042f eliminate or make private most of the new Vocabulary methods (#1436)\r\na0c368a7 Fix an edge case for incompatible vocabulary extension. (#1435)\r\n18d4fee3 remove adaptive iterator (#1433)\r\n0312b16e Add support for configurable vocabulary extension (#1416)\r\n5d382821 Avoid non-model state in predictors (#1422)\r\neaf5b7e1 Call  before logging to tensorboard (#1423)\r\n872acf94 Make evaluation tqdm description ignore metrics starting with _ (#1430)\r\n36d91fd1 Make tqdm description ignore metrics starting with _ (#1425)\r\n70d4d3c3 use sensible default for num_serialised_models_to_keep (#1420)\r\n9dbba338 Fix chdir in ModelTestCase breaking downstream models (#1418)\r\n1031815a duplicate config in Predictor.from_archive (#1413)\r\n2bf1e28b fix a minor typo in docstring causing wrong api usage docs of vocabulary config. (#1415)\r\ne16a6b5b Split off function to find latest checkpoint in Trainer (#1414)\r\n70b4ffb0 (jonborchardt/master) replace hocon with jsonnet (#1409)\r\n8a314940 (upstream/ratecalculus, jonborchardt/ratecalculus) Add --include-sentence-indices flag to ELMo command (#1404)\r\n4bd8e7f2 Add support for prevention of parameter initialization which match the given regexes (#1405)\r\n76deabb5 Remove frontend (#1407)\r\n6800d76a fix elmo command to use line indices and disallow empty lines (#1397)\r\ne9030189 Fix multiple GPU training after upgrading to pytorch 0.4 (#1401)\r\ndb519aff Update README.md with ./allennlp/run.py (#1395)\r\n3dff9c7a remove demo (#1338)\r\n6da17d67 Adds support for reading pretrained embeddings (text format) from uncompressed files and archives (#1364)\r\n5e38a081 Update Dockerfile.pip\r\nda429d6d Update Dockerfile.pip\r\n2b32a867 Update Dockerfile.pip\r\nbb08b065 Add a Dockerfile for downstream usage of AllenNLP. (#1389)\r\nbab565a0 Update elmo.py (#1388)\r\n3aa81e71 In get_from_cache(), allow redirections in head requests (#1387)\r\nf4d8d073 Output answers in wikitables predictor when inputs are batched (#1384)\r\na8072394 create ccgbank dataset reader (#1381)"}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/11472312", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/11472312/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/11472312/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v0.5.1", "id": 11472312, "author": {"login": "joelgrus", "id": 1308313, "node_id": "MDQ6VXNlcjEzMDgzMTM=", "avatar_url": "https://avatars.githubusercontent.com/u/1308313?v=4", "gravatar_id": "", "url": "https://api.github.com/users/joelgrus", "html_url": "https://github.com/joelgrus", "followers_url": "https://api.github.com/users/joelgrus/followers", "following_url": "https://api.github.com/users/joelgrus/following{/other_user}", "gists_url": "https://api.github.com/users/joelgrus/gists{/gist_id}", "starred_url": "https://api.github.com/users/joelgrus/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/joelgrus/subscriptions", "organizations_url": "https://api.github.com/users/joelgrus/orgs", "repos_url": "https://api.github.com/users/joelgrus/repos", "events_url": "https://api.github.com/users/joelgrus/events{/privacy}", "received_events_url": "https://api.github.com/users/joelgrus/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTExNDcyMzEy", "tag_name": "v0.5.1", "target_commitish": "main", "name": "v0.5.1", "draft": false, "prerelease": false, "created_at": "2018-06-13T23:56:40Z", "published_at": "2018-06-14T00:23:07Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v0.5.1", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v0.5.1", "body": "`numpydocs` was left out of requirements.txt and setup.py, so that `pip install allennlp` wouldn't install it and allennlp wouldn't work (unless you installed it manually). This fixes that."}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/10699119", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/10699119/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/10699119/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v0.5.0", "id": 10699119, "author": {"login": "schmmd", "id": 954798, "node_id": "MDQ6VXNlcjk1NDc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/954798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/schmmd", "html_url": "https://github.com/schmmd", "followers_url": "https://api.github.com/users/schmmd/followers", "following_url": "https://api.github.com/users/schmmd/following{/other_user}", "gists_url": "https://api.github.com/users/schmmd/gists{/gist_id}", "starred_url": "https://api.github.com/users/schmmd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/schmmd/subscriptions", "organizations_url": "https://api.github.com/users/schmmd/orgs", "repos_url": "https://api.github.com/users/schmmd/repos", "events_url": "https://api.github.com/users/schmmd/events{/privacy}", "received_events_url": "https://api.github.com/users/schmmd/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTEwNjk5MTE5", "tag_name": "v0.5.0", "target_commitish": "master", "name": "v0.5.0", "draft": false, "prerelease": false, "created_at": "2018-06-13T17:33:49Z", "published_at": "2018-06-13T18:26:49Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v0.5.0", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v0.5.0", "body": "Breaking changes:\r\n\r\n- Starting with v0.5.0, AllenNLP is based on PyTorch 0.4. This version of PyTorch contains [many substantial changes from earlier releases](https://pytorch.org/2018/04/22/0_4_0-migration-guide.html) and the internals of AllenNLP required corresponding changes. It is possible that this may break some of your custom models.\r\n\r\n- The prior `Attention` and `MatrixAttention` modules have been renamed `LegacyAttention` and `LegacyMatrixAttention`, and have been replaced with `Registrable` classes that allow for much more efficient attention implementations.  The minimal change to upgrade if you were using these previously is to just change where you have `Attention` to be `LegacyAttention`.  Though, it'll probably speed up your code and/or reduce memory usage if you switch to using the new `Attention`.\r\n\r\nLarge new features:\r\n\r\n- We have added a Configuration Wizard that makes it easier to create configuration files for your experiments. Its use is detailed [in a new how-to](https://github.com/allenai/allennlp/blob/master/tutorials/how_to/create_a_configuration.md).\r\n\r\nFull list of changes:\r\n\r\n7128324 (tag: v0.5.0) bump version number to v0.5.0\r\ne17cbd1 fix bug (#1363)\r\n691af4e address PR comments (#1362)\r\nd844a9a fancy version of the configuration wizard (#1344)\r\n6739c31 Make test_trainer_respects_keep_serialized_model_every_num_seconds more patient (#1358)\r\nac2e0b9 Make optional args actually optional. (#1357)\r\na08e7fd Move serve functionality out of allennlp command. (#1317)\r\n97484fb Faster Elmo (#1347)\r\nca4d262 Remove norm from question / table similarity computation (#1354)\r\n2cddc69 Add ability to add tokens to vocabulary namespaces (#1352)\r\nb556e53 Format pytest logging correctly (#1351)\r\n6aaeb70 Add bilinear attention (#1349)\r\nf6d9ba3 Remove what appears to be outdated line (#1348)\r\n6c87ff5 Make vocab (#1339)\r\n0c2d5c7 config explorer webapp (#1329)\r\nb6b8955 Fix link to demo.allennlp.org page (#1336)\r\n4326b59 Fix link to installation instructions (#1335)\r\n03d6fad (initializers-regularizers) command-line configuration explorer (#1309)\r\nb0d0d94 LabelField.as_tensor returns 0-tensor (#1320)\r\n765dc20 Add logging for the original path in load_archive. (#1323)\r\n8eb358b Convert run_with_beaker.py to use Blueprints. (#1199)\r\n427a319 Add version to AllenNLP command. (#1322)\r\n2456983 Add option to keep Viterbi scores when predicting (#1314)\r\n62f701e Remove default arguments from the simple server. (#1319)\r\n50fcb0c how to for debugging using pycharm (#1312)\r\ncebf719 SRL elmo 5b (#1310)\r\ne37a68e Add \"Citing\" section to README.md (#1305)\r\n0db4bc5 Add ELMo to BCN (#1308)\r\n097ccd7 Update torch and tensorboard versions in setup.py (#1311)\r\n26e7f41 add user contributed models blurb (#1306)\r\n5352a19 make predictors more discoverable (#1302)\r\n4849589 Script for generating logical forms from ERM model for wikitables (#1247)\r\n504964b allow sanitize to work on custom classes (#1307)\r\n200b27e A new model is only considered the best if it is better than others and not simply better or equal (#1246)\r\ned63b7e upgrade to pytorch 0.4.0 (#1126)\r\nb26031d Ignore tests in coverage metrics (#1293)\r\n986cf17 Improve performance of attention modules (#1235)\r\n69b9af3 In Trainer, require \"patience\" to be a positive integer or None (meaning \"no early stopping\"). Set patience=None by default. (#1271)\r\n9cb0ea3 Add LICENSE to source distribution (#1291)\r\nc26a513 Noam lr schedule (#1258)\r\n59a0e5a Fix test-install by moving tests into module (#1232)\r\nc0afe31 Insteall psycopg2-binary (#1286)\r\n1f8125c (cli-params) Update the version of twine. (#1285)\r\n5c06914 Use full version in docs. (#1283)\r\n20da3fd Set long_description_content_type to markdown in setup.py. (#1284)\r\n7e1080e Uptick version to unreleased. (#1281)\r\n3c623c5 Fix links to the allennlp-as-a-library tutorials (#1279)"}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/11161652", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/11161652/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/11161652/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v0.4.3", "id": 11161652, "author": {"login": "schmmd", "id": 954798, "node_id": "MDQ6VXNlcjk1NDc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/954798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/schmmd", "html_url": "https://github.com/schmmd", "followers_url": "https://api.github.com/users/schmmd/followers", "following_url": "https://api.github.com/users/schmmd/following{/other_user}", "gists_url": "https://api.github.com/users/schmmd/gists{/gist_id}", "starred_url": "https://api.github.com/users/schmmd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/schmmd/subscriptions", "organizations_url": "https://api.github.com/users/schmmd/orgs", "repos_url": "https://api.github.com/users/schmmd/repos", "events_url": "https://api.github.com/users/schmmd/events{/privacy}", "received_events_url": "https://api.github.com/users/schmmd/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTExMTYxNjUy", "tag_name": "v0.4.3", "target_commitish": "master", "name": "v0.4.3", "draft": false, "prerelease": false, "created_at": "2018-05-24T20:39:36Z", "published_at": "2018-05-25T01:42:09Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v0.4.3", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v0.4.3", "body": "This is primarily a bugfix release as well as the final release on PyTorch 0.3.\r\n\r\n* Fixes the [TQDM bug](https://github.com/allenai/allennlp/pull/1168/files) that was causing jobs to hang in our internal job execution framework (Beaker).\r\n\r\n* [Moves tests](https://github.com/allenai/allennlp/issues/1217) into the source distribution, which a user needed to create a conda installation.\r\n\r\nHere is a full list of commits:\r\n\r\n66da4eed Update version numbers referred to in tutorials.\r\n1c8b1492 (upstream/master, master) Fix crash when validation metric is 0 (#1274)\r\nb4c9fe41 (release-v0.4.2) Migrate gevent's wsgi to pywsgi (#1272)\r\n6b5b0172 `allennlp train` should fail faster if trainer.cuda_device >= 0 (#1196)\r\n40691d3f Fix LR scheduler and test usage inside Trainer (#1255)\r\ned3697c3 Remove most mentions of python -m allennlp.run (#1264)\r\n7b9d6a25 Consolidate installation instructions (#1261)\r\n622ea1bf (origin/master, origin/HEAD) Tutorial cleanup (#1260)\r\nc2b61c60 make evalb use unique temporary directories per process (#1254)\r\n906b26f1 make it OK if serialization_dir exists but is empty (#1251)\r\n5ebe103d Add missing close brace, standardize indentation in ELMo howto (#1252)\r\neabb37fc Output predictions in CSV format for official eval (#1249)\r\nd1c36de9 Constituency Parser Demo Reference Update (#1248)\r\n4f2db42f Replace slicing with chunk along the last dim (#1244)\r\na4f2fb8f Cache remote datasets with fixed-length filenames (#1230)\r\n9ef013d3 Fix Constituency Parser / EVALB for pip-installed allennlp (#1233)\r\n3dcde323 Add custom extensions to distribution and install (#1229)\r\n229361c6 In train_model(), load the best weights at the end so that: (#1234)\r\n76a5a80a changed checklist cost computation (#1231)\r\nd0bd42d5 edit project root test (#1227)\r\n356829d9 Add allennlp test-install command (#1213)\r\n820cbfba gitignore EVALB generated files (#1214)\r\n3a8fdb01 Include tests in source distribution (#1208)\r\n10ea3b36 Add the attention visualization to the textual entailment demo (#1219)\r\nb71ef434 Fix for https://github.com/allenai/allennlp/issues/203 (#1210)\r\nf246da73 Add missing import to setup.py (#1215)\r\n70862e09 Make pytest_runner a conditional requirement in setup.py (#1203)\r\n10e7e99e Remove argparse from requirements (#1202)\r\ne1fdad6e Added a ConstrainedBeamSearch, switched MML to use it (#1189)\r\n7bd52cb7 Use README.md for PyPI long description (#1201)\r\n2dda95be Changed how checklist balance is used in action prediction (#1204)\r\n1554fb03 Update test requirements in setup.py, remove pytest-pythonpath (#1207)\r\na8f7adae Added dropout to the NLVR parser (#1205)\r\nb5af3e47 Prune finished states in ERM (#1187)\r\n77b33fb7 Add compute model number to Compile custom LSTM for Tesla V100 (#1198)\r\n63f2108e Update ELMo command help docstring (#1195)\r\n8ba58675 Added coverage to WikiTables ERM parser (#1181)\r\n69ae074e Rename tutorials so they're clearly connected. (#1186)\r\n65aecdd7 Add a tutorial for using the pre-trained models. (#1182)\r\n3add61da check for zero grads only if grad is not none (#1188)\r\n7142962d ERM parser for wikitables (#1159)\r\nc369502a Add missing --all flag to ELMo howto (#1183)\r\nea2e431c Update using_in_your_repo.md\r\n7cc4f72c Update using_in_your_repo.md\r\n19980b52 Add shebang to bin/allennlp (#1179)\r\n58f90337 Clarify the ELMo readme (#1167)\r\n4f8834c7 fix comment (#1171)\r\naf283d16 hack to stop tqdm hanging (#1168)\r\na6d4b1b7 Merge branch 'release-v0.4.2'\r\n15f61028 bump version numbers in README\r\n60e03e3d remove spaces in SpacyWordSplitter.batch_tokenize (#1156)"}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/10699079", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/10699079/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/10699079/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v0.4.2", "id": 10699079, "author": {"login": "schmmd", "id": 954798, "node_id": "MDQ6VXNlcjk1NDc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/954798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/schmmd", "html_url": "https://github.com/schmmd", "followers_url": "https://api.github.com/users/schmmd/followers", "following_url": "https://api.github.com/users/schmmd/following{/other_user}", "gists_url": "https://api.github.com/users/schmmd/gists{/gist_id}", "starred_url": "https://api.github.com/users/schmmd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/schmmd/subscriptions", "organizations_url": "https://api.github.com/users/schmmd/orgs", "repos_url": "https://api.github.com/users/schmmd/repos", "events_url": "https://api.github.com/users/schmmd/events{/privacy}", "received_events_url": "https://api.github.com/users/schmmd/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTEwNjk5MDc5", "tag_name": "v0.4.2", "target_commitish": "master", "name": "v0.4.2", "draft": false, "prerelease": false, "created_at": "2018-05-01T17:04:52Z", "published_at": "2018-05-01T18:03:29Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v0.4.2", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v0.4.2", "body": "The most immediate change is that now PyTorch is installed via `pip`, and you don't have to do it separately.   We also have new code to support a state-of-the-art NER model.\r\n\r\nHere's the full list of commits:\r\n\r\nd4b286c Mattp/ner (#1162)\r\nd05673a Add a default cuda_device to model. (#1151)\r\n281d032 Added two feature extractors that were in PNP and not in our version (#1155)\r\nf8d64ef add stacked bilm (#1154)\r\nf2257a6 Use separate action embeddings for prediction vs. input feeding (#1153)\r\ncc510fa Allow ablation where linking features are removed (#1145)\r\n121d3e8 unidecode in setup (#1148)\r\n55b0411 Adding ReLUs after linear projections (#1150)\r\n645379e Allow default predictors when calling programatically (#1112)\r\n6d15d17 Allow to restart with optimizers on CUDA (#1144)\r\nc684216 Streamlining the docker build a little bit (#1140)\r\nbfff55f bug fix for empty decoder outputs (#1141)\r\nb9630e6 Fix state scores in WikiTablesDecoderStep (#1129)\r\na869e0b Move imports to inside of CACHE_MODELS=true condition (#1133)\r\n53ae735 clean EVALB via make, add to Dockerfile (#1132)\r\n812dfcf Added a script to generate data from an ERM trained model (#1115)\r\n2b81da7 Remove the KeyError catch block in the server (#1130)\r\n960f913 Adding WikiTables and NLVR predictors (#1118)\r\n7627a09 Use xavier_uniform for initializing embedding weights (#1120)\r\n0df8978 Adding the wikitables parser (#1114)\r\nf7b736c increase metrics logging precision (#1113)\r\n7dfc342 Adding NLVR model (#1108)\r\nbc8ac1a Adding WikiTables dataset reader (#1106)\r\ncbe5897 Adding NLVR dataset reader (#1105)\r\nf7924bc Adding KnowledgeGraphField and ProductionRuleField (#1104)\r\n245a9af add name argument to run_with_beaker.py (#1103)\r\n2e6f2dd Adding the type system / logical form parsing code from the wikitables branch (#1087)\r\neb81ac9 Update elmo.md (#1092)\r\n1655f22 Adding the decoding framework from the wikitables branch (#1086)\r\n3ea82b1 Moving over modifications to common / util modules (#1085)\r\nf7b736c increase metrics logging precision (#1113)\r\n7dfc342 Adding NLVR model (#1108)\r\nbc8ac1a Adding WikiTables dataset reader (#1106)\r\ncbe5897 Adding NLVR dataset reader (#1105)\r\nf7924bc Adding KnowledgeGraphField and ProductionRuleField (#1104)\r\n245a9af add name argument to run_with_beaker.py (#1103)\r\n2e6f2dd Adding the type system / logical form parsing code from the wikitables branch (#1087)\r\neb81ac9 Update elmo.md (#1092)\r\n1655f22 Adding the decoding framework from the wikitables branch (#1086)\r\n3ea82b1 Moving over modifications to common / util modules (#1085)\r\n78dc1df Data cleanup and additions from the wikitables branch (#1084)\r\n6829edb Move the top nav bar to the left pane. (#1082)\r\n14bb4e3 Revert \"Top menu -> Left menu (#1074)\" (#1081)\r\na92bf48 Top menu -> Left menu (#1074)\r\nb72c838 Adding an optional projection layer to ElmoTokenEmbedder (#1076)\r\n4c02d92 Reading glove embeddings: strip() --> rstrip() (#1056)\r\n0a918aa read vocab params in model test when available (#1071)\r\n8ddc8cb initial dry run command (#1063)\r\nda1e6f4 Update setup.py\r\ne8e2499 install pytorch via pip (#1062)\r\nbcbc12e don't use mutable sys.stdout for both logs (#1060)\r\nebf25ea One line update in file_utiles.py (#1059)\r\n9299131 Add a multilabel field (#1054)\r\ndb44a25 Add a get_final_encoder_states function that handles bidirectional encoders (#1048)\r\n77da425 don't allow U tags in BIO to spans, use BIOUL in SpanF1Measure (#1045)\r\ne444216 Add an ensemble model for BiDAF (#1000)\r\na5a827e Add Biattentive Classification Network (#1038)\r\n78aa071 proper configuration of all dropout in self attention (#1042)\r\nd0bb3d2 CoNLL dataset reader that allows BIOLU tags (#1041)\r\nd57717e srl model tweaks (#1040)\r\n1700db4 Minor enhancements to seq2seq data reader (#1037)\r\n2b55bfb re-jig readme (#1034)\r\n395646a Add Stanford Sentiment Treebank dataset reader (#1032)"}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/10239790", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/10239790/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/10239790/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v0.4.1", "id": 10239790, "author": {"login": "DeNeutoy", "id": 16001974, "node_id": "MDQ6VXNlcjE2MDAxOTc0", "avatar_url": "https://avatars.githubusercontent.com/u/16001974?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DeNeutoy", "html_url": "https://github.com/DeNeutoy", "followers_url": "https://api.github.com/users/DeNeutoy/followers", "following_url": "https://api.github.com/users/DeNeutoy/following{/other_user}", "gists_url": "https://api.github.com/users/DeNeutoy/gists{/gist_id}", "starred_url": "https://api.github.com/users/DeNeutoy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DeNeutoy/subscriptions", "organizations_url": "https://api.github.com/users/DeNeutoy/orgs", "repos_url": "https://api.github.com/users/DeNeutoy/repos", "events_url": "https://api.github.com/users/DeNeutoy/events{/privacy}", "received_events_url": "https://api.github.com/users/DeNeutoy/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTEwMjM5Nzkw", "tag_name": "v0.4.1", "target_commitish": "main", "name": "v0.4.1", "draft": false, "prerelease": false, "created_at": "2018-03-23T22:12:36Z", "published_at": "2018-03-23T22:40:12Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v0.4.1", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v0.4.1", "body": "496867ca add an initial string representation to fields and instances (#1021)\r\n008ceb26 Break up AWS login command in two to catch failures. (#1022)\r\na0ced895 reset meteric when done evaluation (#1020)\r\n3ea3e64d fix logging error (#1017)\r\nab3bd0f5 add optional exclusive start indices flag to Enpoint Extractor (#1005)\r\nf37bc6cc add some new optimisers and a cosine LR schedule (#1012)\r\na327d03c make srl predictor more robust (#1011)\r\n5f722fcd Add .pytest_cache folder to gitignore (#1006)\r\n31f4f604 better error for gradient check (#1010)\r\nf06d62f5 readme: add port publishing for docker (#1009)\r\n760853c5 Winobias reader (#968)\r\n3c8e299a fix predictor issue for constituency parser (#1004)\r\nd351ac7e Fix simple tagger link in creating a model tutorial (#1003)\r\ne28415ca remove PREDICTOR_OVERRIDES and MODEL_OVERRIDES (#999)\r\n1d68f889 (michael/master) add allennlp command (#987)\r\n0c532d50 use correct PTB results on demo page (#990)\r\ndc7a9d68 make hierplane div overflow so it's scrollable (#989)\r\n565b95cf (matt/master) Constituency parser demo (#985)\r\neb794288 Change input name from \"source_string\" to \"source\" in Seq2SeqPredictor (#984)\r\n7b2e09db Fix issue #973 and add predictor for SimpleSeq2Seq (#976)\r\n40d337ea Keyboard interrupt + training admin (#983)\r\nc7a4e6d4 don't require that EVALB is compiled on instantiation (#980)\r\n5c663c45 fix span construction (#975)\r\n3d100d31 Fix max_vocab_size bug (#964) (#966)\r\n18eb4218 fix sentence encoder from params (#963)\r\n9121eed6 Move --include-packages flag up to top level (#962)\r\ncc2756db Add a link to the span representations tutorial (#959)\r\n0ef211da fix online metric calculation for EVALB (#956)\r\nc84c714c use biased estimator for layernorm, fix some views in multi head attention (#953)\r\n71c80e39 Install nltk the normal way. (#932)\r\n6044cced Allow to provide a custom `Module` in `Elmo` class (#945)\r\ne6573532 add label smoothing to loss (#942)\r\nf81e27a5 get the dimensions correct for the transformer (#941)\r\nc12d4356 Automatically install spacy models, if missing (#933)\r\ncdfe9aed add ImportError to lstm import (#938)\r\nc66fac02 pos -> tag (#935)\r\ndcd6487e More parser improvements (#934)\r\nacf21a53 Fix #928 - Minor naming inconsistency in tutorial json files (#929)\r\n0cb2f606 speed up metrics (#927)\r\n394c26d8 add try except block with helpful message (#923)\r\n97bc7a2d Combined recovery logic, made it not crash on beaker (#925)\r\nde5df629 Fix random hash that's appened to git SHA (#924)\r\nb7a9784e Nested constituents (#920)\r\n085a7509 use gold pos tags for tree evaluation (#921)\r\n8b706e42 Constituency js (#916)\r\n740f4fbb Constituency Parser Predictor and tests (#914)\r\n6a108577 Constituency training pr (#913)\r\n6ad7d5bc Migrating to 0.4.0 (#909)\r\nbca992ef Fix span masking (#905)\r\n6f4de853 Update TextualEntailment text to highlight ELMo. (#906)\r\nb891d374 (log-learning-rate) Merge branch 'release-v0.4.0'\r\n5756ff6c Span tutorial (#903)"}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/9617955", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/9617955/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/9617955/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v0.4.0", "id": 9617955, "author": {"login": "schmmd", "id": 954798, "node_id": "MDQ6VXNlcjk1NDc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/954798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/schmmd", "html_url": "https://github.com/schmmd", "followers_url": "https://api.github.com/users/schmmd/followers", "following_url": "https://api.github.com/users/schmmd/following{/other_user}", "gists_url": "https://api.github.com/users/schmmd/gists{/gist_id}", "starred_url": "https://api.github.com/users/schmmd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/schmmd/subscriptions", "organizations_url": "https://api.github.com/users/schmmd/orgs", "repos_url": "https://api.github.com/users/schmmd/repos", "events_url": "https://api.github.com/users/schmmd/events{/privacy}", "received_events_url": "https://api.github.com/users/schmmd/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTk2MTc5NTU=", "tag_name": "v0.4.0", "target_commitish": "master", "name": "v0.4.0", "draft": false, "prerelease": false, "created_at": "2018-02-20T19:43:09Z", "published_at": "2018-02-20T21:54:32Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v0.4.0", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v0.4.0", "body": "**New features:**\r\n\r\n* A major feature in the 0.4 release is the inclusion of ELMo which produces contextualized word embeddings that greatly improve model performance.  You can read more in our [ELMo HowTo](https://github.com/allenai/allennlp/blob/master/tutorials/how_to/elmo.md).\r\n* Support for [lazy datasets](https://github.com/allenai/allennlp/blob/master/tutorials/how_to/laziness.md), so you can stream data through the trainer with a lower memory footprint.  This is a breaking change for some parts of the API; if you've written a `DatasetReader`, you will probably need to change a little bit of code.  The `Dataset` class is now gone.\r\n* First-class support for models that operate on [_spans_ instead of on _tokens_](https://github.com/allenai/allennlp/blob/master/tutorials/how_to/span_representations.md).\r\n* Support for [programmatically importing additional dependencies](https://github.com/allenai/allennlp/pull/727) so you don't need to write your own `run.py` script.\r\n* A simple server to [create a stand-alone web demo](https://github.com/allenai/allennlp/blob/master/tutorials/getting_started/making_predictions_and_creating_a_demo.md) for your model.\r\n* Added [constrained decoding](https://github.com/allenai/allennlp/pull/818) to the `ConditionalRandomField` module (and to the corresponding [NER tagger](https://github.com/allenai/allennlp/blob/master/allennlp/models/crf_tagger.py) model)\r\n\r\n**Additional [tutorials](http://allennlp.org/tutorials):**\r\n\r\n* A tutorial on [writing `Predictors`](https://github.com/allenai/allennlp/blob/master/tutorials/getting_started/making_predictions_and_creating_a_demo.md) for using `python -m allennlp.run predict` and for creating demos.\r\n* Instructions for how to [visualize model internals](https://github.com/allenai/allennlp/blob/master/tutorials/how_to/visualizing_model_internals.md) in a live demo of your model, for gaining better insights about what your model is doing.\r\n* A tutorial on [how laziness works](https://github.com/allenai/allennlp/blob/master/tutorials/how_to/laziness.md) in AllenNLP.\r\n\r\n**Additional models / dataset readers:**\r\n* A [span-based constituency parser](https://github.com/allenai/allennlp/blob/master/allennlp/models/constituency_parser.py) that independently predicts a non-terminal label for each span in an input sentence, along with a [Penn treebank dataset reader](https://github.com/allenai/allennlp/blob/master/allennlp/data/dataset_readers/penn_tree_bank.py) that reads data for this model.  Trained model and demo coming soon.\r\n\r\n**Minor bugfixes and features:**\r\n* This release is compatible with pytorch 0.3.1 (and still compatible with pytorch 0.3.0).\r\n* Made it possible to do batch tokenization with spacy inside a `DatasetReader`.\r\n* Added a `make-vocab` command to precompute a vocabulary for a dataset.\r\n* Added a `fine-tune` command to fine-tune a trained model on a new dataset.\r\n* Unified handling of Ontonotes-based datasets, so it is now easier to write new `DatasetReaders` that use Ontonotes.\r\n* `Predictors` now support non-json formats for bulk prediction.\r\n* More flexible batching code, and bug fixes when batching / padding using `ListFields`.\r\n* Added ability to handle different data reading configurations at train and test time.\r\n\r\n**Breaking Changes**\r\n\r\nThis release contains several breaking changes. Please see [the migration guide](https://github.com/allenai/allennlp/blob/master/tutorials/misc/migrating-to-0.4.0.md) if you have pre-0.4.0 code you need to update."}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/8924762", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/8924762/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/8924762/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v0.3.0", "id": 8924762, "author": {"login": "schmmd", "id": 954798, "node_id": "MDQ6VXNlcjk1NDc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/954798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/schmmd", "html_url": "https://github.com/schmmd", "followers_url": "https://api.github.com/users/schmmd/followers", "following_url": "https://api.github.com/users/schmmd/following{/other_user}", "gists_url": "https://api.github.com/users/schmmd/gists{/gist_id}", "starred_url": "https://api.github.com/users/schmmd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/schmmd/subscriptions", "organizations_url": "https://api.github.com/users/schmmd/orgs", "repos_url": "https://api.github.com/users/schmmd/repos", "events_url": "https://api.github.com/users/schmmd/events{/privacy}", "received_events_url": "https://api.github.com/users/schmmd/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTg5MjQ3NjI=", "tag_name": "v0.3.0", "target_commitish": "main", "name": "v0.3.0", "draft": false, "prerelease": false, "created_at": "2017-12-14T21:58:07Z", "published_at": "2017-12-15T16:26:27Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v0.3.0", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v0.3.0", "body": "We updated our key dependencies to Spacy 2.0 and PyTorch 0.3, and we have a few additional models and many new features since our 0.2 release.\r\n\r\n**Additional models**.  More details for our models are available at http://allennlp.org/models and you can use them interactively online at http://demo.allennlp.org/.\r\n\r\n* The baseline NER model from [Semi-supervised Sequence Tagging with Bidirectional Language Models](https://www.semanticscholar.org/paper/Semi-supervised-sequence-tagging-with-bidirectiona-Peters-Ammar/73e59cb556351961d1bdd4ab68cbbefc5662a9fc).\r\n* A coreference model, based on the publication [End-to-end Neural Coreference Resolution](https://www.semanticscholar.org/paper/End-to-end-Neural-Coreference-Resolution-Lee-He/3f2114893dc44eacac951f148fbff142ca200e83), which achieved state-of-the-art performance in early 2017.\r\n\r\n**Additional examples and tutorials**.  Find them at http://allennlp.org/tutorials/installation.\r\n\r\n* A tutorial on u[sing AllenNLP in your project](http://allennlp.org/tutorials/using-in-your-repo) as a pip dependency.\r\n* Example code for[ training a seq2seq model](https://github.com/allenai/allennlp/blob/master/allennlp/models/encoder_decoders/simple_seq2seq.py).\r\n\r\n**New features**.\r\n\r\n* Improved SRL visualization on the demo.\r\n* `ListField` padding fixes."}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/8800813", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/8800813/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/8800813/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v0.2.3", "id": 8800813, "author": {"login": "DeNeutoy", "id": 16001974, "node_id": "MDQ6VXNlcjE2MDAxOTc0", "avatar_url": "https://avatars.githubusercontent.com/u/16001974?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DeNeutoy", "html_url": "https://github.com/DeNeutoy", "followers_url": "https://api.github.com/users/DeNeutoy/followers", "following_url": "https://api.github.com/users/DeNeutoy/following{/other_user}", "gists_url": "https://api.github.com/users/DeNeutoy/gists{/gist_id}", "starred_url": "https://api.github.com/users/DeNeutoy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DeNeutoy/subscriptions", "organizations_url": "https://api.github.com/users/DeNeutoy/orgs", "repos_url": "https://api.github.com/users/DeNeutoy/repos", "events_url": "https://api.github.com/users/DeNeutoy/events{/privacy}", "received_events_url": "https://api.github.com/users/DeNeutoy/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTg4MDA4MTM=", "tag_name": "v0.2.3", "target_commitish": "master", "name": "", "draft": false, "prerelease": false, "created_at": "2017-12-07T01:31:57Z", "published_at": "2017-12-06T19:05:30Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v0.2.3", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v0.2.3", "body": "- Fixes bugs in stateful versions of `Seq2XXXEncoders`.\r\n- Adds an `ArrayField` for array inputs to models.\r\n- Uses an up to date stand-alone version of tensorboard such that you can use it inside a single environment.\r\n- Adds the ability to read hdf5 formatted embedding matrices.\r\n- Adds a flag to specify what `Datasets` should be used to build vocabularies in `train`.\r\n"}
{"url": "https://api.github.com/repos/allenai/allennlp/releases/7658174", "assets_url": "https://api.github.com/repos/allenai/allennlp/releases/7658174/assets", "upload_url": "https://uploads.github.com/repos/allenai/allennlp/releases/7658174/assets{?name,label}", "html_url": "https://github.com/allenai/allennlp/releases/tag/v0.2.0", "id": 7658174, "author": {"login": "schmmd", "id": 954798, "node_id": "MDQ6VXNlcjk1NDc5OA==", "avatar_url": "https://avatars.githubusercontent.com/u/954798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/schmmd", "html_url": "https://github.com/schmmd", "followers_url": "https://api.github.com/users/schmmd/followers", "following_url": "https://api.github.com/users/schmmd/following{/other_user}", "gists_url": "https://api.github.com/users/schmmd/gists{/gist_id}", "starred_url": "https://api.github.com/users/schmmd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/schmmd/subscriptions", "organizations_url": "https://api.github.com/users/schmmd/orgs", "repos_url": "https://api.github.com/users/schmmd/repos", "events_url": "https://api.github.com/users/schmmd/events{/privacy}", "received_events_url": "https://api.github.com/users/schmmd/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTc2NTgxNzQ=", "tag_name": "v0.2.0", "target_commitish": "master", "name": "v0.2.0", "draft": false, "prerelease": false, "created_at": "2017-09-06T18:45:48Z", "published_at": "2017-09-08T13:57:04Z", "assets": [], "tarball_url": "https://api.github.com/repos/allenai/allennlp/tarball/v0.2.0", "zipball_url": "https://api.github.com/repos/allenai/allennlp/zipball/v0.2.0", "body": "The first release of AllenNLP using PyTorch.\r\n\r\nThis release of AllenNLP includes three models:\r\n\r\n* Semantic Role Labeling (78.9 dev F1, 78.9 test F1, CoNLL 2012)\r\n* Machine Comprehension (68.7 EM, SQuAD)\r\n* Textual Entailment (84.7 test accuracy, SNLI)"}
