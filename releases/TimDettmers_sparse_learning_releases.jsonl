{"url": "https://api.github.com/repos/TimDettmers/sparse_learning/releases/19960933", "assets_url": "https://api.github.com/repos/TimDettmers/sparse_learning/releases/19960933/assets", "upload_url": "https://uploads.github.com/repos/TimDettmers/sparse_learning/releases/19960933/assets{?name,label}", "html_url": "https://github.com/TimDettmers/sparse_learning/releases/tag/v1.0", "id": 19960933, "author": {"login": "TimDettmers", "id": 5260050, "node_id": "MDQ6VXNlcjUyNjAwNTA=", "avatar_url": "https://avatars.githubusercontent.com/u/5260050?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TimDettmers", "html_url": "https://github.com/TimDettmers", "followers_url": "https://api.github.com/users/TimDettmers/followers", "following_url": "https://api.github.com/users/TimDettmers/following{/other_user}", "gists_url": "https://api.github.com/users/TimDettmers/gists{/gist_id}", "starred_url": "https://api.github.com/users/TimDettmers/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TimDettmers/subscriptions", "organizations_url": "https://api.github.com/users/TimDettmers/orgs", "repos_url": "https://api.github.com/users/TimDettmers/repos", "events_url": "https://api.github.com/users/TimDettmers/events{/privacy}", "received_events_url": "https://api.github.com/users/TimDettmers/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTE5OTYwOTMz", "tag_name": "v1.0", "target_commitish": "master", "name": "Release v1.0", "draft": false, "prerelease": false, "created_at": "2019-09-13T01:23:07Z", "published_at": "2019-09-13T01:59:07Z", "assets": [], "tarball_url": "https://api.github.com/repos/TimDettmers/sparse_learning/tarball/v1.0", "zipball_url": "https://api.github.com/repos/TimDettmers/sparse_learning/zipball/v1.0", "body": "\r\n## Release v1.0: Bug fixes, New ImageNet Baselines\r\n\r\n### Bug fixes:\r\n - Changed to boolean indexing for PyTorch 1.2 compatibility.\r\n - Fixed an error where an error can occur for global pruning algorithms if very few weights were removed for a layer.\r\n - Removed momentum reset. This feature did not have any effect on performance and made the algorithm more complex.\r\n - Fixed an error where two layers of VGG16 were removed by use of the `remove_weight_partial_name()` function. Results were slightly degraded, but weights needed for dense performance and relative ordering compared to other methods remained the same.\r\n\r\n### Features:\r\n - Evaluation script can now aggregate log files organized in a folder hierarchy. For each folder results will be aggregated.\r\n - Added decay schedule argument. One can choose between Linear and Cosine prune rate decay schedules.\r\n - Added new ImageNet baseline which is based on the codebase of [Mostafa & Wang, 2019](https://arxiv.org/abs/1902.05967).\r\n - Added a max-thread argument which can be used to set the total maximum data loader threads for training, validation and test set data loaders."}
{"url": "https://api.github.com/repos/TimDettmers/sparse_learning/releases/19151716", "assets_url": "https://api.github.com/repos/TimDettmers/sparse_learning/releases/19151716/assets", "upload_url": "https://uploads.github.com/repos/TimDettmers/sparse_learning/releases/19151716/assets{?name,label}", "html_url": "https://github.com/TimDettmers/sparse_learning/releases/tag/v0.2", "id": 19151716, "author": {"login": "TimDettmers", "id": 5260050, "node_id": "MDQ6VXNlcjUyNjAwNTA=", "avatar_url": "https://avatars.githubusercontent.com/u/5260050?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TimDettmers", "html_url": "https://github.com/TimDettmers", "followers_url": "https://api.github.com/users/TimDettmers/followers", "following_url": "https://api.github.com/users/TimDettmers/following{/other_user}", "gists_url": "https://api.github.com/users/TimDettmers/gists{/gist_id}", "starred_url": "https://api.github.com/users/TimDettmers/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TimDettmers/subscriptions", "organizations_url": "https://api.github.com/users/TimDettmers/orgs", "repos_url": "https://api.github.com/users/TimDettmers/repos", "events_url": "https://api.github.com/users/TimDettmers/events{/privacy}", "received_events_url": "https://api.github.com/users/TimDettmers/received_events", "type": "User", "site_admin": false}, "node_id": "MDc6UmVsZWFzZTE5MTUxNzE2", "tag_name": "v0.2", "target_commitish": "v0.2", "name": "Release v0.2", "draft": false, "prerelease": false, "created_at": "2019-08-08T00:44:04Z", "published_at": "2019-08-08T00:44:52Z", "assets": [], "tarball_url": "https://api.github.com/repos/TimDettmers/sparse_learning/tarball/v0.2", "zipball_url": "https://api.github.com/repos/TimDettmers/sparse_learning/zipball/v0.2", "body": "# Release v0.2: FP16 support, modularity of prune/growth/redistribution algorithms.\r\n\r\n## Bug fixes:\r\n - Fixed a but where magnitude pruning pruned too many parameters when the weight was dense (>95% density) and the pruning rate was small (<5%).\r\n   First experiments on LeNet-5 Caffe indicate that this change did not affect performance for networks that learn to have dense weights.\r\n   I will replicate this across architectures to make sure this bugfix does not change performance.\r\n - Fixed instabilities in SET (sparse evolutionary training) pruning which could cause nan values in specific circumstances.\r\n - Fixed a bug where global pruning would throw an error if a layer was fully dense and had a low prune rate.\r\n\r\n## Documentation:\r\n - Added basic docstring documentation\r\n\r\n## Features:\r\n - MNIST/CIFAR: Separate log files are not created for different models/densities/names.\r\n - MNIST/CIFAR: Aggregate mean test accuracy with standard errors can now be automatically extracted from logs with `python get_results_from_logs.py`.\r\n - Added FP16 support. Any model can now be run in 16-bit by passing the [apex](https://github.com/NVIDIA/apex) `FP16_Optimizer` into the `Masking` class and replacing `loss.backward()` with `optimizer.backward(loss)`.\r\n - Added adapted [Dynamic Sparse Reparameterization](https://arxiv.org/abs/1902.05967) [codebase](https://github.com/IntelAI/dynamic-reparameterization) that works with sparse momentum.\r\n - Added modular architecture for growth/prune/redistribution algorithms which is decoupled from the main library. This enables you to write your own prune/growth/redistribution algorithms without touched the library internals. A tutorial on how to add your own functions was also added: [How to Add Your Own Algorithms](How_to_add_your_own_algorithms.md).\r\n\r\n## API:\r\n - Changed names from \"death\" to \"prune\" to be more consistent with the terminology in the paper.\r\n - Added --verbose argument to print the parameter distribution before/after pruning at the end of each epoch. By default, the pruning distribution will no longer be printed.\r\n - Removed --sparse flag and added --dense flag. The default is args.dense==False and thus sparse mode is enabled by default. To run a dense model just pass the --dense argument.\r\n"}
