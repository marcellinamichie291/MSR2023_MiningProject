{"url": "https://api.github.com/repos/pytorch/torchrec/releases/81132963", "assets_url": "https://api.github.com/repos/pytorch/torchrec/releases/81132963/assets", "upload_url": "https://uploads.github.com/repos/pytorch/torchrec/releases/81132963/assets{?name,label}", "html_url": "https://github.com/pytorch/torchrec/releases/tag/v0.3.0", "id": 81132963, "author": {"login": "YLGH", "id": 9512279, "node_id": "MDQ6VXNlcjk1MTIyNzk=", "avatar_url": "https://avatars.githubusercontent.com/u/9512279?v=4", "gravatar_id": "", "url": "https://api.github.com/users/YLGH", "html_url": "https://github.com/YLGH", "followers_url": "https://api.github.com/users/YLGH/followers", "following_url": "https://api.github.com/users/YLGH/following{/other_user}", "gists_url": "https://api.github.com/users/YLGH/gists{/gist_id}", "starred_url": "https://api.github.com/users/YLGH/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/YLGH/subscriptions", "organizations_url": "https://api.github.com/users/YLGH/orgs", "repos_url": "https://api.github.com/users/YLGH/repos", "events_url": "https://api.github.com/users/YLGH/events{/privacy}", "received_events_url": "https://api.github.com/users/YLGH/received_events", "type": "User", "site_admin": false}, "node_id": "RE_kwDOFvjel84E1f2j", "tag_name": "v0.3.0", "target_commitish": "v0.3.0", "name": "v0.3.0", "draft": false, "prerelease": false, "created_at": "2022-10-27T16:11:24Z", "published_at": "2022-10-27T16:21:49Z", "assets": [], "tarball_url": "https://api.github.com/repos/pytorch/torchrec/tarball/v0.3.0", "zipball_url": "https://api.github.com/repos/pytorch/torchrec/zipball/v0.3.0", "body": "## [ProtoType] Simplified Optimizer Fusion APIs\r\n\r\nWe\u2019ve provided a simplified and more intuitive API for setting fused optimizer settings via apply_optimizer_in_backward. This new approach enables the ability to specify optimizer settings on a per-parameter basis and sharded modules will configure [FBGEMM\u2019s TableBatchedEmbedding modules accordingly](https://github.com/pytorch/FBGEMM/blob/main/fbgemm_gpu/fbgemm_gpu/split_table_batched_embeddings_ops.py#L181). Additionally, this now let's TorchRec\u2019s planner account for optimizer memory usage. This should alleviate reports of sharding jobs OOMing after using Adam using a plan generated from planner.\r\n\r\n## [ProtoType] Simplified Sharding APIs\r\n\r\nWe\u2019re introducing the shard API, which now allows you to shard only the embedding modules within a model, and provides an alternative to the current main entry point - DistributedModelParallel. This lets you have a finer grained control over the rest of the model, which can be useful for customized parallelization logic, and inference use cases (which may not require any parallelization on the dense layers). We\u2019re also introducing construct_module_sharding_plan, providing a simpler interface to the TorchRec sharder.\r\n\r\n## [Beta] Integration with FBGEMM's Quantized Comms Library\r\n\r\nApplying [quantization or mixed precision](https://dlp-kdd.github.io/assets/pdf/a11-yang.pdf) to tensors in a collective call during model parallel training greatly improves training efficiency, with little to no effect on model quality. TorchRec now integrates with the [quantized comms library provided by FBGEMM GPU](https://github.com/pytorch/FBGEMM/blob/main/fbgemm_gpu/fbgemm_gpu/quantize_comm.py) and provides an interface to construct encoders and decoders (codecs) that surround the all_to_all, and reduce_scatter collective calls in the output_dist of a sharded module. We also allow you to construct your own codecs to apply to your sharded module. The codces provided by FBGEMM allow FP16, BF16, FP8, and INT8 compressions, and you may use different quantizations for the forward path and backward pass.\r\n\r\n## Planner\r\n\r\n* We removed several unnecessary copies inside of planner that drastically decreases the runtime.\r\n* Cleaned up the Topology interface (no longer takes in unrelated information like batch size). "}
{"url": "https://api.github.com/repos/pytorch/torchrec/releases/70708064", "assets_url": "https://api.github.com/repos/pytorch/torchrec/releases/70708064/assets", "upload_url": "https://uploads.github.com/repos/pytorch/torchrec/releases/70708064/assets{?name,label}", "html_url": "https://github.com/pytorch/torchrec/releases/tag/v0.2.0", "id": 70708064, "author": {"login": "YLGH", "id": 9512279, "node_id": "MDQ6VXNlcjk1MTIyNzk=", "avatar_url": "https://avatars.githubusercontent.com/u/9512279?v=4", "gravatar_id": "", "url": "https://api.github.com/users/YLGH", "html_url": "https://github.com/YLGH", "followers_url": "https://api.github.com/users/YLGH/followers", "following_url": "https://api.github.com/users/YLGH/following{/other_user}", "gists_url": "https://api.github.com/users/YLGH/gists{/gist_id}", "starred_url": "https://api.github.com/users/YLGH/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/YLGH/subscriptions", "organizations_url": "https://api.github.com/users/YLGH/orgs", "repos_url": "https://api.github.com/users/YLGH/repos", "events_url": "https://api.github.com/users/YLGH/events{/privacy}", "received_events_url": "https://api.github.com/users/YLGH/received_events", "type": "User", "site_admin": false}, "node_id": "RE_kwDOFvjel84ENutg", "tag_name": "v0.2.0", "target_commitish": "release/0.2.0", "name": "v0.2.0", "draft": false, "prerelease": false, "created_at": "2022-06-28T00:41:07Z", "published_at": "2022-06-28T06:37:57Z", "assets": [], "tarball_url": "https://api.github.com/repos/pytorch/torchrec/tarball/v0.2.0", "zipball_url": "https://api.github.com/repos/pytorch/torchrec/zipball/v0.2.0", "body": "# Changelog\r\n\r\n## PyPi Installation\r\nThe recommended install location is now from pypy. Additionally, TorchRec's binary will not longer contain fbgemm_gpu. Instead fbgemm_gpu will be installed as a dependency. See README for details\r\n\r\n## Planner Improvements\r\n\r\nWe added some additional features and bug fixed some bugs\r\nVariable batch size per feature to support request only features\r\nBetter calculations for quant UVM Caching \r\nBug fix for shard storage fitting on device\r\n\r\n## Single process Batched + Fused Embeddings\r\n\r\nPreviously TorchRec\u2019s abstractions (EmbeddingBagCollection/EmbeddingCollection) over FBGEMM kernels, which provide benefits such as table batching, optimizer fusion, and UVM placement, could only be used in conjunction with DistributedModelParallel. We\u2019ve decoupled these notions from sharding, and introduced the [FusedEmbeddingBagCollection](https://github.com/pytorch/torchrec/blob/eb1247d8a2d16edc4952e5c2617e69acfe5477a5/torchrec/modules/fused_embedding_modules.py#L271), which can be used as a standalone module, with all of the above features, and can also be sharded.\r\n\r\n## Sharder\r\nWe enabled embedding sharding support for variable batch sizes across GPUs.\r\n\r\n## Benchmarking and Examples\r\nWe introduce \r\nA set of [benchmarking tests](https://github.com/pytorch/torchrec/tree/eb1247d8a2d16edc4952e5c2617e69acfe5477a5/benchmarks), showing performance characteristics of TorchRec\u2019s base modules  and research models built out of TorchRec.\r\nWe provide an example demonstrating training a distributed TwoTower (i.e. User-Item) Retrieval model that is sharded using TorchRec. The projected item embeddings are added to an IVFPQ FAISS index for candidate generation. The retrieval model and KNN lookup are bundled in a Pytorch model for efficient end-to-end retrieval.\r\ninference example with Torch Deploy for both single and multi GPU\r\n\r\n## Integrations\r\nWe demonstrate that TorchRec works out of the box with many components commonly used alongside PyTorch models in production like systems, such as \r\n* [Training](https://github.com/pytorch/torchrec/tree/main/examples/ray) a TorchRec model on Ray Clusters utilizing the Torchx Ray scheduler\r\n* [Preprocessing](https://github.com/pytorch/torchrec/tree/main/torchrec/datasets/scripts/nvt) and DataLoading with NVTabular on DLRM\r\n* [Training](https://github.com/pytorch/torchrec/tree/main/examples/torcharrow) a TorchRec model with on-the-fly preprocessing with TorchArrow showcasing RecSys domain UDFs.\r\n\r\n## Scriptable Unsharded Modules\r\nThe unsharded embedding modules (EmbeddingBagCollection/EmbeddingCollection and variants) are now torch scriptable.\r\n\r\n## EmbeddingCollection Column Wise Sharding\r\nWe now support column wise sharding for EmbeddingCollection, enabling sequence embeddings to be sharded column wise.\r\n\r\n## JaggedTensor\r\nBoost performance of `to_padded_dense` function by implementing with FBGEMM.\r\n\r\n## Linting\r\nAdd lintrunner to allow contributors to lint and format their changes quickly, matching our internal formatter.", "reactions": {"url": "https://api.github.com/repos/pytorch/torchrec/releases/70708064/reactions", "total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}}
{"url": "https://api.github.com/repos/pytorch/torchrec/releases/67105944", "assets_url": "https://api.github.com/repos/pytorch/torchrec/releases/67105944/assets", "upload_url": "https://uploads.github.com/repos/pytorch/torchrec/releases/67105944/assets{?name,label}", "html_url": "https://github.com/pytorch/torchrec/releases/tag/v0.1.1", "id": 67105944, "author": {"login": "colin2328", "id": 78784971, "node_id": "MDQ6VXNlcjc4Nzg0OTcx", "avatar_url": "https://avatars.githubusercontent.com/u/78784971?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colin2328", "html_url": "https://github.com/colin2328", "followers_url": "https://api.github.com/users/colin2328/followers", "following_url": "https://api.github.com/users/colin2328/following{/other_user}", "gists_url": "https://api.github.com/users/colin2328/gists{/gist_id}", "starred_url": "https://api.github.com/users/colin2328/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colin2328/subscriptions", "organizations_url": "https://api.github.com/users/colin2328/orgs", "repos_url": "https://api.github.com/users/colin2328/repos", "events_url": "https://api.github.com/users/colin2328/events{/privacy}", "received_events_url": "https://api.github.com/users/colin2328/received_events", "type": "User", "site_admin": false}, "node_id": "RE_kwDOFvjel84D__SY", "tag_name": "v0.1.1", "target_commitish": "release/0.1.1", "name": "v0.1.1", "draft": false, "prerelease": false, "created_at": "2022-05-13T17:17:23Z", "published_at": "2022-05-17T18:12:15Z", "assets": [], "tarball_url": "https://api.github.com/repos/pytorch/torchrec/tarball/v0.1.1", "zipball_url": "https://api.github.com/repos/pytorch/torchrec/zipball/v0.1.1", "body": "# Changelog\r\n## pytorch.org Install\r\nThe recommended install location is now from download.pytorch.org. See README for details\r\n## Recmetrics \r\nRecMetrics is a metrics library that collects common utilities and optimizations for Recommendation models.\r\n\r\n- A centralized metrics module that allows users to add new metrics\r\n- Commonly used metrics, including AUC, Calibration, CTR, MSE/RMSE, NE & Throughput\r\n- Optimization for metrics related operations to reduce the overhead of metric computation\r\n- Checkpointing\r\n\r\n## Torchrec inference\r\nLarger models need GPU support for inference. Also, there is a difference between features used in common training stacks and inference stacks. The goal of this library is to make use of some features seen in training to make inference more unified and easier to use.\r\n## EmbeddingTower and EmbeddingTowerCollection\r\na new sharadable nn.Module called EmbeddingTower/EmbeddingTowerCollection.  This module will give model authors the basic building block to establish a clear relationship between a set of embedding tables and post lookup modules.\r\n## Examples/tutorials\r\n### Inference example\r\ndocumentation (installation and example), updated cmake build and gRPC server example\r\n### Bert4rec example\r\nReproduction of bert4rec paper showcasing EmbeddingCollection module (non pooling)\r\n### Sharding Tutorial\r\nOverview of sharding in torchrec and the five types of sharding https://pytorch.org/tutorials/advanced/sharding.html\r\n## Improved Planner\r\n\r\n- Updated static estimates for perf\r\n- Models full model parallel path\r\n- Includes support for sequence embeddings, weighted features, and feature processors\r\n- Added grid search proposer\r\n"}
{"url": "https://api.github.com/repos/pytorch/torchrec/releases/64503857", "assets_url": "https://api.github.com/repos/pytorch/torchrec/releases/64503857/assets", "upload_url": "https://uploads.github.com/repos/pytorch/torchrec/releases/64503857/assets{?name,label}", "html_url": "https://github.com/pytorch/torchrec/releases/tag/v0.1.0", "id": 64503857, "author": {"login": "colin2328", "id": 78784971, "node_id": "MDQ6VXNlcjc4Nzg0OTcx", "avatar_url": "https://avatars.githubusercontent.com/u/78784971?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colin2328", "html_url": "https://github.com/colin2328", "followers_url": "https://api.github.com/users/colin2328/followers", "following_url": "https://api.github.com/users/colin2328/following{/other_user}", "gists_url": "https://api.github.com/users/colin2328/gists{/gist_id}", "starred_url": "https://api.github.com/users/colin2328/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colin2328/subscriptions", "organizations_url": "https://api.github.com/users/colin2328/orgs", "repos_url": "https://api.github.com/users/colin2328/repos", "events_url": "https://api.github.com/users/colin2328/events{/privacy}", "received_events_url": "https://api.github.com/users/colin2328/received_events", "type": "User", "site_admin": false}, "node_id": "RE_kwDOFvjel84D2EAx", "tag_name": "v0.1.0", "target_commitish": "release/0.1.0", "name": "Beta release", "draft": false, "prerelease": false, "created_at": "2022-02-11T00:27:39Z", "published_at": "2022-04-14T23:52:37Z", "assets": [], "tarball_url": "https://api.github.com/repos/pytorch/torchrec/tarball/v0.1.0", "zipball_url": "https://api.github.com/repos/pytorch/torchrec/zipball/v0.1.0", "body": "We are excited to announce [TorchRec](https://github.com/pytorch/torchrec), a PyTorch domain library for Recommendation Systems. This new library provides common sparsity and parallelism primitives, enabling researchers to build state-of-the-art personalization models and deploy them in production.\r\n\r\nModeling primitives, such as embedding bags and jagged tensors, that enable easy authoring of large, performant multi-device/multi-node models using hybrid data-parallelism and model-parallelism.\r\nOptimized RecSys kernels powered by [FBGEMM](https://github.com/pytorch/FBGEMM) , including support for sparse and quantized operations.\r\nA sharder which can partition embedding tables with a variety of different strategies including data-parallel, table-wise, row-wise, table-wise-row-wise, and column-wise sharding.\r\nA planner which can automatically generate optimized sharding plans for models.\r\nPipelining to overlap dataloading device transfer (copy to GPU), inter-device communications (input_dist), and computation (forward, backward) for increased performance.\r\nGPU inference support.\r\nCommon modules for RecSys, such as models and public datasets (Criteo & Movielens).\r\n\r\nSee our [announcement](https://pytorch.org/blog/introducing-torchrec/) and [docs](https://pytorch.org/torchrec/)", "reactions": {"url": "https://api.github.com/repos/pytorch/torchrec/releases/64503857/reactions", "total_count": 2, "+1": 1, "-1": 0, "laugh": 0, "hooray": 1, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}}
